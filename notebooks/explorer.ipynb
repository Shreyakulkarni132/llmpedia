{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-01T23:50:11.041776Z",
     "start_time": "2023-08-01T23:50:11.032341Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "with open('../arxiv_code_map.json') as f:\n",
    "    data = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T23:50:13.801890Z",
     "start_time": "2023-08-01T23:50:13.794579Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "titles = list(data.values())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T23:50:33.230213Z",
     "start_time": "2023-08-01T23:50:33.223974Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "['Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction',\n 'Language models are weak learners',\n 'Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm',\n 'Augmenting Language Models with Long-Term Memory',\n 'Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code',\n 'Bring Your Own Data! Self-Supervised Evaluation for Large Language Models',\n \"Why Johnny Still, Still Can't Encrypt: Evaluating the Usability of a Modern PGP Client\",\n 'Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors',\n 'In-context Autoencoder for Context Compression in a Large Language Model',\n 'Educational data augmentation in physics education research using ChatGPT',\n 'Llama 2: Open Foundation and Fine-Tuned Chat Models',\n 'Demystifying GPT Self-Repair for Code Generation',\n 'Parsel: Algorithmic Reasoning with Language Models by Composing Decompositions',\n 'Teaching Algorithmic Reasoning via In-context Learning',\n 'WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences',\n \"Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting\",\n 'Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models',\n 'Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration',\n 'Cedille: A large autoregressive French language model',\n 'Retentive Network: A Successor to Transformer for Large Language Models',\n 'Measuring and Narrowing the Compositionality Gap in Language Models',\n \"Seeing ChatGPT Through Students' Eyes: An Analysis of TikTok Data\",\n 'PoET: A generative model of protein families as sequences-of-sequences',\n 'Connecting Neural Response measurements & Computational Models of language: a non-comprehensive guide',\n 'One-shot Machine Teaching: Cost Very Few Examples to Converge Faster',\n 'Learning to Retrieve In-Context Examples for Large Language Models',\n 'PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback',\n 'A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models',\n 'Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models',\n 'Extending Context Window of Large Language Models via Positional Interpolation',\n 'KoLA: Carefully Benchmarking World Knowledge of Large Language Models',\n 'Stack More Layers Differently: High-Rank Training Through Low-Rank Updates',\n 'Inspecting and Editing Knowledge Representations in Language Models',\n 'TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT',\n 'Differentially Private Fine-tuning of Language Models',\n 'Large Language Models Can Be Easily Distracted by Irrelevant Context',\n 'Automatic Generation of Programming Exercises and Code Explanations using Large Language Models',\n 'LLMs as Workers in Human-Computational Algorithms? Replicating Crowdsourcing Pipelines with LLMs',\n 'Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge',\n 'GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models',\n 'Urdu text in natural scene images: a new dataset and preliminary text detection',\n \"Co-Writing with Opinionated Language Models Affects Users' Views\",\n 'BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs',\n 'Is GPT-4 a Good Data Analyst?',\n 'LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition',\n 'Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods',\n 'KI-BERT: Infusing Knowledge Context for Better Language and Domain Understanding',\n 'Questioning the Survey Responses of Large Language Models',\n 'Beyond Black Box AI-Generated Plagiarism Detection: From Sentence to Document Level',\n 'Guiding Pretraining in Reinforcement Learning with Large Language Models',\n 'Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model',\n 'Quantifying Memorization Across Neural Language Models',\n 'Becoming self-instruct: introducing early stopping criteria for minimal instruct tuning',\n 'A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation',\n 'One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning',\n 'Building Cooperative Embodied Agents Modularly with Large Language Models',\n \"Mind's Eye: Grounded Language Model Reasoning through Simulation\",\n 'ChatGPT and a New Academic Reality: Artificial Intelligence-Written Research Papers and the Ethics of the Large Language Models in Scholarly Publishing',\n 'Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning',\n 'Constitutional AI: Harmlessness from AI Feedback',\n 'Large Language Models for Supply Chain Optimization',\n 'Images in Language Space: Exploring the Suitability of Large Language Models for Vision & Language Tasks',\n 'The Internal State of an LLM Knows When its Lying',\n 'ALERT: Adapting Language Models to Reasoning Tasks',\n 'Challenges and Applications of Large Language Models',\n 'Emergent autonomous scientific research capabilities of large language models',\n 'LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond',\n 'Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling',\n 'StarCoder: may the source be with you!',\n 'Open-Source Large Language Models Outperform Crowd Workers and Approach ChatGPT in Text-Annotation Tasks',\n 'OpenAGI: When LLM Meets Domain Experts',\n 'Causal Reasoning and Large Language Models: Opening a New Frontier for Causality',\n 'Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies',\n 'LIMA: Less Is More for Alignment',\n 'Ranking Distillation: Learning Compact Ranking Models With High Performance for Recommender System',\n 'The Wisdom of Hindsight Makes Language Models Better Instruction Followers',\n 'LongNet: Scaling Transformers to 1,000,000,000 Tokens',\n \"Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change)\",\n 'Causal-Discovery Performance of ChatGPT in the context of Neuropathic Pain Diagnosis',\n 'Automatic Evaluation of Attribution by Large Language Models',\n 'Explainable Verbal Reasoner Plus (EVR+): A Natural Language Reasoning Framework that Supports Diverse Compositional Reasoning',\n 'Solving Quantitative Reasoning Problems with Language Models',\n 'Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective',\n 'Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation',\n 'Invariant Language Modeling',\n 'CascadER: Cross-Modal Cascading for Knowledge Graph Link Prediction',\n 'Explanation of Reinforcement Learning Model in Dynamic Multi-Agent System',\n 'Interactive Fashion Content Generation Using LLMs and Latent Diffusion Models',\n 'Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents',\n 'ARB: Advanced Reasoning Benchmark for Large Language Models',\n 'Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks',\n 'FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance',\n 'Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs',\n 'Dissociating language and thought in large language models: a cognitive perspective',\n 'Robot Task Planning and Situation Handling in Open Worlds',\n 'Teaching Arithmetic to Small Transformers',\n 'BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models',\n 'FinGPT: Open-Source Financial Large Language Models',\n 'Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models',\n 'A Watermark for Large Language Models',\n 'ChatGPT, Can You Generate Solutions for my Coding Exercises? An Evaluation on its Effectiveness in an undergraduate Java Programming Course',\n 'Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks',\n 'Prototypical Fine-tuning: Towards Robust Performance Under Varying Data Sizes',\n 'WizardCoder: Empowering Code Large Language Models with Evol-Instruct',\n 'Language models show human-like content effects on reasoning',\n 'Can Large Language Models design a Robot?',\n 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face',\n 'Crowd Score: A Method for the Evaluation of Jokes using Large Language Model AI Voters as Judges',\n 'From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought',\n 'Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback',\n 'Measuring Faithfulness in Chain-of-Thought Reasoning',\n 'Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations',\n 'A Systematic Evaluation of Large Language Models of Code',\n 'Decomposed Prompting: A Modular Approach for Solving Complex Tasks',\n 'Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences',\n 'Deep Language Networks: Joint Prompt Training of Stacked LLMs using Variational Inference',\n 'When Geometric Deep Learning Meets Pretrained Protein Language Models',\n 'ReAct: Synergizing Reasoning and Acting in Language Models',\n 'CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis',\n 'Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision',\n 'Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models',\n 'VNHSGE: VietNamese High School Graduation Examination Dataset for Large Language Models',\n 'Large Language Model Guided Tree-of-Thought',\n 'Large Language Models as Tax Attorneys: A Case Study in Legal Capabilities Emergence',\n 'Language Is Not All You Need: Aligning Perception with Language Models',\n 'Shortcut Learning of Large Language Models in Natural Language Understanding',\n 'Theory of Mind May Have Spontaneously Emerged in Large Language Models',\n 'A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis',\n 'SPOT: Knowledge-Enhanced Language Representations for Information Extraction',\n 'Self-Critique Prompting with Large Language Models for Inductive Instructions',\n 'Copy Is All You Need',\n 'Structured abbreviation expansion in context',\n 'TinyStories: How Small Can Language Models Be and Still Speak Coherent English?',\n 'Evaluating Large Language Models on a Highly-specialized Topic, Radiation Oncology Physics',\n 'Do Large Language Models know what humans know?',\n 'Are Emergent Abilities of Large Language Models a Mirage?',\n 'UO2/BeO interfacial thermal resistance and its effect on fuel thermal conductivity',\n 'Stay on topic with Classifier-Free Guidance',\n 'Towards Reasoning in Large Language Models: A Survey',\n 'Understanding Social Reasoning in Language Models with Language Models',\n 'Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding',\n 'Selective Annotation Makes Language Models Better Few-Shot Learners',\n 'The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only',\n 'Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks',\n 'Performance of ChatGPT on USMLE: Unlocking the Potential of Large Language Models for AI-Assisted Medical Education',\n 'Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models',\n 'Modeling Protein Using Large-scale Pretrain Language Model',\n 'Re-visiting Automated Topic Model Evaluation with Large Language Models',\n 'Identifying Mentions of Pain in Mental Health Records Text: A Natural Language Processing Approach',\n 'Large Language Models Perform Diagnostic Reasoning',\n 'Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback',\n 'Orca: Progressive Learning from Complex Explanation Traces of GPT-4',\n 'Chain-Of-Thought Prompting Under Streaming Batch: A Case Study',\n 'QLoRA: Efficient Finetuning of Quantized LLMs',\n 'Social Simulacra: Creating Populated Prototypes for Social Computing Systems',\n 'Textbooks Are All You Need',\n 'AlpaGasus: Training A Better Alpaca with Fewer Data',\n 'Jigsaw: Large Language Models meet Program Synthesis',\n 'Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning',\n 'Code as Policies: Language Model Programs for Embodied Control',\n 'PolyLM: An Open Source Polyglot Large Language Model',\n 'Ada-Ranker: A Data Distribution Adaptive Ranking Paradigm for Sequential Recommendation',\n 'DarkBERT: A Language Model for the Dark Side of the Internet',\n 'GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models',\n 'Can large language models reason about medical questions?',\n 'Entity Projection via Machine Translation for Cross-Lingual NER',\n 'Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding',\n 'Learning ASR pathways: A sparse multilingual ASR model',\n 'Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models',\n 'Large Language Models Are Reasoning Teachers',\n 'Prompting Large Language Models with Speech Recognition Abilities',\n 'Large-Scale Multilingual Speech Recognition with a Streaming End-to-End Model',\n 'Long-range Language Modeling with Self-retrieval',\n 'Faithful Chain-of-Thought Reasoning',\n 'From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models',\n 'On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models',\n 'Data-Driven Approach for Formality-Sensitive Machine Translation: Language-Specific Handling and Synthetic Data Generation',\n 'Structured information extraction from complex scientific text with fine-tuned large language models',\n 'Getting More out of Large Language Models for Proofs',\n 'Beyond Generating Code: Evaluating GPT on a Data Visualization Course',\n 'WebArena: A Realistic Web Environment for Building Autonomous Agents',\n 'Probing Factually Grounded Content Transfer with Factual Ablation',\n 'ChatGPT: A Study on its Utility for Ubiquitous Software Engineering Tasks',\n 'How does the pre-training objective affect what large language models learn about linguistic properties?',\n 'Prompt Sapper: LLM-Empowered Software Engineering Infrastructure for AI-Native Services',\n 'Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language',\n 'INT2.1: Towards Fine-Tunable Quantized Large Language Models with Error Correction through Low-Rank Adaptation',\n 'Explicit Motion Risk Representation',\n 'Large language models effectively leverage document-level context for literary translation, but critical errors persist',\n \"How is ChatGPT's behavior changing over time?\",\n 'Scaling TransNormer to 175 Billion Parameters',\n 'News Summarization and Evaluation in the Era of GPT-3',\n 'Kosmos-2: Grounding Multimodal Large Language Models to the World',\n 'GPT is becoming a Turing machine: Here are some ways to program it',\n 'Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling',\n 'Personality Traits in Large Language Models',\n 'Adaptive Test Generation Using a Large Language Model',\n 'Block Belief Propagation for Parameter Learning in Markov Random Fields']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T23:50:34.906704Z",
     "start_time": "2023-08-01T23:50:34.902225Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def update_gist(token, gist_id, gist_filename, gist_description, gist_content):\n",
    "    \"\"\"\n",
    "    Update a gist on github.\n",
    "\n",
    "    Parameters:\n",
    "    token (str): GitHub API token\n",
    "    gist_id (str): The ID of the gist to update\n",
    "    gist_filename (str): Name of the file to be updated\n",
    "    gist_description (str): Updated description of the gist\n",
    "    gist_content (str): Updated content of the gist\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {token}\",\n",
    "        \"Accept\": \"application/vnd.github.v3+json\",\n",
    "    }\n",
    "    params = {\n",
    "        \"description\": gist_description,\n",
    "        \"files\": {\n",
    "            gist_filename: {\n",
    "                \"content\": gist_content\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = requests.patch(\n",
    "        f\"https://api.github.com/gists/{gist_id}\",\n",
    "        headers=headers,\n",
    "        data=json.dumps(params)\n",
    "    )\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Gist {gist_filename} updated successfully\")\n",
    "        return response.json()[\"html_url\"]\n",
    "    else:\n",
    "        print(f\"Failed to update gist. Status code: {response.status_code}.\")\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-02T00:00:48.221388Z",
     "start_time": "2023-08-02T00:00:48.181971Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gist llm_papers.txt updated successfully\n",
      "Gist URL: https://gist.github.com/masta-g3/8f7227397b1053b42e727bbd6abf1d2e\n"
     ]
    }
   ],
   "source": [
    "token = \"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\"\n",
    "gist_id = \"8f7227397b1053b42e727bbd6abf1d2e\"\n",
    "gist_filename = \"llm_papers.txt\"\n",
    "gist_description = \"A nice gist\"\n",
    "gist_content = \"\\n\".join(titles)\n",
    "\n",
    "gist_url = update_gist(token, gist_id, gist_filename, gist_description, gist_content)\n",
    "print(f\"Gist URL: {gist_url}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-02T00:00:55.719723Z",
     "start_time": "2023-08-02T00:00:54.355618Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
