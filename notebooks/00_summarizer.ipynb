{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-23T16:40:09.433249Z",
     "start_time": "2023-09-23T16:40:09.424616Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summarizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI, AzureChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "import tiktoken\n",
    "\n",
    "import os, sys\n",
    "import re, json\n",
    "import arxiv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-23T17:41:52.049148Z",
     "start_time": "2023-09-23T17:41:52.020066Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "sys.path.append(os.environ.get(\"PROJECT_PATH\"))\n",
    "\n",
    "import utils.paper_utils as pu"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-23T16:40:11.567411Z",
     "start_time": "2023-09-23T16:40:11.523169Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper Funcs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\" Clean and simplify text string. \"\"\"\n",
    "    text = ''.join(c.lower() if c.isalnum() else ' ' for c in text)\n",
    "    return text\n",
    "\n",
    "def reformat_text(doc_content):\n",
    "    content = doc_content.replace('-\\n', '')\n",
    "    content = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', content)\n",
    "    content = re.sub(' +', ' ', content)\n",
    "    return content\n",
    "\n",
    "def tfidf_similarity(title1, title2):\n",
    "    \"\"\" Compute cosine similarity of TF-IDF representation between 2 strings. \"\"\"\n",
    "    title1 = preprocess(title1)\n",
    "    title2 = preprocess(title2)\n",
    "    vectorizer = TfidfVectorizer(use_idf=False, analyzer='char', ngram_range=(2,3)).fit_transform([title1, title2])\n",
    "    vectors = vectorizer.toarray()\n",
    "    return cosine_similarity(vectors[0:1], vectors[1:2])[0][0]\n",
    "\n",
    "\n",
    "def get_arxiv_info(title):\n",
    "    \"\"\" Search article in Arxiv by name and retrieve meta-data. \"\"\"\n",
    "    search = arxiv.Search(\n",
    "        query=preprocess(title),\n",
    "        max_results=20,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    res = list(search.results())\n",
    "    if len(res) > 0:\n",
    "        ## Sort by title similarity.\n",
    "        res = sorted(res, key=lambda x: tfidf_similarity(title, x.title), reverse=True)\n",
    "        new_title = res[0].title\n",
    "        title_sim = tfidf_similarity(title, new_title)\n",
    "        if title_sim > 0.7:\n",
    "            return res[0]\n",
    "        else:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "token_encoder = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-23T16:40:11.732496Z",
     "start_time": "2023-09-23T16:40:11.554390Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LLM Chain Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "## Underlying LLM.\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-23T17:19:17.971049Z",
     "start_time": "2023-09-23T17:19:17.941232Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "## Create prompt.\n",
    "system_prompt = \"\"\"\n",
    "As an applied AI researcher specialized in the field of Large Language Models (LLMs), you are currently conducting a survey of the literature, building a catalogue of the main contributions and innovations of each paper, determining how they can be applied to build systems or create new products. This catalogue will be published by a prestigious organization and will serve as the foundation for all applied LLM knowledge going forward. Now, carefully read the following paper:\n",
    "\n",
    "{content}\n",
    "\n",
    "========================\n",
    "\n",
    "SUMMARY\n",
    "\n",
    "{prev_summary}\n",
    "\n",
    "Now answer the following questions:\n",
    "\n",
    "1. What is the `main_contribution` of this paper? (1 line headline + 8-12 sentences)\n",
    "    - If a new algorithm or technique is introduced, describe its workings clearly and step by step.\n",
    "    - Do not assume that the reader knows the meaning of new terminology presented in the paper or complex concepts. \n",
    "    - Ensure that your answer provides practical insights that offer a solid understanding of the paper.\n",
    "    - Detail the benefits or advantages of what has been presented, along with the practical implications for an LLM practitioner.\n",
    "    - Do not include anything already discussed in the summary or abstract.\n",
    "\n",
    "2. What is the main `takeaway`? (1 line headline + 8-12 sentences)\n",
    "    - Focusing on the paper's contributions, explain how they can be used to create an interesting LLM application, improve current workflows, or increase efficiency when working with LLMs.\n",
    "    - If different models were evaluated and their performance recorded, please note this and its practical implications (in detailed manner, i.e.: which model is best for what).\n",
    "    - Be very precise, practical and specific as possible. Eliminate any irrelevant content from the paper's applied perspective.\n",
    "    - Always provide a minimal but detailed applied example related to the takeaway.\n",
    "\n",
    "3. Which category best describes this paper's primary focus? Choose one from the following options, with \"OTHER\" being the least desirable choice.\n",
    "    a. \"TRAINING\": Discussions on LLM training methods, technical stack improvements, alternative training routines, etc.\n",
    "    b. \"FINE-TUNING\": Discussions on fine-tuning, re-training, and specialization of LLMs.\n",
    "    c. \"ARCHITECTURES\": Discussions on new LLM architectures, neural network components, etc., excluding prompting or computational systems to manage LLMs.\n",
    "    d. \"PROMPTING\": Discussions on prompting methods, agent architectures, etc.\n",
    "    e. \"USE CASES\": Discussions on LLM use in specific tasks, such as summarization, question answering, stock prediction, etc.\n",
    "    f. \"BEHAVIOR\": Discussions on LLM behavior, including probing, interpretability, risks, biases, emerging abilities, etc.\n",
    "    g. \"OTHER\": None of the above.\n",
    "\n",
    "4. On a scale from 1 to 3, how novel is this paper? (1: not novel, 2: incrementally novel, 3: very novel)\n",
    "    - Compare the paper's findings and contributions with what is presented in previous and related work. How unique and significant are the findings?\n",
    "    - Be strict and rigorous; few papers should receive a high score.\n",
    "    - Pay close attention to the comparison with prior work and the degree of difference in the author's contributions.\n",
    "\n",
    "5. On a scale from 1 to 3, how technical is this paper? (1: not technical, 2: somewhat technical, 3: very technical)\n",
    "    a) A very technical paper is difficult for a non-expert to understand, requires considerable technical knowledge, is filled with equations and jargon, and demands advanced mathematical knowledge.\n",
    "    b) A somewhat technical paper may be challenging for a layman but can be understood reasonably well by someone with a computer science background. These papers, while not overly complex, explain processes in great detail and are practical and applicable (can be replicated).\n",
    "    c) A non-technical paper is understandable for anyone with a college degree. These papers often discuss generalities, and the takeaways are more conceptual than technical.\n",
    "\n",
    "6. On a scale from 1 to 3, how enjoyable is this paper? (1: hard to read, 2: ok, 3: a delight)\n",
    "    a) A very enjoyable paper is well-written, organized, presents a novel and intriguing contribution, and is easy to read.\n",
    "    b) An 'ok' paper is primarily plain and unexciting but is easy to read and contains some interesting parts. Most papers\n",
    "    c) A non-enjoyable paper is difficult to read, poorly written, and lacks meaningful, practical, and insightful content.\n",
    "\n",
    "When assigning numerical ratings consider these guidelines:\n",
    "- Rating 3/3: Only about 20% of papers reach this standard.\n",
    "- Rating 2/3: Most papers (50%) fall into this category.\n",
    "- Rating 1/3: Around 30% of papers belong to this category.\n",
    "\n",
    "Do not repeat the same comments across different answers. Make your \"applied_example\" different from the ones presented in the paper. Make sure your answers are coherent, clear and truthful.\n",
    "\n",
    "Use the JSON format as in the following examples to respond.\n",
    "\n",
    "EXAMPLE 1\n",
    "==========\n",
    "```\n",
    "{{\n",
    "    \"main_contribution\": {{\n",
    "        \"headline\": \"Chain-of-Thought (CoT) boosts LLM accuracy in financial sentiment analysis\",\n",
    "    \"description\": \"The paper introduces the Chain-of-Thought (CoT) prompting technique for Large Language Models (LLMs) specifically targeting financial sentiment analysis. The core of CoT lies in its deviation from direct predictions. Instead, it guides the model to build a sequence of interconnected thoughts leading to an accurate sentiment score. In a comparative study, LLMs equipped with CoT achieved a 94% accuracy, surpassing the established FinBERT's 88% and the naive prompting model's 81%.\"\n",
    "    }},\n",
    "    \"takeaways\": {{\n",
    "        \"headline\": \"CoT opens new, efficient avenues for LLMs in financial analysis\",\n",
    "        \"description\": \"Using the CoT prompting technique, LLMs can achieve enhanced accuracy in financial news sentiment analysis, ultimately refining stock market predictions. This method not only improves prediction accuracy but also renders the model's thought process transparent. When pitted against FinBERT, the LLM with CoT demonstrated superior performance, signaling its potential dominance in financial analysis tasks.\",\n",
    "        \"applied_example\": \"When processing a news snippet like 'Company X has strong Q3 earnings', an LLM with CoT could generate: 'Strong Q3 earnings -> Likely effective management -> Expected investor trust growth -> Potential bullish market -> Possible stock price ascent.' This layered output simplifies decision-making for market analysts.\"\n",
    "    }},\n",
    "    \"category\": \"USE CASES\",\n",
    "    \"novelty_analysis\": \"The paper extends the boundaries of current research by applying LLMs to financial news sentiment analysis. The introduction of the CoT prompting technique, tailored specifically for this application, represents an incremental advancement in the field.\",\n",
    "    \"novelty_score\": 2,\n",
    "    \"technical_analysis\": \"While the paper discusses a computational framework for managing LLM inputs and outputs, it does not delve into complex mathematical theories or algorithms, making it accessible to a wider audience.\",\n",
    "    \"technical_score\": 1,\n",
    "    \"enjoyable_analysis\": \"The engaging narrative style, coupled with practical insights, makes the paper an enjoyable read. It balances technical details with easily digestible information and an interesting practical application.\",\n",
    "    \"enjoyable_score\": 3\n",
    "}}\n",
    "```\n",
    "\n",
    "EXAMPLE 2\n",
    "==========\n",
    "```\n",
    "{{\n",
    "    \"main_contribution\": {{\n",
    "        \"headline\": \"Zero-shot Prompting Technique for GPT-4 Code Interpreter\",\n",
    "        \"description\": \"This paper proposes a zero-shot prompting technique for GPT-4 Code Interpreter that explicitly encourages the use of code for self-verification, which further boosts performance on math reasoning problems. They report a positive correlation between the better performance of GPT4-Code and the higher Code Usage Frequency. Initial experiments show that GPT4-Code achieved a zero-shot accuracy of 69.7% on the MATH dataset which is an improvement of 27.5% over GPT-4’s performance (42.2%).\"\n",
    "    }},\n",
    "    \"takeaways\": {{\n",
    "        \"headline\": \"Leveraging Self-verification and Code Execution in LLMs\",\n",
    "        \"description\": \"Self-verification is already a powerful approach to enhance the performance of LLMs on many tasks but this approach leverages the evaluation of code execution which could make it interesting to solve other kinds of problems. This work highlights the importance of code understanding and generation capabilities in LLMs.\",\n",
    "        \"applied_example\": \"Some of the ideas presented in this paper (specifically, the code-based self-verification and verification-guided weighted majority voting technique) can lead to building high-quality datasets that could potentially help improve the mathematical capabilities in open-source LLMs like Llama 2.\"\n",
    "    }},\n",
    "    \"category\": \"PROMPTING\",\n",
    "    \"novelty_analysis\": \"The research innovatively combines LLMs with code-based self-verification, achieving a 20% boost over state-of-the-art coding task accuracies. This method's practicality is evident, with tests showing a 30% reduction in coding errors, redefining efficiency in LLM-driven code generation.\",\n",
    "    \"novelty_score\": 3,\n",
    "    \"technical_analysis\": \"The paper delve into advanced algorithms, such as the Hypothetical Code-Integration Algorithm (HCIA), making it a dense read for those unfamiliar with theoretical computer science. While the introduction of a novel concept is enlightening, the paper's reliance on complex algorithms, logical proofs and symbolic reasoning makes it a technically advanced read.\",\n",
    "    \"technical_score\": 3,\n",
    "    \"enjoyable_analysis\": \"For those deeply engrossed in the LLM landscape, this paper promises an engaging journey. While its technical nuances can be challenging, the clearly presented transformative results, such as the significant performance leap in the MATH dataset, ensure a gripping narrative.\",\n",
    "    \"enjoyable_score\": 2\n",
    "}}\n",
    "```\n",
    "\n",
    "EXAMPLE 3\n",
    "==========\n",
    "```\n",
    "{{\n",
    "    \"main_contribution\": {{\n",
    "        \"headline\": \"LLMManager: LLM-Driven Database Maintenance Knowledge Acquisition\",\n",
    "        \"description\": \"LLMManager leverages a retriever system paired with a LLM to extract database maintenance knowledge from diverse textual sources. It incorporates a hybrid mechanism that combines transformer-based models with traditional relational database algorithms. The framework's ability to parse vast amounts of text and convert them into actionable database maintenance tasks has led to notable metrics: a 47% increase in real-time database issue detection and a 32% improvement in automated problem resolution compared to existing SotA systems.\"\n",
    "    }},\n",
    "    \"takeaways\": {{\n",
    "        \"headline\": \"Leveraging 'Tree of Thought' Reasoning for Enhanced Maintenance\",\n",
    "        \"description\": \"LLMManager integration of the 'tree of thought' reasoning not only enhances root cause analysis but also creates a dynamic learning environment. Over time, LLMManager ability to revert to prior steps during anomalies becomes more refined, ensuring adaptive and evolving responses to complex database issues. Furthermore, its modular design allows for seamless integration with other LLMs, magnifying the collaborative aspect of the framework.\",\n",
    "        \"applied_example\": \"Automating database maintenance with D-Bot can lead to significant reductions in downtime and costs. Developers could design LLM systems that proactively address issues even before they escalate, unlocking more efficient and streamlined database operations.\"\n",
    "    }},\n",
    "    \"category\": \"USE CASES\",\n",
    "    \"novelty_analysis\": \"D-Bot's utilization of the 'tree of thought' reasoning in database maintenance is novel, although a targeted application inspired by similar work on other engineering areas.\",\n",
    "    \"novelty_score\": 2,\n",
    "    \"technical_analysis\": \"The paper delves into Entity-Relationship Diagrams and database management algorithms essential to LLMManagers's operations. However, it manages to remain accessible, avoiding overly complex jargon and ensuring a broader audience comprehension.\",\n",
    "    \"technical_score\": 2,\n",
    "    \"enjoyable_analysis\": \"The work provides a balanced blend of technical details and real-world applications, giving insights into LLMManager's functions and potential impacts.\",\n",
    "    \"enjoyable_score\": 2\n",
    "}}\n",
    "```\n",
    "\n",
    "EXAMPLE 4\n",
    "==========\n",
    "{{\n",
    "    \"main_contribution\": {{\n",
    "        \"headline\": \"Performance Analysis of LLMs in Entity Recognition\",\n",
    "        \"description\": \"The paper undertakes a systematic comparison of four Large Language Models (LLMs) - GPT-4, Claude, GPT-3.5, and Prodisol-001 - with a focus on entity recognition. Each model was subjected to a consistent dataset, and their entity extraction capabilities were assessed based on precision, recall, and F1 score. Results highlighted that GPT-4 outperformed the other models, with Claude closely following, and GPT-3.5 and Prodisol-001 trailing behind. This comparative study offers insights into the current capabilities of prominent LLMs in the domain of entity recognition.\"\n",
    "    }},\n",
    "    \"takeaways\": {{\n",
    "        \"headline\": \"Entity Recognition Capabilities Vary Across LLMs\",\n",
    "        \"description\": \"The paper underscores variations in the performance of different LLMs when tasked with entity recognition. The presented findings provide a benchmark for professionals and researchers aiming to choose an LLM for entity recognition tasks. The nuanced comparison suggests that while GPT-4 exhibits top-tier performance in this domain, other models like Claude also present strong capabilities.\",\n",
    "        \"applied_example\": \"When parsing a complex news article about the merger between two tech giants, it becomes crucial to accurately recognize and categorize entities such as company names, CEOs, financial figures, and locations. An LLM with superior entity recognition, in such a context, aids in extracting critical data points efficiently, enabling a more thorough analysis of the situation.\"\n",
    "    }},\n",
    "    \"category\": \"USE CASES\",\n",
    "    \"novelty_analysis\": \"The study contributes to existing literature by offering a contemporary comparison of the latest LLMs in entity recognition. While the task itself isn't novel, the inclusion of GPT-4 and Claude in the comparison introduces an incremental advancement to the current body of research.\",\n",
    "    \"novelty_score\": 2,\n",
    "    \"technical_analysis\": \"The paper balances technical depth with accessibility, providing a detailed outline of evaluation metrics and methodologies. This ensures insights are communicated comprehensively, catering to both technical and non-technical readers.\",\n",
    "    \"technical_score\": 2,\n",
    "    \"enjoyable_analysis\": \"Through its well-structured approach and clear visualizations, the paper facilitates an engaging read. The methodical presentation of results aids in drawing comparisons and understanding the landscape of LLMs in entity recognition.\",\n",
    "    \"enjoyable_score\": 2\n",
    "}}\n",
    "```\n",
    "\n",
    "YOUR TURN\n",
    "==========\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-23T17:19:18.425202Z",
     "start_time": "2023-09-23T17:19:18.397641Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"Tip: Make sure to provide your response in the correct format. Do not forget to iclude the 'applied_example' under 'takeaways'!\"),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-23T17:40:23.999208Z",
     "start_time": "2023-09-23T17:40:23.968051Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "class Contribution(BaseModel):\n",
    "    headline: str = Field(..., description=\"Headline of the main contribution.\")\n",
    "    description: str = Field(..., description=\"Description of the main contribution.\")\n",
    "\n",
    "class Takeaways(BaseModel):\n",
    "    headline: str = Field(..., description=\"Headline of the main takeaway.\")\n",
    "    description: str = Field(..., description=\"Description of the main takeaway.\")\n",
    "    applied_example: str = Field(..., description=\"Applied example related to the main takeaway.\")\n",
    "\n",
    "class PaperReview(BaseModel):\n",
    "    main_contribution: Contribution = Field(..., description=\"The main contribution of the paper.\")\n",
    "    takeaways: Takeaways = Field(..., description=\"The main takeaways from the paper.\")\n",
    "    category: str = Field(..., description=\"The primary focus category of the paper.\")\n",
    "    novelty_analysis: str = Field(..., description=\"Analysis of the paper's novelty.\")\n",
    "    novelty_score: int = Field(..., description=\"Score representing the novelty of the paper.\")\n",
    "    technical_analysis: str = Field(..., description=\"Analysis of the paper's technical depth.\")\n",
    "    technical_score: int = Field(..., description=\"Score representing the technical depth of the paper.\")\n",
    "    enjoyable_analysis: str = Field(..., description=\"Analysis of the paper's readability and engagement level.\")\n",
    "    enjoyable_score: int = Field(..., description=\"Score representing the enjoyability of reading the paper.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-23T17:40:24.731967Z",
     "start_time": "2023-09-23T17:40:24.705232Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "chain = create_structured_output_chain(PaperReview, llm, prompt, verbose=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-23T17:40:26.640581Z",
     "start_time": "2023-09-23T17:40:26.599106Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Iterate Papers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "paper_names = [\n",
    "    # \"ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models\",\n",
    "    # \"Efficient RLHF: Reducing the Memory Usage of PPO\",\n",
    "    # \"Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior\",\n",
    "    # \"Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following\",\n",
    "    # \"Can Programming Languages Boost Each Other via Instruction Tuning?\",\n",
    "    # \"The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants\",\n",
    "    # \"BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge\",\n",
    "    # \"LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models\",\n",
    "    # \"LLaSM: Large Language and Speech Model\",\n",
    "    # \"Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models\",\n",
    "    # \"MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic Medical Records\",\n",
    "    # \"OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models\",\n",
    "    # \"SoTaNa: The Open-Source Software Development Assistant\",\n",
    "    # \"Teach LLMs to Personalize -- An Approach inspired by Writing Education\",\n",
    "    # \"AnomalyGPT: Detecting Industrial Anomalies using Large Vision-Language Models\",\n",
    "    # \"FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios\",\n",
    "    # \"Graph of Thoughts: Solving Elaborate Problems with Large Language Models\",\n",
    "    # \"LLaSM: Large Language and Speech Model\",\n",
    "    # \"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\",\n",
    "    # \"GPT Can Solve Mathematical Problems Without a Calculator\",\n",
    "    # \"Large Language Models as Optimizers\",\n",
    "    # \"DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models\",\n",
    "    # \"FLM-101B: An Open LLM and How to Train It with $100K Budget\",\n",
    "    # \"XGen-7B Technical Report\",\n",
    "    # \"Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models\",\n",
    "    # \"Physically Grounded Vision-Language Models for Robotic Manipulation\",\n",
    "    # \"Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning\",\n",
    "    # \"Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology\",\n",
    "    # \"Gated recurrent neural networks discover attention\",\n",
    "    # \"Topic Discovery via Latent Space Clustering of Pretrained Language Model Representations\",\n",
    "    # \"Improving Language Models with Advantage-based Offline Policy Gradients\",\n",
    "    # \"DrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs\",\n",
    "    # \"Large-Scale Automatic Audiobook Creation\",\n",
    "]\n",
    "\n",
    "print(len(paper_names))\n",
    "failed_papers = []\n",
    "existing_papers = [\"XXXX\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-23T17:43:04.791415Z",
     "start_time": "2023-09-23T17:43:04.761232Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:22<05:18, 22.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:42<04:35, 21.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: GPT Can Solve Mathematical Problems Without a Calculator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [01:00<03:54, 19.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [01:21<03:42, 20.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: NExT-GPT: Any-to-Any Multimodal LLM\n",
      "1 validation error for _OutputFormatter\n",
      "__root__\n",
      "  Expecting property name enclosed in double quotes: line 6 column 5 (char 835) (type=value_error.jsondecode; msg=Expecting property name enclosed in double quotes; doc={\n",
      "  \"output\": {\n",
      "    \"main_contribution\": {\n",
      "      \"headline\": \"NExT-GPT: Any-to-Any Multimodal LLM\",\n",
      "      \"description\": \"The paper presents NExT-GPT, an end-to-end general-purpose any-to-any Multimodal Large Language Model (MM-LLM) system. NExT-GPT connects an LLM with multimodal adaptors and diffusion decoders, enabling it to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. The system leverages existing well-trained encoders and decoders, requiring only a small amount of parameter adjustment for efficient training and potential expansion to more modalities. Additionally, the paper introduces a modality-switching instruction tuning (MosIT) and a high-quality dataset for MosIT, empowering NExT-GPT with complex cross-modal semantic understanding and content generation.\",\n",
      "    },\n",
      "    \"takeaways\": {\n",
      "      \"headline\": \"Building a Universal Any-to-Any MM-LLM\",\n",
      "      \"description\": \"NExT-GPT showcases the possibility of developing a more human-like MM-LLM agent capable of modeling universal modalities. The system enables seamless handling of input and output in any combination of text, images, videos, and audio. It leverages existing encoders and decoders, reducing training costs and facilitating potential expansion. The modality-switching instruction tuning (MosIT) enhances the system's cross-modal understanding and content generation capabilities. NExT-GPT opens up new opportunities for building AI systems that can comprehend and generate content across various modalities.\",\n",
      "      \"applied_example\": \"For example, NExT-GPT can be used to build a conversational AI agent that understands and responds to user inputs in any modality. Users can communicate with the agent using text, images, videos, or audio, and the agent can generate responses in the appropriate modality. This enables more natural and human-like interactions between users and AI systems, enhancing the user experience and expanding the range of applications for AI technology.\"\n",
      "    },\n",
      "    \"category\": \"USE CASES\",\n",
      "    \"novelty_analysis\": \"The paper introduces NExT-GPT, an end-to-end any-to-any MM-LLM system, which is a novel contribution in the field of multimodal language models. It combines existing encoders and decoders to achieve universal multimodal understanding and generation, with the ability to handle inputs and outputs in any combination of modalities.\",\n",
      "    \"novelty_score\": 3,\n",
      "    \"technical_analysis\": \"The paper provides a technical description of the NExT-GPT system, including the architecture, encoding and decoding stages, alignment learning techniques, and modality-switching instruction tuning. It also discusses the use of existing encoders and decoders, as well as the manual curation of a high-quality dataset for instruction tuning.\",\n",
      "    \"technical_score\": 2,\n",
      "    \"enjoyable_analysis\": \"The paper presents a compelling vision of building a unified AI agent capable of modeling universal modalities. It combines technical details with practical insights and examples, making it an enjoyable read for both technical and non-technical readers.\",\n",
      "    \"enjoyable_score\": 3\n",
      "  }\n",
      "}; pos=835; lineno=6; colno=5)\n",
      "Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [01:57<04:20, 26.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [02:13<03:21, 22.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Textbooks Are All You Need II: phi-1.5 technical report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [02:32<02:49, 21.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Neurons in Large Language Models: Dead, N-gram, Positional\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [02:54<02:31, 21.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: FIAT: Fusing learning paradigms with Instruction-Accelerated Tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [03:11<02:01, 20.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: MADLAD-400: A Multilingual And Document-Level Large Audited Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [03:30<01:38, 19.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Hypothesis Search: Inductive Reasoning with Language Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [03:48<01:16, 19.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ANALYZING TRANSFORMER DYNAMICS AS MOVEMENT THROUGH EMBEDDING SPACE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [04:15<00:31, 15.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found locally: From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting:\n",
      "Processing: What In-Context Learning Learns In-Context: Disentangling Task Recognition and Task Learning\n",
      "1 validation error for _OutputFormatter\n",
      "__root__\n",
      "  Expecting property name enclosed in double quotes: line 6 column 5 (char 1095) (type=value_error.jsondecode; msg=Expecting property name enclosed in double quotes; doc={\n",
      "  \"output\": {\n",
      "    \"main_contribution\": {\n",
      "      \"headline\": \"Disentangling Task Recognition and Task Learning in In-Context Learning\",\n",
      "      \"description\": \"This paper investigates the mechanisms of in-context learning (ICL) in large language models (LLMs) by characterizing two distinct abilities: task recognition (TR) and task learning (TL). TR refers to the model's ability to recognize a task through demonstrations and apply its pre-trained priors, while TL refers to the model's ability to learn a new input-label mapping from demonstrations. The paper conducts controlled experiments using a wide range of classification datasets and three LLM families (GPT-3, LLaMA, and OPT) to disentangle the roles of TR and TL in ICL. The findings reveal that TR is a broader capability that does not scale with model sizes or numbers of demonstrations, while TL is enabled by larger models and improves with more demonstrations. This distinction between TR and TL provides a better understanding of the mechanisms behind ICL and highlights the need to differentiate them in future research.\",\n",
      "    },\n",
      "    \"takeaways\": {\n",
      "      \"headline\": \"Understanding the Dual Forces of Task Recognition and Task Learning in In-Context Learning\",\n",
      "      \"description\": \"The paper's findings have practical implications for LLM practitioners. The distinction between task recognition (TR) and task learning (TL) in ICL allows practitioners to better understand the capabilities and limitations of LLMs. TR, which is the ability to recognize a task based on pre-trained priors, is a broader capability that is not significantly affected by model sizes or numbers of demonstrations. TL, on the other hand, is an emergent ability that is enabled by larger models and improves with more demonstrations. This understanding can guide practitioners in selecting the appropriate model size and number of demonstrations for their specific task. For example, if a task can be recognized through pre-trained priors, a smaller model and fewer demonstrations may be sufficient. However, if a new input-label mapping needs to be learned, a larger model and more demonstrations may be necessary. By leveraging the insights from this paper, practitioners can optimize their use of LLMs and improve the performance of their applications.\",\n",
      "      \"applied_example\": \"For a sentiment analysis task, an LLM practitioner can use the findings from this paper to determine the optimal approach. If the sentiment analysis task can be recognized based on pre-trained priors (e.g., recognizing positive or negative sentiment from movie reviews), a smaller LLM model and a few demonstrations may be sufficient. However, if the sentiment analysis task requires learning a new input-label mapping (e.g., recognizing sentiment from customer feedback in a specific domain), a larger LLM model and more demonstrations would be necessary to achieve better performance. By understanding the distinction between task recognition and task learning, the practitioner can make informed decisions and optimize the performance of the sentiment analysis application.\"\n",
      "    },\n",
      "    \"category\": \"BEHAVIOR\",\n",
      "    \"novelty_analysis\": \"The paper provides novel insights into the mechanisms of in-context learning (ICL) in large language models (LLMs) by characterizing the distinct abilities of task recognition (TR) and task learning (TL). While previous work has explored ICL as a single phenomenon, this paper disentangles TR and TL and demonstrates their different behaviors under varying conditions. This novel approach contributes to a deeper understanding of ICL and highlights the need to differentiate TR and TL in future research.\",\n",
      "    \"novelty_score\": 3,\n",
      "    \"technical_analysis\": \"The paper presents a technical analysis of the roles of task recognition (TR) and task learning (TL) in in-context learning (ICL) in large language models (LLMs). It describes the experimental setup using a wide range of classification datasets and three LLM families (GPT-3, LLaMA, and OPT). The technical analysis includes the evaluation of TR and TL performance across different model sizes and numbers of demonstrations. The findings provide insights into the scalability of TR and the emergence of TL with larger models and more demonstrations. The technical analysis contributes to the understanding of the mechanisms behind ICL and provides practical guidance for LLM practitioners.\",\n",
      "    \"technical_score\": 2,\n",
      "    \"enjoyable_analysis\": \"The paper presents a comprehensive analysis of task recognition (TR) and task learning (TL) in in-context learning (ICL) in large language models (LLMs). The findings are presented in a clear and organized manner, making it easy to follow the experimental setup and understand the results. The paper strikes a good balance between technical details and practical implications, making it an enjoyable read for both researchers and practitioners in the field of LLMs.\",\n",
      "    \"enjoyable_score\": 3\n",
      "  }\n",
      "}; pos=1095; lineno=6; colno=5)\n",
      "Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [04:53<00:22, 22.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: A Latent Space Theory for Emergent Abilities in Large Language Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [05:09<00:00, 20.63s/it]\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    for paper_name in tqdm(paper_names):\n",
    "        ## Get paper.\n",
    "        # pre_similarity = max([tfidf_similarity(paper_name, t) for t in existing_papers])\n",
    "        # if pre_similarity > 0.9:\n",
    "        #     continue\n",
    "\n",
    "        docs = ArxivLoader(\n",
    "            query=preprocess(paper_name), \n",
    "            doc_content_chars_max=70000, \n",
    "            load_max_docs=2).load()\n",
    "        if len(docs) == 0:\n",
    "            print(f\"Could not find {paper_name}.\")\n",
    "            failed_papers.append(paper_name)\n",
    "            continue\n",
    "\n",
    "        docs = sorted(docs, key=lambda x: tfidf_similarity(paper_name, x.metadata[\"Title\"]), reverse=True)\n",
    "        new_title = docs[0].metadata[\"Title\"]\n",
    "        title_sim = tfidf_similarity(paper_name, new_title)\n",
    "        if title_sim < 0.7:\n",
    "            print(f\"No similar title name found for {paper_name}.\")\n",
    "            continue\n",
    "        doc_meta = docs[0].metadata\n",
    "        doc_content = docs[0].page_content\n",
    "        doc_content = reformat_text(doc_content)\n",
    "        \n",
    "        ## Remove references.\n",
    "        if len(doc_content.split(\"References\")) == 2:\n",
    "            doc_content = doc_content.split(\"References\")[0]\n",
    "        \n",
    "        ## Check size.\n",
    "        system_template = \" \".join([m.prompt.template for m in chain.prompt.messages])\n",
    "        ntokens_prompt = len(token_encoder.encode(system_template))\n",
    "        ntokens_doc = len(token_encoder.encode(doc_content))\n",
    "        if ntokens_prompt + ntokens_doc > 15000:\n",
    "            ## Aprox tokens token len is 3.\n",
    "            doc_content = doc_content[:15000 - ntokens_prompt * 3]\n",
    "        \n",
    "        first_author = doc_meta[\"Authors\"].split(\" \")[0]\n",
    "        published = pd.to_datetime(doc_meta[\"Published\"]).strftime(\"%Y_%m_%d\")\n",
    "        prev_summary = doc_meta[\"Summary\"].replace(\"\\n\", \" \")\n",
    "\n",
    "        ## Name serially.\n",
    "        base_name = f\"{published}_{first_author.lower()}\"\n",
    "        i = 1\n",
    "        if os.path.exists(f\"../summaries/{base_name}_{str(i).zfill(3)}.json\"):\n",
    "            print(f\"Found locally: {paper_name}:\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Processing: {paper_name}\")\n",
    "\n",
    "        ## Run model.\n",
    "        try:\n",
    "            summary = chain.run({'content': doc_content, \"prev_summary\": prev_summary})\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"Retrying...\")\n",
    "            summary = chain.run({'content': doc_content, \"prev_summary\": prev_summary})\n",
    "\n",
    "        ## Extract and combine results.\n",
    "        parsed_summary = summary.json()\n",
    "        parsed_summary = json.loads(parsed_summary)\n",
    "\n",
    "        result_dict = {**doc_meta, **parsed_summary}\n",
    "        result_dict[\"Summary\"] = prev_summary\n",
    "\n",
    "        ## Store.\n",
    "        with open(f\"../summaries/{base_name}_{str(i).zfill(3)}.json\", 'w') as f:\n",
    "            json.dump(result_dict, f)\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Failed on {paper_name}:\")\n",
    "        #     print(e)\n",
    "        #     failed_papers.append(paper_name)\n",
    "        #     continue"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-23T17:48:24.788455Z",
     "start_time": "2023-09-23T17:43:15.293082Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 154455\n",
      "\tPrompt Tokens: 143518\n",
      "\tCompletion Tokens: 10937\n",
      "Successful Requests: 16\n",
      "Total Cost (USD): $0.4743020000000001\n"
     ]
    }
   ],
   "source": [
    "print(cb)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-23T18:23:39.016879Z",
     "start_time": "2023-09-23T18:23:38.985191Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "data": {
      "text/plain": "{'Published': '2023-09-13',\n 'Title': 'A Latent Space Theory for Emergent Abilities in Large Language Models',\n 'Authors': 'Hui Jiang',\n 'Summary': 'Languages are not created randomly but rather to communicate information. There is a strong association between languages and their underlying meanings, resulting in a sparse joint distribution that is heavily peaked according to their correlations. Moreover, these peak values happen to match with the marginal distribution of languages due to the sparsity. With the advent of LLMs trained on big data and large models, we can now precisely assess the marginal distribution of languages, providing a convenient means of exploring the sparse structures in the joint distribution for effective inferences. In this paper, we categorize languages as either unambiguous or {\\\\epsilon}-ambiguous and present quantitative results to demonstrate that the emergent abilities of LLMs, such as language understanding, in-context learning, chain-of-thought prompting, and effective instruction fine-tuning, can all be attributed to Bayesian inference on the sparse joint distribution of languages.',\n 'main_contribution': {'headline': 'A Latent Space Theory for Emergent Abilities in Large Language Models',\n  'description': 'The paper proposes a novel latent space theory to explain the emergent abilities of Large Language Models (LLMs). It categorizes languages as either unambiguous or ε-ambiguous and demonstrates that the emergent abilities of LLMs, such as language understanding, in-context learning, chain-of-thought prompting, and effective instruction fine-tuning, can all be attributed to Bayesian inference on the sparse joint distribution of languages.'},\n 'takeaways': {'headline': 'Exploring the Emergent Abilities of LLMs',\n  'description': 'The paper highlights the remarkable emergent abilities of LLMs and their underlying mechanisms. It provides insights into how LLMs can understand language, learn in-context, prompt chain-of-thought reasoning, and align with instructions. These findings have practical implications for building LLM-based systems and products.',\n  'applied_example': 'For example, an LLM with language understanding abilities can be used to develop chatbots that accurately comprehend and respond to user queries. In the context of in-context learning, LLMs can quickly adapt to new tasks by observing a few examples, enabling efficient few-shot learning. Chain-of-thought prompting can enhance the performance of LLMs in complex reasoning tasks, such as question answering or problem-solving. Finally, aligning LLMs with instructions allows for fine-tuning and customization, making them more useful and reliable in specific applications.'},\n 'category': 'BEHAVIOR',\n 'novelty_analysis': 'The paper presents a novel latent space theory that explains the emergent abilities of LLMs and provides a comprehensive analysis of their underlying mechanisms. It builds upon previous research on language understanding, in-context learning, and instruction fine-tuning, offering new insights into the Bayesian inference process and the sparse joint distribution of languages.',\n 'novelty_score': 3,\n 'technical_analysis': 'The paper is moderately technical, discussing concepts such as latent space models, universal density approximators, and Bayesian inference. It provides a detailed analysis of the theoretical foundations and presents simulation experiments to validate the proposed theory. While some mathematical understanding is required, the paper is accessible to researchers and practitioners in the field of LLMs.',\n 'technical_score': 2,\n 'enjoyable_analysis': 'The paper is well-written and engaging, presenting a clear narrative and practical implications. It combines theoretical discussions with simulation experiments, making it informative and enjoyable for readers interested in the emergent abilities of LLMs.',\n 'enjoyable_score': 3}"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-23T18:23:44.516150Z",
     "start_time": "2023-09-23T18:23:44.488719Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
