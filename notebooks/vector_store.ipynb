{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T03:29:24.356160Z",
     "start_time": "2023-09-07T03:29:24.344825Z"
    }
   },
   "id": "d3e7c9e21dd4790"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import json\n",
    "import re, os\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import USearch\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import utils.paper_utils as pu"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T03:31:18.664990Z",
     "start_time": "2023-09-07T03:31:18.634926Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "## Mapping file.\n",
    "arxiv_code_map = pu.get_arxiv_title_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T03:30:55.922360Z",
     "start_time": "2023-09-07T03:30:55.649447Z"
    }
   },
   "id": "96989ba37e4cd91a"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "split_docs = text_splitter.create_documents([doc_content])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T04:02:31.266725Z",
     "start_time": "2023-08-30T04:02:31.246559Z"
    }
   },
   "id": "612456196a0e1635"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "## Embeddings.\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    "\n",
    "## Store.\n",
    "db = USearch.from_documents(split_docs, embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T04:02:47.069525Z",
     "start_time": "2023-08-30T04:02:37.429553Z"
    }
   },
   "id": "329c88fbc178c89c"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "[(Document(page_content='that reproduces this issue”, the start of a block of code in Markdown, (i.e., ```), and finally the partial code snippet public void test whose role is to induce the LLM to write a test method. TABLE II: Example bug report (Defects4J Math-63). Issue No. MATH-3701 Title NaN in “equals” methods Description In “MathUtils”, some “equals” methods will return true if both argument are NaN. Unless I’m mistaken, this contradicts the IEEE standard. If nobody objects, I’m going to make the changes. Listing 1: Example prompt without examples. 1 # NaN in \"equals\" methods 2 ## Description 3 In \"MathUtils\", some \"equals\" methods will return true if both argument are NaN. 4 Unless I\\'m mistaken, this contradicts the IEEE standard. 5 If nobody objects, I\\'m going to make the changes. 6 7 ## Reproduction 8 >Provide a self-contained example that reproduces this issue. 9 ``` 10 public void test We evaluate a range of variations of this basic prompt. Brown et al. [11] report that LLMs benefit from', metadata={'start_index': 14726}),\n  0.49690473),\n (Document(page_content='LLMs benefit from questionanswer examples provided in the prompt. In our case, this means providing examples of bug reports (questions) and the corresponding bug reproducing tests (answers). With this in mind, we experiment with a varying number of examples, to see whether adding more examples, and whether having examples from within the same project or from other projects, significantly influences performance. As there is no real restriction to the prompt format, we also experiment with providing stack traces for crash bugs (to simulate situations where a stack trace was provided), or providing constructors of the class where the fault is located (to simulate situations where the location of the bug is reported). Our specific template format makes it highly unlikely that prompts we generate exist verbatim within the LLM training data. Further, most reports in practice are only connected to the bug-revealing test via a chain of references. As such, our format partly mitigates data', metadata={'start_index': 15699}),\n  0.5678036),\n (Document(page_content='(Figure 1:(C)). LIBRO subsequently identifies and curates tests that are likely to be bug reproducing, and if so, ranks them to minimize developer inspection effort (Figure 1:(D)). The rest of this section explains each stage in more detail using the running example provided in Table II. A. Prompt Engineering LLMs are, at the core, large autocomplete neural networks: prior work have found that different ways of ‘asking’ the LLM to solve a problem will lead to significantly varying levels of performance [21]. Finding the best query to accomplish the given task is known as prompt engineering [22]. To make an LLM to generate a test method from a given bug report, we construct a Markdown document, which is to be used in the prompt, from the bug report: consider the example in Listing 1, which is a Markdown document constructed from the bug report shown in Table II. LIBRO adds a few distinctive parts to the Markdown document: the command “Provide a self-contained example that reproduces', metadata={'start_index': 13745}),\n  0.58182657),\n (Document(page_content='in bug report and synthesizing code aligned with given a natural language description. VIII. THREATS TO VALIDITY Internal Validity concerns whether our experiments demonstrate causality. In our case, two sources of randomness threat internal validity: the flakiness of tests and the randomness of LLM querying. While we do observe a small number of flaky tests generated, the number of them is significantly smaller (<2%) than the overall number of tests generated, and as such we do not believe their existence significantly affects our conclusions. Meanwhile, we engage with the randomness of the LLM, performing an analysis in RQ2-1. External Validity concerns whether the results presented would generalize. In this case, it is difficult to tell whether the results we presented here would generalize to other Java projects, or projects in other languages. While the uniqueness of our prompts and our use of GHRB cases provide some evidence that LIBRO is not simply relying on the memorization', metadata={'start_index': 53949}),\n  0.62984335)]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search_with_score(\"example prompts\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T04:07:39.365987Z",
     "start_time": "2023-08-30T04:07:39.298786Z"
    }
   },
   "id": "ee91d81a08b7b63a"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "'Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_title"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T04:02:54.602399Z",
     "start_time": "2023-08-30T04:02:54.592718Z"
    }
   },
   "id": "bd6f3aeb4a48429"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a23d4ccfe23856bc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
