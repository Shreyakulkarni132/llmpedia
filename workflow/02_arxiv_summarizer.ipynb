{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-17T04:41:00.400849Z",
     "start_time": "2023-08-17T04:41:00.385141Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summarizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI, AzureChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "import os\n",
    "import json\n",
    "import arxiv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-17T04:55:17.260062Z",
     "start_time": "2023-08-17T04:55:17.218557Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper Funcs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\" Clean and simplify text string. \"\"\"\n",
    "    text = ''.join(c.lower() if c.isalnum() else ' ' for c in text)\n",
    "    return text\n",
    "\n",
    "def reformat_text(doc_content):\n",
    "    content = doc_content.replace('-\\n', '')\n",
    "    content = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', content)\n",
    "    content = re.sub(' +', ' ', content)\n",
    "    return content\n",
    "\n",
    "def tfidf_similarity(title1, title2):\n",
    "    \"\"\" Compute cosine similarity of TF-IDF representation between 2 strings. \"\"\"\n",
    "    title1 = preprocess(title1)\n",
    "    title2 = preprocess(title2)\n",
    "    vectorizer = TfidfVectorizer().fit_transform([title1, title2])\n",
    "    vectors = vectorizer.toarray()\n",
    "    return cosine_similarity(vectors[0:1], vectors[1:2])[0][0]\n",
    "\n",
    "\n",
    "def get_arxiv_info(title):\n",
    "    \"\"\" Search article in Arxiv by name and retrieve meta-data. \"\"\"\n",
    "    search = arxiv.Search(\n",
    "        query=preprocess(title),\n",
    "        max_results=20,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    res = list(search.results())\n",
    "    if len(res) > 0:\n",
    "        ## Sort by title similarity.\n",
    "        res = sorted(res, key=lambda x: tfidf_similarity(title, x.title), reverse=True)\n",
    "        new_title = res[0].title\n",
    "        title_sim = tfidf_similarity(title, new_title)\n",
    "        if title_sim > 0.7:\n",
    "            return res[0]\n",
    "        else:\n",
    "            return None\n",
    "    return None\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-17T04:48:48.727254Z",
     "start_time": "2023-08-17T04:48:48.688714Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LLM Chain Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "## Underlying LLM.\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.1)\n",
    "\n",
    "## Create prompt.\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"content\", \"prev_summary\"],\n",
    "    template=\"\"\"\n",
    "As an applied AI researcher specialized in the field of Large Language Models (LLMs), you are currently conducting a survey of the literature, building a catalogue of the main contributions and innovations of each paper, determining how they can be applied to build systems or create new products. This catalogue will be published by a prestigious organization and will serve as the foundation for all applied LLM knowledge going forward. Now, carefully read the following paper:\n",
    "\n",
    "{content}\n",
    "\n",
    "========================\n",
    "\n",
    "SUMMARY\n",
    "\n",
    "{prev_summary}\n",
    "\n",
    "Upon completion, answer the following questions:\n",
    "\n",
    "1. What is the `main_contribution` of this paper? (1 line headline + 8-12 sentences)\n",
    "    - If a new algorithm or technique is introduced, describe its workings clearly and comprehensively.\n",
    "    - Do not assume that the reader knows terminology not part of the common AI/ML knowledge base.\n",
    "    - Ensure that your answer provides practical insights that offer a solid understanding of the paper.\n",
    "    - Detail the benefits or advantages of what has been presented, along with the practical implications for an LLM practitioner.\n",
    "    - Do not include anything already discussed in the summary or abstract.\n",
    "\n",
    "2. What is the main `takeaway`? (1 line headline + 8-12 sentences)\n",
    "    - Focusing on the paper's contributions, explain how they can be used to create an interesting LLM application, improve current workflows, or increase efficiency when working with LLMs.\n",
    "    - If different models were evaluated and their performance recorded, please note this and its practical implications (in detailed manner, i.e.: which model is best for what).\n",
    "    - Be very precise, practical and specific as possible. Eliminate any irrelevant content from the paper's applied perspective.\n",
    "    - If possible, provide a minimal code example or at least sketch the application.\n",
    "\n",
    "3. Which category best describes this paper's primary focus? Choose one from the following options, with \"OTHER\" being the least desirable choice.\n",
    "    a. \"TRAINING\": Discussions on LLM training methods, technical stack improvements, alternative training routines, etc.\n",
    "    b. \"FINE-TUNING\": Discussions on fine-tuning, re-training, and specialization of LLMs.\n",
    "    c. \"ARCHITECTURES\": Discussions on new LLM architectures, neural network components, etc., excluding prompting or computational systems to manage LLMs.\n",
    "    d. \"PROMPTING\": Discussions on prompting methods, agent architectures, etc.\n",
    "    e. \"USE CASES\": Discussions on LLM use in specific tasks, such as summarization, question answering, stock prediction, etc.\n",
    "    f. \"BEHAVIOR\": Discussions on LLM behavior, including probing, interpretability, risks, biases, emerging abilities, etc.\n",
    "    g. \"OTHER\": None of the above.\n",
    "\n",
    "4. On a scale from 1 to 3, how novel is this paper? (1: not novel, 2: incrementally novel, 3: very novel)\n",
    "    - Compare the paper's findings and contributions with what is presented in previous and related work. How unique and significant are the findings?\n",
    "    - Be strict and rigorous; few papers should receive a high score.\n",
    "    - Pay close attention to the comparison with prior work and the degree of difference in the author's contributions.\n",
    "\n",
    "5. On a scale from 1 to 3, how technical is this paper? (1: not technical, 2: somewhat technical, 3: very technical)\n",
    "    a) A very technical paper is difficult for a non-expert to understand, requires considerable technical knowledge, is filled with equations and jargon, and demands advanced mathematical knowledge.\n",
    "    b) A somewhat technical paper may be challenging for a layman but can be understood reasonably well by someone with a computer science background. These papers, while not overly complex, explain processes in great detail and are practical and applicable (can be replicated).\n",
    "    c) A non-technical paper is understandable for anyone with a college degree. These papers often discuss generalities, and the takeaways are more conceptual than technical.\n",
    "\n",
    "6. On a scale from 1 to 3, how enjoyable is this paper? (1: hard to read, 2: ok, 3: a delight)\n",
    "    a) A very enjoyable paper is well-written, organized, presents a novel and intriguing contribution, and is easy to read.\n",
    "    b) An 'ok' paper is primarily plain and unexciting but is easy to read and contains some interesting parts. Most papers\n",
    "    c) A non-enjoyable paper is difficult to read, poorly written, and lacks meaningful, practical, and insightful content.\n",
    "\n",
    "When assigning numerical ratings consider these guidelines:\n",
    "- Rating 3/3: Only about 20% of papers reach this standard.\n",
    "- Rating 2/3: Most papers (50%) fall into this category.\n",
    "- Rating 1/3: Around 30% of papers belong to this category.\n",
    "\n",
    "Do not repeat the same comments across different answers.\n",
    "\n",
    "Use the JSON format as in the following examples to respond.\n",
    "\n",
    "EXAMPLE 1\n",
    "==========\n",
    "```\n",
    "{{\n",
    "    \"main_contribution\": {{\n",
    "        \"headline\": \"Chain-of-Thought (CoT) boosts LLM accuracy in financial sentiment analysis\",\n",
    "    \"description\": \"The paper introduces the Chain-of-Thought (CoT) prompting technique for Large Language Models (LLMs) specifically targeting financial sentiment analysis. The core of CoT lies in its deviation from direct predictions. Instead, it guides the model to build a sequence of interconnected thoughts leading to an accurate sentiment score. In a comparative study, LLMs equipped with CoT achieved a 94% accuracy, surpassing the established FinBERT's 88% and the naive prompting model's 81%.\"\n",
    "    }},\n",
    "    \"takeaways\": {{\n",
    "        \"headline\": \"CoT opens new, efficient avenues for LLMs in financial analysis\",\n",
    "        \"description\": \"Using the CoT prompting technique, LLMs can achieve enhanced accuracy in financial news sentiment analysis, ultimately refining stock market predictions. This method not only improves prediction accuracy but also renders the model's thought process transparent. When pitted against FinBERT, the LLM with CoT demonstrated superior performance, signaling its potential dominance in financial analysis tasks.\",\n",
    "        \"example\": \"When processing a news snippet like 'Company X has strong Q3 earnings', an LLM with CoT could generate: 'Strong Q3 earnings -> Likely effective management -> Expected investor trust growth -> Potential bullish market -> Possible stock price ascent.' This layered output simplifies decision-making for market analysts.\"\n",
    "    }},\n",
    "    \"category\": \"USE CASES\",\n",
    "    \"novelty_analysis\": \"The paper extends the boundaries of current research by applying LLMs to financial news sentiment analysis. The introduction of the CoT prompting technique, tailored specifically for this application, represents an incremental advancement in the field.\",\n",
    "    \"novelty_score\": 2,\n",
    "    \"technical_analysis\": \"While the paper discusses a computational framework for managing LLM inputs and outputs, it does not delve into complex mathematical theories or algorithms, making it accessible to a wider audience.\",\n",
    "    \"technical_score\": 1,\n",
    "    \"enjoyable_analysis\": \"The engaging narrative style, coupled with practical insights, makes the paper an enjoyable read. It balances technical details with easily digestible information and an interesting practical application.\",\n",
    "    \"enjoyable_score\": 3\n",
    "}}\n",
    "```\n",
    "\n",
    "EXAMPLE 2\n",
    "==========\n",
    "```\n",
    "{{\n",
    "    \"main_contribution\": {{\n",
    "        \"headline\": \"Zero-shot Prompting Technique for GPT-4 Code Interpreter\",\n",
    "        \"description\": \"This paper proposes a zero-shot prompting technique for GPT-4 Code Interpreter that explicitly encourages the use of code for self-verification, which further boosts performance on math reasoning problems. They report a positive correlation between the better performance of GPT4-Code and the higher Code Usage Frequency. Initial experiments show that GPT4-Code achieved a zero-shot accuracy of 69.7% on the MATH dataset which is an improvement of 27.5% over GPT-4’s performance (42.2%).\"\n",
    "    }},\n",
    "    \"takeaways\": {{\n",
    "        \"headline\": \"Leveraging Self-verification and Code Execution in LLMs\",\n",
    "        \"description\": \"Self-verification is already a powerful approach to enhance the performance of LLMs on many tasks but this approach leverages the evaluation of code execution which could make it interesting to solve other kinds of problems. This work highlights the importance of code understanding and generation capabilities in LLMs.\",\n",
    "        \"example\": \"Some of the ideas presented in this paper (specifically, the code-based self-verification and verification-guided weighted majority voting technique) can lead to building high-quality datasets that could potentially help improve the mathematical capabilities in open-source LLMs like Llama 2.\"\n",
    "    }},\n",
    "    \"category\": \"PROMPTING\",\n",
    "    \"novelty_analysis\": \"The research innovatively combines LLMs with code-based self-verification, achieving a 20% boost over state-of-the-art coding task accuracies. This method's practicality is evident, with tests showing a 30% reduction in coding errors, redefining efficiency in LLM-driven code generation.\",\n",
    "    \"novelty_score\": 3,\n",
    "    \"technical_analysis\": \"The paper delve into advanced algorithms, such as the Hypothetical Code-Integration Algorithm (HCIA), making it a dense read for those unfamiliar with theoretical computer science. While the introduction of a novel concept is enlightening, the paper's reliance on complex algorithms, logical proofs and symbolic reasoning makes it a technically advanced read.\",\n",
    "    \"technical_score\": 3,\n",
    "    \"enjoyable_analysis\": \"For those deeply engrossed in the LLM landscape, this paper promises an engaging journey. While its technical nuances can be challenging, the clearly presented transformative results, such as the significant performance leap in the MATH dataset, ensure a gripping narrative.\",\n",
    "    \"enjoyable_score\": 2\n",
    "}}\n",
    "```\n",
    "\n",
    "EXAMPLE 3\n",
    "==========\n",
    "```\n",
    "{{\n",
    "    \"main_contribution\": {{\n",
    "        \"headline\": \"LLMManager: LLM-Driven Database Maintenance Knowledge Acquisition\",\n",
    "        \"description\": \"LLMManager leverages a retriever system paired with a LLM to extract database maintenance knowledge from diverse textual sources. It incorporates a hybrid mechanism that combines transformer-based models with traditional relational database algorithms. The framework's ability to parse vast amounts of text and convert them into actionable database maintenance tasks has led to notable metrics: a 47% increase in real-time database issue detection and a 32% improvement in automated problem resolution compared to existing SotA systems.\"\n",
    "    }},\n",
    "    \"takeaways\": {{\n",
    "        \"headline\": \"Leveraging 'Tree of Thought' Reasoning for Enhanced Maintenance\",\n",
    "        \"description\": \"LLMManager integration of the 'tree of thought' reasoning not only enhances root cause analysis but also creates a dynamic learning environment. Over time, LLMManager ability to revert to prior steps during anomalies becomes more refined, ensuring adaptive and evolving responses to complex database issues. Furthermore, its modular design allows for seamless integration with other LLMs, magnifying the collaborative aspect of the framework.\",\n",
    "        \"example\": \"Automating database maintenance with D-Bot can lead to significant reductions in downtime and costs. Developers could design LLM systems that proactively address issues even before they escalate, unlocking more efficient and streamlined database operations.\"\n",
    "    }},\n",
    "    \"category\": \"USE CASES\",\n",
    "    \"novelty_analysis\": \"D-Bot's utilization of the 'tree of thought' reasoning in database maintenance is novel, although a targeted application inspired by similar work on other engineering areas.\",\n",
    "    \"novelty_score\": 2,\n",
    "    \"technical_analysis\": \"The paper delves into Entity-Relationship Diagrams and database management algorithms essential to LLMManagers's operations. However, it manages to remain accessible, avoiding overly complex jargon and ensuring a broader audience comprehension.\",\n",
    "    \"technical_score\": 2,\n",
    "    \"enjoyable_analysis\": \"The work provides a balanced blend of technical details and real-world applications, giving insights into LLMManager's functions and potential impacts.\",\n",
    "    \"enjoyable_score\": 2\n",
    "}}\n",
    "```\n",
    "\n",
    "EXAMPLE 4\n",
    "==========\n",
    "{{\n",
    "    \"main_contribution\": {{\n",
    "        \"headline\": \"Performance Analysis of LLMs in Entity Recognition\",\n",
    "        \"description\": \"The paper undertakes a systematic comparison of four Large Language Models (LLMs) - GPT-4, Claude, GPT-3.5, and Prodisol-001 - with a focus on entity recognition. Each model was subjected to a consistent dataset, and their entity extraction capabilities were assessed based on precision, recall, and F1 score. Results highlighted that GPT-4 outperformed the other models, with Claude closely following, and GPT-3.5 and Prodisol-001 trailing behind. This comparative study offers insights into the current capabilities of prominent LLMs in the domain of entity recognition.\"\n",
    "    }},\n",
    "    \"takeaways\": {{\n",
    "        \"headline\": \"Entity Recognition Capabilities Vary Across LLMs\",\n",
    "        \"description\": \"The paper underscores variations in the performance of different LLMs when tasked with entity recognition. The presented findings provide a benchmark for professionals and researchers aiming to choose an LLM for entity recognition tasks. The nuanced comparison suggests that while GPT-4 exhibits top-tier performance in this domain, other models like Claude also present strong capabilities.\",\n",
    "        \"example\": \"When parsing a complex news article about the merger between two tech giants, it becomes crucial to accurately recognize and categorize entities such as company names, CEOs, financial figures, and locations. An LLM with superior entity recognition, in such a context, aids in extracting critical data points efficiently, enabling a more thorough analysis of the situation.\"\n",
    "    }},\n",
    "    \"category\": \"USE CASES\",\n",
    "    \"novelty_analysis\": \"The study contributes to existing literature by offering a contemporary comparison of the latest LLMs in entity recognition. While the task itself isn't novel, the inclusion of GPT-4 and Claude in the comparison introduces an incremental advancement to the current body of research.\",\n",
    "    \"novelty_score\": 2,\n",
    "    \"technical_analysis\": \"The paper balances technical depth with accessibility, providing a detailed outline of evaluation metrics and methodologies. This ensures insights are communicated comprehensively, catering to both technical and non-technical readers.\",\n",
    "    \"technical_score\": 2,\n",
    "    \"enjoyable_analysis\": \"Through its well-structured approach and clear visualizations, the paper facilitates an engaging read. The methodical presentation of results aids in drawing comparisons and understanding the landscape of LLMs in entity recognition.\",\n",
    "    \"enjoyable_score\": 2\n",
    "}}\n",
    "```\n",
    "\n",
    "YOUR TURN\n",
    "==========\n",
    "\"\"\"\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-17T04:41:05.111544Z",
     "start_time": "2023-08-17T04:41:05.074284Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Iterate Papers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "paper_names = [\n",
    "    \"Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes\",\n",
    "]\n",
    "\n",
    "failed_papers = []\n",
    "existing_papers = [\"XX\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-17T04:41:06.404300Z",
     "start_time": "2023-08-17T04:41:06.382398Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:31<00:00, 31.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed on Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes:\n",
      "The model `gpt-4-32k` does not exist or you do not have access to it. Learn more: https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    for paper_name in tqdm(paper_names):\n",
    "        ## Get paper.\n",
    "        pre_similarity = max([tfidf_similarity(paper_name, t) for t in existing_papers])\n",
    "        if pre_similarity > 0.9:\n",
    "            continue\n",
    "\n",
    "        ## Load by the force.\n",
    "        try:\n",
    "            docs = ArxivLoader(query=preprocess(paper_name), load_max_docs=10).load()\n",
    "        except:\n",
    "            try:\n",
    "                docs = ArxivLoader(query=preprocess(paper_name), load_max_docs=6).load()\n",
    "            except:\n",
    "                docs = ArxivLoader(query=preprocess(paper_name), load_max_docs=1).load()\n",
    "\n",
    "        if len(docs) == 0:\n",
    "            print(f\"Could not find {paper_name}.\")\n",
    "            failed_papers.append(paper_name)\n",
    "            continue\n",
    "        try:\n",
    "            docs = sorted(docs, key=lambda x: tfidf_similarity(paper_name, x.metadata[\"Title\"]), reverse=True)\n",
    "            new_title = docs[0].metadata[\"Title\"]\n",
    "            title_sim = tfidf_similarity(paper_name, new_title)\n",
    "            if title_sim < 0.7:\n",
    "                print(f\"No similar title name found for {paper_name}.\")\n",
    "                continue\n",
    "            doc_meta = docs[0].metadata\n",
    "            doc_content = docs[0].page_content\n",
    "            doc_content = reformat_text(doc_content)\n",
    "            first_author = doc_meta[\"Authors\"].split(\" \")[0]\n",
    "            published = pd.to_datetime(doc_meta[\"Published\"]).strftime(\"%Y_%m_%d\")\n",
    "            prev_summary = doc_meta[\"Summary\"].replace(\"\\n\", \" \")\n",
    "\n",
    "            ## Name serially.\n",
    "            base_name = f\"{published}_{first_author.lower()}\"\n",
    "            i = 1\n",
    "            if os.path.exists(f\"papers/{base_name}_{str(i).zfill(3)}.json\"):\n",
    "                print(f\"Found locally: {paper_name}:\")\n",
    "                # print(f\"Reworking: {paper_name}.\")\n",
    "                continue\n",
    "\n",
    "            ## Run model.\n",
    "            summary = chain.run({'content': doc_content, \"prev_summary\": prev_summary})\n",
    "\n",
    "            ## Extract and combine results.\n",
    "            parsed_summary = summary.replace(\"```\", \"\").strip()\n",
    "            parsed_summary = json.loads(parsed_summary)\n",
    "\n",
    "            result_dict = {**doc_meta, **parsed_summary}\n",
    "            result_dict[\"Summary\"] = prev_summary\n",
    "\n",
    "            ## Store.\n",
    "            with open(f\"papers/{base_name}_{str(i).zfill(3)}.json\", 'w') as f:\n",
    "                json.dump(result_dict, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed on {paper_name}:\")\n",
    "            print(e)\n",
    "            failed_papers.append(paper_name)\n",
    "            continue"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-17T04:41:39.452369Z",
     "start_time": "2023-08-17T04:41:07.868140Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilling Step-by-Step! Outperforming Larger Language Models\n",
      "with Less Training Data and Smaller Model Sizes\n",
      "Cheng-Yu Hsieh1∗, Chun-Liang Li2, Chih-Kuan Yeh3, Hootan Nakhost2,\n",
      "Yasuhisa Fujii3, Alexander Ratner1, Ranjay Krishna1, Chen-Yu Lee2, Tomas Pfister2\n",
      "1University of Washington, 2Google Cloud AI Research, 3Google Research\n",
      "cydhsieh@cs.washington.edu\n",
      "Abstract\n",
      "Deploying large language models (LLMs) is\n",
      "challenging because they are memory inef-\n",
      "ficient and compute-intensive for practical\n",
      "applications.\n",
      "In reaction, researchers train\n",
      "smaller task-specific models by either finetun-\n",
      "ing with human labels or distilling using LLM-\n",
      "generated labels. However, finetuning and dis-\n",
      "tillation require large amounts of training data\n",
      "to achieve comparable performance to LLMs.\n",
      "We introduce Distilling step-by-step, a new\n",
      "mechanism that (a) trains smaller models that\n",
      "outperform LLMs, and (b) achieves so by lever-\n",
      "aging less training data needed by finetuning\n",
      "or distillation. Our method extracts LLM ra-\n",
      "tionales as additional supervision for training\n",
      "small models within a multi-task framework.\n",
      "We present three findings across 4 NLP bench-\n",
      "marks: First, compared to both finetuning and\n",
      "distillation, our mechanism achieves better per-\n",
      "formance with much fewer labeled/unlabeled\n",
      "training examples. Second, compared to few-\n",
      "shot prompted LLMs, we achieve better perfor-\n",
      "mance using substantially smaller model sizes.\n",
      "Third, we reduce both the model size and the\n",
      "amount of data required to outperform LLMs;\n",
      "our finetuned 770M T5 model outperforms the\n",
      "few-shot prompted 540B PaLM model using\n",
      "only 80% of available data on a benchmark,\n",
      "whereas standard finetuning the same T5 model\n",
      "struggles to match even by using 100% of the\n",
      "dataset.1\n",
      "1\n",
      "Introduction\n",
      "Despite the impressive few-shot ability offered by\n",
      "large language models (LLMs) (Brown et al., 2020;\n",
      "Chowdhery et al., 2022; Thoppilan et al., 2022;\n",
      "Hoffmann et al., 2022; Smith et al., 2022b; Zhang\n",
      "et al., 2022), these models are challenging to de-\n",
      "ploy in real world applications due to their sheer\n",
      "∗Work done while the author was a student researcher at\n",
      "Google Cloud AI Research.\n",
      "1Source code is available at:\n",
      "https://github.com/\n",
      "google-research/distilling-step-by-step.\n",
      "Figure 1: While large language models (LLMs) offer\n",
      "strong zero/few-shot performance, they are challenging\n",
      "to serve in practice. Traditional ways of training small\n",
      "task-specific models, on the other hand, requires large\n",
      "amount of training data. We propose Distilling step-\n",
      "by-step, a new paradigm that extracts rationales from\n",
      "LLMs as informative task knowledge into training small\n",
      "models, which reduces both the deployed model size as\n",
      "well as the data required for training.\n",
      "size. Serving a single 175 billion LLM requires\n",
      "at least 350GB GPU memory using specialized in-\n",
      "frastructure (Zheng et al., 2022). To make matters\n",
      "worse, today’s state-of-the-art LLMs are composed\n",
      "of over 500B parameters (Chowdhery et al., 2022),\n",
      "requiring significantly more memory and compute.\n",
      "Such computational requirements are far beyond\n",
      "affordable for most product teams, especially for\n",
      "applications that require low latency performance.\n",
      "To circumvent these deployment challenges of\n",
      "large models, practitioners often choose to de-\n",
      "ploy smaller specialized models instead. These\n",
      "smaller models are trained using one of two\n",
      "common paradigms:\n",
      "finetuning or distillation.\n",
      "Finetuning updates a pretrained smaller model\n",
      "(e.g.\n",
      "BERT (Devlin et al., 2018) or T5 (Raffel\n",
      "et al., 2020)) using downstream human annotated\n",
      "data (Howard and Ruder, 2018). Distillation trains\n",
      "the same smaller models with labels generated by\n",
      "a larger LLM (Tang et al., 2019; Wang et al., 2021;\n",
      "Smith et al., 2022a; Arora et al., 2022). Unfortu-\n",
      "nately, these paradigms reduce model size at a cost:\n",
      "to achieve comparable performance to LLMs, fine-\n",
      "tuning requires expensive human labels, and dis-\n",
      "arXiv:2305.02301v2  [cs.CL]  5 Jul 2023\n",
      "tillation requires large amounts of unlabeled data\n",
      "which can be hard to obtain (Tang et al., 2019;\n",
      "Liang et al., 2020).\n",
      "In this work, we introduce Distilling step-by-\n",
      "step, a new simple mechanism for training smaller\n",
      "models with less training data. Our mechanism re-\n",
      "duces the amount of training data required for both\n",
      "finetuning and distillation of LLMs into smaller\n",
      "model sizes. Core to our mechanism is changing\n",
      "our perspective from viewing LLMs as a source\n",
      "of noisy labels to viewing them as agents that can\n",
      "reason: LLMs can produce natural language ratio-\n",
      "nales justifying their predicted labels (Wei et al.,\n",
      "2022; Kojima et al., 2022). For example, when\n",
      "asked “Jesse’s room is 11 feet long and 15 feet\n",
      "wide. If she already has 16 square feet of carpet.\n",
      "How much more carpet does she need to cover\n",
      "the whole floor?”, an LLM can be prompted by\n",
      "chain-of-thought (CoT) technique (Wei et al., 2022)\n",
      "to provide intermediate rationales “Area = length\n",
      "× width. Jesse’s room has 11 × 15 square feet.”\n",
      "that better connects the input to the final answer\n",
      "“(11 × 15) − 16”. These rationales can contain\n",
      "relevant task knowledge, such as “Area = length ×\n",
      "width”, that may originally require many data for\n",
      "small task-specific models to learn. We thus utilize\n",
      "these extracted rationales as additional, richer infor-\n",
      "mation to train small models through a multi-task\n",
      "training setup, with both label prediction and ratio-\n",
      "nale prediction tasks (Raffel et al., 2020; Narang\n",
      "et al., 2020).\n",
      "Distilling step-by-step allows us to learn task-\n",
      "specific smaller models that outperform LLMs us-\n",
      "ing over 500× less model parameters, and it does\n",
      "so with far fewer training examples compared to\n",
      "traditional finetuning or distillation (Figure 1). Our\n",
      "results show three promising empirical conclusions\n",
      "across 4 NLP benchmarks. First, compared to both\n",
      "finetuning and distillation, our resulting models\n",
      "achieve better performance with over 50% less\n",
      "training examples on average across datasets (and\n",
      "up to over 85% reduction). Second, our models\n",
      "outperform LLMs with much smaller model sizes\n",
      "(up to 2000× smaller), drastically reducing the\n",
      "computation cost required for model deployment.\n",
      "Third, we simultaneously reduce the model size\n",
      "as well as the amount of data required to outper-\n",
      "form LLMs. We surpass the performance of 540B\n",
      "parameter LLMs using a 770M T5 model; this\n",
      "smaller model only uses 80% of a labeled dataset\n",
      "that would otherwise be required if using an exist-\n",
      "ing finetuning method. When only unlabeled data\n",
      "is present, our small models still perform on par or\n",
      "better than LLMs. We outperform 540B PaLM’s\n",
      "performance with only a 11B T5 model. We further\n",
      "show that when a smaller model performs worse\n",
      "than an LLM, Distilling step-by-step can more effi-\n",
      "ciently leverage additional unlabeled data to match\n",
      "the LLM performance compared to the standard\n",
      "distillation approach.\n",
      "2\n",
      "Related work\n",
      "Our work distills task-specific knowledge of LLMs\n",
      "into smaller specialist models by leveraging the\n",
      "emergent reasoning capabilities of today’s LLMs.\n",
      "We draw on knowledge distillation research and\n",
      "methods that learn from both human-generated ra-\n",
      "tionales and LLM-generated rationales.\n",
      "Knowledge distillation from large models.\n",
      "Knowledge distillation has been successfully used\n",
      "to transfer knowledge from larger, more compe-\n",
      "tent teacher models into smaller student models\n",
      "affordable for practical applications (Buciluˇa et al.,\n",
      "2006; Hinton et al., 2015; Beyer et al., 2022; West\n",
      "et al., 2021; Fu et al., 2023). It supports learning\n",
      "from limited labeled data, since the larger teacher\n",
      "model is often used to generate a training dataset\n",
      "with noisy pseudo labels (Chen et al., 2020; Il-\n",
      "iopoulos et al., 2022; Wang et al., 2021; Smith\n",
      "et al., 2022a; Arora et al., 2022; Agrawal et al.,\n",
      "2022). The one limitation that knowledge distil-\n",
      "lation often faces is its reliance on large amounts\n",
      "of unlabelled data required to create a useful noisy\n",
      "training dataset. Although prior work has explored\n",
      "using data augmentation techniques to reduce this\n",
      "hunger for data (Tang et al., 2019; Liang et al.,\n",
      "2020; Srinivas and Fleuret, 2018; Milli et al., 2019),\n",
      "we propose an alternative approach: we reduce the\n",
      "need for large unlabeled data by distilling not just\n",
      "labels but also the teacher’s rationales.\n",
      "Learning with human rationales.\n",
      "While utiliz-\n",
      "ing LLM-generated rationales is a new exciting\n",
      "area of investigation, using human-generated ratio-\n",
      "nales has a rich history (Hase and Bansal, 2021).\n",
      "For instance, human rationales can be used to reg-\n",
      "ularize model behavior (Ross et al., 2017); it can\n",
      "be used as additional inputs to guide a model’s\n",
      "predictions (Rajani et al., 2019); it can be used to\n",
      "improve overall model performance (Zaidan et al.,\n",
      "2007; Zhang et al., 2016; Camburu et al., 2018;\n",
      "Hancock et al., 2019; Pruthi et al., 2022); and hu-\n",
      "Figure 2: Overview on Distilling step-by-step. We first utilize CoT prompting to extract rationales from an LLM\n",
      "(Section 3.1). We then use the generated rationales to train small task-specific models within a multi-task learning\n",
      "framework where we prepend task prefixes to the input examples and train the model to output differently based on\n",
      "the given task prefix (Section 3.2).\n",
      "man rationales can be used as gold standard labels\n",
      "to make models more interpretable by generating\n",
      "similar rationales (Wiegreffe et al., 2021; Narang\n",
      "et al., 2020; Eisenstein et al., 2022). Unfortunately,\n",
      "human rationales are expensive.\n",
      "Learning with LLM generated rationales.\n",
      "To-\n",
      "day’s LLMs are capable of explaining their pre-\n",
      "dictions by generating high-quality reasoning\n",
      "steps (Wei et al., 2022; Kojima et al., 2022). These\n",
      "reasoning steps have been used to augment input\n",
      "prompts to LLMs, improving their few-shot or zero-\n",
      "shot performance (Wei et al., 2022; Kojima et al.,\n",
      "2022; Wang et al., 2022b); reasoning steps have\n",
      "also been used as additional finetuning data “self-\n",
      "improve” LLMs (Zelikman et al., 2022; Huang\n",
      "et al., 2022). Unfortunately, regardless of how\n",
      "LLMs are improved, their large size limits their\n",
      "utility in most test-time applications.\n",
      "By contrast, we leverage generated rationales\n",
      "as informative supervision to train smaller task-\n",
      "specific models, i.e. models that can be deployed\n",
      "without incurring large computation or memory\n",
      "costs. Several concurrent works have also proposed\n",
      "a similar idea to ours – that of using extracted ra-\n",
      "tionales as supervision (Wang et al., 2022a; Ho\n",
      "et al., 2022; Magister et al., 2022; Li et al., 2023).\n",
      "Amongst them, PINTO (Wang et al., 2022a) relies\n",
      "on an LLM to generate rationales at test-time, and\n",
      "thus does not fully solve deployment challenges.\n",
      "Compared with Ho et al. (2022) and Magister et al.\n",
      "(2022), we go beyond their experiments to provide\n",
      "a granular study by varying training dataset size,\n",
      "exploring downstream model sizes, and demon-\n",
      "strating the effectiveness of our method on fully\n",
      "unlabeled datasets.\n",
      "3\n",
      "Distilling step-by-step\n",
      "We propose a new paradigm, Distilling step-by-\n",
      "step, that leverages the ability of LLMs to reason\n",
      "about their predictions to train smaller models in\n",
      "a data-efficient way. Our overall framework is il-\n",
      "lustrated in Figure 2. Our paradigm has two sim-\n",
      "ple steps: First, given an LLM and an unlabeled\n",
      "dataset, we prompt the LLM to generate output\n",
      "labels along with rationales to justify the labels.\n",
      "Rationales are natural language explanations that\n",
      "provide support for the model’s predicted label\n",
      "(see Figure 2). Second, we leverage these ratio-\n",
      "nales in addition to the task labels to train smaller\n",
      "downstream models. Intuitively, rationales provide\n",
      "richer, more detailed information about why an in-\n",
      "put is mapped to a specific output label, and often\n",
      "contain relevant task knowledge that may be hard\n",
      "to infer solely from the original inputs.\n",
      "3.1\n",
      "Extracting rationales from LLMs\n",
      "Recent studies observe one intriguing emerging\n",
      "property of LLMs: their ability to generate ra-\n",
      "tionales that support their predictions (Wei et al.,\n",
      "2022; Kojima et al., 2022). While the studies have\n",
      "largely focused on how to elicit such reasoning ca-\n",
      "pability from LLMs (Nye et al., 2021; Wei et al.,\n",
      "2022; Kojima et al., 2022), we use them in training\n",
      "smaller downstream models.\n",
      "Specifically, we utilize Chain-of-Thought (CoT)\n",
      "prompting (Wei et al., 2022) to elicit and extract\n",
      "Figure 3: We use few-shot CoT prompting that contains\n",
      "both an example rationale (highlighted in green) and a\n",
      "label (highlighted in blue) to elicit rationales from an\n",
      "LLM on new input examples.\n",
      "rationales from LLMs. As illustrated in Figure 3,\n",
      "given an unlabeled dataset xi ∈ D, we first cu-\n",
      "rate a prompt template p that articulates how the\n",
      "task should be solved. Each prompt is a triplet\n",
      "(xp, rp, yp), where xp is an example input, yp is\n",
      "its corresponding label and rp is a user-provided\n",
      "rationale that explains why xp can be categorized\n",
      "as yp. We append each input xi to p and use it as\n",
      "an input to prompt the LLM to generate rationales\n",
      "and labels for each xi ∈ D. With the demonstra-\n",
      "tions seen in p, the LLM is able to mimic the triplet\n",
      "demonstration to generate the rationale ˆri and out-\n",
      "put ˆyi for xi.\n",
      "3.2\n",
      "Training smaller models with rationales\n",
      "We first describe the current framework for learn-\n",
      "ing task-specific models. With this framework in\n",
      "place, we extend it to incorporate rationales into\n",
      "the training process. Formally, we denote a dataset\n",
      "as D = {(xi, yi)}N\n",
      "i=1 where each xi represents an\n",
      "input and yi is the corresponding desired output\n",
      "label. While our framework supports inputs and\n",
      "outputs of any modality, our experiments limits\n",
      "x and y to be natural language. This text-to-text\n",
      "framework (Raffel et al., 2020) encompasses a va-\n",
      "riety of NLP tasks: classification, natural language\n",
      "inference, question answering and more.\n",
      "Standard finetuning and task distillation.\n",
      "The\n",
      "most common practice to train a task-specific\n",
      "model is to finetune a pretrained model with su-\n",
      "pervised data (Howard and Ruder, 2018). In the\n",
      "absence of human-annotated labels, task-specific\n",
      "distillation (Hinton et al., 2015; Tang et al., 2019)\n",
      "uses LLM teachers to generates pseudo noisy train-\n",
      "ing labels, ˆyi in place of yi (Wang et al., 2021;\n",
      "Smith et al., 2022a; Arora et al., 2022).\n",
      "For both scenarios, the smaller model f is\n",
      "trained to minimize the label prediction loss:\n",
      "Llabel = 1\n",
      "N\n",
      "N\n",
      "�\n",
      "i=1\n",
      "ℓ(f(xi), ˆyi),\n",
      "(1)\n",
      "where ℓ is the cross-entropy loss between the pre-\n",
      "dicted and target tokens. Note that for ease of\n",
      "exposition, we overload ˆyi in Eq. 1 to be either\n",
      "human-annotated labels yi for the standard finetun-\n",
      "ing case, or LLM-predicted labels ˆyi for the model\n",
      "distillation case.\n",
      "Multi-task learning with rationales.\n",
      "To create\n",
      "a more explicit connection between xi’s to ˆyi’s, we\n",
      "use extracted rationales ˆri as additional supervi-\n",
      "sion. There are several ways to incorporate ratio-\n",
      "nales into the downstream model’s training process.\n",
      "One straightforward approach is feed ˆri as an ad-\n",
      "ditional input—as proposed by other concurrent\n",
      "research (Rajani et al., 2019; Wang et al., 2022a).\n",
      "In other words, the f(xi, ˆri) → ˆyi is trained with\n",
      "both text and rationale [x, r] as inputs:\n",
      "L = 1\n",
      "N\n",
      "N\n",
      "�\n",
      "i=1\n",
      "ℓ(f(xi, ˆri), ˆyi).\n",
      "(2)\n",
      "Unfortunately, this design requires an LLM to first\n",
      "generate a rationale before the f can make a pre-\n",
      "diction. The LLM is still necessary during deploy-\n",
      "ment, limited its deployability.\n",
      "In this work, instead of using rationales as ad-\n",
      "ditional model inputs, we frame learning with ra-\n",
      "tionales as a multi-task problem. Specifically, we\n",
      "train the model f(xi) → (ˆyi, ˆri) to not only predict\n",
      "the task labels but also generate the corresponding\n",
      "rationales given the text inputs:\n",
      "L = Llabel + λLrationale,\n",
      "(3)\n",
      "where Llabel is the label prediction loss in Eq. 1\n",
      "and Lrationale is the rationale generation loss:\n",
      "Lrationale = 1\n",
      "N\n",
      "N\n",
      "�\n",
      "i=1\n",
      "ℓ(f(xi), ˆri).\n",
      "(4)\n",
      "The rationale generation loss enables the model to\n",
      "learn to generate the intermediate reasoning steps\n",
      "for the prediction, and could therefore guide the\n",
      "model in better predicting the resultant label. This\n",
      "is our proposed Distilling step-by-step. Compared\n",
      "with Eq. 2, the rationale ˆri is not required in the\n",
      "test time, which removes the need for an LLM at\n",
      "test-time.\n",
      "We\n",
      "prepend\n",
      "“task\n",
      "prefixes”\n",
      "([label],\n",
      "[rationale])\n",
      "to\n",
      "the\n",
      "input\n",
      "examples\n",
      "and\n",
      "train the smaller model to output ˆyi when\n",
      "[label] is provided and to produce ˆri with\n",
      "[rationale] (Raffel et al., 2020).\n",
      "4\n",
      "Experiments\n",
      "We empirically validate the effectiveness of Dis-\n",
      "tilling step-by-step.\n",
      "First, we show that when\n",
      "compared to standard finetuning and task distil-\n",
      "lation approaches, Distilling step-by-step achieves\n",
      "better performance with much fewer number of\n",
      "training examples, substantially improving the\n",
      "data efficiency to learn small task-specific mod-\n",
      "els (Sec. 4.1). Second, we show that Distilling\n",
      "step-by-step surpasses the performance of LLMs\n",
      "with much smaller model size, drastically lowering\n",
      "the deployment cost compared to LLMs (Sec. 4.2).\n",
      "Third, we investigate the minimum resources re-\n",
      "quired, w.r.t. both number of training examples and\n",
      "model size, for Distilling step-by-step to outper-\n",
      "form LLMs. We show that Distilling step-by-step\n",
      "outperforms LLMs by using less data and smaller\n",
      "model, simultaneously improving both data- and\n",
      "deployability-efficiency (Sec. 4.3). Finally, we per-\n",
      "form ablation studies to understand the influence\n",
      "of different components and design choices in the\n",
      "Distilling step-by-step framework (Sec. 4.4).\n",
      "Setup.\n",
      "In the experiments, we consider the 540B\n",
      "PaLM model (Chowdhery et al., 2022) as the LLM.\n",
      "For task-specific downstream models, we use T5\n",
      "models (Raffel et al., 2020) where we initialize the\n",
      "models with pretrained weights obtained from pub-\n",
      "licly available sources2. For CoT prompting, we\n",
      "follow Wei et al. (2022) when available, and curate\n",
      "our own examples for new datasets. We include\n",
      "more implementation details in Appendix A.1.\n",
      "Datasets.\n",
      "We conduct the experiments on 4\n",
      "popular benchmark datasets across 3 different\n",
      "NLP tasks: e-SNLI (Camburu et al., 2018) and\n",
      "ANLI (Nie et al., 2020) for natural language infer-\n",
      "ence; CQA (Talmor et al., 2019; Rajani et al., 2019)\n",
      "for commonsense question answering; SVAMP (Pa-\n",
      "tel et al., 2021) for arithmetic math word problems.\n",
      "We include more dataset details in Appendix A.2.\n",
      "4.1\n",
      "Reducing training data\n",
      "We compare Distilling step-by-step to two most\n",
      "common methods in learning task-specific models:\n",
      "2https://huggingface.co/\n",
      "(1) STANDARD FINETUNING when human-labeled\n",
      "examples are available, and (2) STANDARD TASK\n",
      "DISTILLATION when only unlabeled examples are\n",
      "available. Specifically, standard finetuning refers to\n",
      "the prevailing pretrain-then-finetune paradigm that\n",
      "finetunes a model with ground-truth labels via stan-\n",
      "dard label supervision (Howard and Ruder, 2018).\n",
      "On the other hand, when only unlabeled examples\n",
      "are available, standard task distillation learns the\n",
      "task-specific model by treating a teacher LLM’s\n",
      "predicted labels as ground-truths (Hinton et al.,\n",
      "2015; Chen et al., 2020; Wang et al., 2021; Smith\n",
      "et al., 2022a; Arora et al., 2022).\n",
      "In the following set of experiments, we fix the\n",
      "task-specific models to be 220M T5-Base models,\n",
      "and compare the task performances achieved by dif-\n",
      "ferent methods under varying number of available\n",
      "training examples.\n",
      "Distilling step-by-step outperforms standard\n",
      "finetuning with much less labeled examples.\n",
      "When finetuned with human-labeled examples, Fig-\n",
      "ure 4 shows that Distilling step-by-step consistently\n",
      "achieves better performance than standard finetun-\n",
      "ing across varying numbers of labeled examples\n",
      "used. Furthermore, we see that Distilling step-by-\n",
      "step can achieve the same performance as stan-\n",
      "dard finetuning with much less labeled examples.\n",
      "In particular, by using only 12.5% of the full e-\n",
      "SNLI dataset, Distilling step-by-step can outper-\n",
      "form standard finetuning trained with 100% of the\n",
      "full dataset. Similarly, we achieve 75%, 25%, and\n",
      "20% reduction in training examples required to out-\n",
      "perform standard finetuning on ANLI, CQA, and\n",
      "SVAMP respectively.\n",
      "Distilling step-by-step outperforms standard dis-\n",
      "tillation with much less unlabeled examples.\n",
      "When only unlabeled data is available, we compare\n",
      "Distilling step-by-step to standard task distillation.\n",
      "In Figure 5, we observe an overall similar trend to\n",
      "the finetuning setup. Specifically, we see that Dis-\n",
      "tilling step-by-step outperforms standard task distil-\n",
      "lation on all 4 datasets under different numbers of\n",
      "unlabeled data used. We as well see that Distilling\n",
      "step-by-step requires much less unlabeled data to\n",
      "outperform standard task distillation. For instance,\n",
      "we need only 12.5% of the full unlabeled dataset\n",
      "to outperform the performance achieved by stan-\n",
      "dard task distillation using 100% of the training\n",
      "examples on e-SNLI dataset.\n",
      "Figure 4: We compare Distilling step-by-step and Standard finetuning using 220M T5 models on varying sizes of\n",
      "human-labeled datasets. On all datasets, Distilling step-by-step is able to outperform Standard finetuning, trained on\n",
      "the full dataset, by using much less training examples (e.g., 12.5% of the full e-SNLI dataset).\n",
      "Figure 5: Similar to the plots above, we compare Distilling step-by-step and Standard task distillation using 220M\n",
      "T5 models on varying sizes of unlabeled datasets. Distilling step-by-step is able to outperform Standard task\n",
      "distillation by using only a small subset of the full unlabeled dataset (e.g., 12.5% on ANLI dataset).\n",
      "4.2\n",
      "Reducing model size\n",
      "In the following set of experiments, we hold the\n",
      "training set size fixed (using 100% of the datasets),\n",
      "and compare varying sizes of small T5 models\n",
      "trained with Distilling step-by-step and standard\n",
      "approaches to LLMs. Specifically, we consider 3\n",
      "different sizes of T5 models, i.e., 220M T5-Base,\n",
      "770M T5-Large, and 11B T5-XXL. For LLMs,\n",
      "we include two baseline methods: (1) FEW-SHOT\n",
      "COT (Wei et al., 2022), and (2) PINTO TUN-\n",
      "ING (Wang et al., 2022a). Few-shot CoT directly\n",
      "utilizes CoT demonstrations to prompt the 540B\n",
      "PaLM to generate intermediate steps before pre-\n",
      "dicting the final labels without any further fine-\n",
      "tuning of the LLM. PINTO tuning refers to our\n",
      "extension of Wang et al. (2022a) to handle tasks\n",
      "beyond question-answering, which are not stud-\n",
      "ied by Wang et al. (2022a). Here, we finetune a\n",
      "220M T5-Base model on top of the outputs gener-\n",
      "ated from the PaLM model, which can be viewed\n",
      "as a finetuning method for LLMs with additional\n",
      "parameters (Zhang et al., 2020; Lester et al., 2021).\n",
      "We present the experimental results under the\n",
      "two broad scenarios of having access to labeled\n",
      "datasets or unlabeled datasets in Figure 6 and Fig-\n",
      "ure 7, respectively. We plot each method by their\n",
      "deployed model sizes for prediction (x-axis), and\n",
      "their corresponding task performances (y-axis).\n",
      "Distilling step-by-step improves over standard\n",
      "baselines across varying model sizes used.\n",
      "In\n",
      "Figure 6 and Figure 7 respectively, we see that\n",
      "Distilling step-by-step consistently improves over\n",
      "standard finetuning and standard distillation across\n",
      "all sizes of T5 models. The improvements are most\n",
      "pronounced on ANLI, where Distilling step-by-\n",
      "step outperforms standard finetuning and distilla-\n",
      "tion by an average of 8% and 13% on task accuracy\n",
      "respectively.\n",
      "Distilling step-by-step outperforms LLMs by\n",
      "using much smaller task-specific models.\n",
      "In\n",
      "Figure 6 when human-labeled datasets are avail-\n",
      "able, Distilling step-by-step can always outper-\n",
      "form Few-shot CoT and PINTO tuning on all 4\n",
      "datasets considered, by using much smaller T5\n",
      "models. For instance, we can achieve better perfor-\n",
      "mances than 540B PaLM model’s Few-shot CoT\n",
      "Figure 6: We perform Distilling step-by-step and Standard finetuning, using the full human-labeled datasets, on\n",
      "varying sizes of T5 models and compare their performance to LLM baselines, i.e., Few-shot CoT and PINTO\n",
      "Tuning. Distilling step-by-step is able to outperform LLM baselines by using much smaller models, e.g., over 700×\n",
      "smaller model on ANLI. Standard finetuning fails to match LLM’s performance using the same model size.\n",
      "Figure 7: Using unlabeled datasets, we perform Distilling step-by-step and Standard task distillation on varying\n",
      "sizes of T5 models and compare them to Few-shot CoT. Distilling step-by-step outperforms Few-shot CoT by using\n",
      "2000× smaller models on e-SNLI and 45× smaller models on ANLI and CQA. On SVAMP, by adding unlabeled\n",
      "examples from ASDiv, we close the gap to Few-shot CoT whereas Standard distillation still struggles to catch up.\n",
      "with 220M (over 2000× smaller) T5 model on e-\n",
      "SNLI, 770M (over 700× smaller) T5 models on\n",
      "ANLI and SVAMP, and 11B (over 45× smaller)\n",
      "T5 model on CQA. These results hold true even\n",
      "by further finetuning the 540B PaLM model on\n",
      "available labeled data with PINTO tuning3.\n",
      "In Figure 7, by only utilizing unlabeled exam-\n",
      "ples, Distilling step-by-step also outperforms the\n",
      "teacher LLM on 3 out of 4 datasets. Specifically,\n",
      "Distilling step-by-step surpasses the 540B PaLM\n",
      "model’s Few-shot CoT performance by using 11B\n",
      "T5 with less than 3% of PaLM’s size. On SVAMP\n",
      "where the distilled model underperforms, we hy-\n",
      "pothesize that the performance gap is due to the\n",
      "relatively small number of data points in the dataset\n",
      "(i.e., 800). In reaction, we propose to augment the\n",
      "dataset with additional unlabeled examples to close\n",
      "the performance gap as shown in next.\n",
      "3We note that PETuning methods may outperform PINTO\n",
      "tuning. However, they require massive resource in both train-\n",
      "ing and deployment, which is not the focus of this work.\n",
      "Unlabeled data augmentation further improves\n",
      "Distilling step-by-step.\n",
      "We augment the SVAMP\n",
      "training set with unlabeled examples from the AS-\n",
      "Div dataset (Miao et al., 2020). ASDiv dataset\n",
      "contains a total of 2, 305 examples, where each ex-\n",
      "ample is a math word problem similar to the ones in\n",
      "SVAMP. In Figure 7 on SVAMP, we show the per-\n",
      "formances of Distilling step-by-step and standard\n",
      "task distillation using 11B T5 model after augment-\n",
      "ing the training set with ASDiv. We see the data\n",
      "augmentation much improves the performance for\n",
      "both Distilling step-by-step and standard task dis-\n",
      "tillation. However, even with the added unlabeled\n",
      "examples, standard task distillation still underper-\n",
      "forms Few-shot CoT. On the other hand, Distilling\n",
      "step-by-step is able to much more efficiently ex-\n",
      "ploit the value of the added examples to achieve the\n",
      "same performance level of Few-shot CoT, again,\n",
      "using a T5 model of size less than 3% of the 540B\n",
      "PaLM.\n",
      "Figure 8: We show the minimum size of T5 models and the least amount of human-labeled examples required\n",
      "for Distilling step-by-step to outperform LLM’s Few-shot CoT by a coarse-grained search. Distilling step-by-step\n",
      "is able to outperform Few-shot CoT using not only much smaller models, but it also achieves so with much less\n",
      "training examples compared to Standard finetuning. On ANLI, we outperform the LLM CoT with a 770M model\n",
      "using only 80% of the dataset, whereas Standard finetuning struggles to match even using 100% of the dataset.\n",
      "Figure 9: Similar to Figure 8 but using only unlabeled examples, Distilling step-by-step is able to outperform\n",
      "Few-shot CoT using much smaller models and with much less examples compared to Standard task distillation. On\n",
      "SVAMP, the x-axis corresponds to the size of ASDiv dataset used for augmenting the original SVAMP dataset, i.e.,\n",
      "x = 0 is without augmentation and x = 100 corresponds to adding the full ASDiv dataset.\n",
      "4.3\n",
      "Outperforming LLMs using minimum\n",
      "model size and least training data\n",
      "Here, using the LLM’s performance as an anchor\n",
      "point, we explore the most efficient resource re-\n",
      "quirements in terms of both number of training\n",
      "examples and deployed model size, that Distill-\n",
      "ing step-by-step and standard finetuning/distillation\n",
      "need to outperform the LLM. We present the re-\n",
      "sults, again under human-labeled setting and unla-\n",
      "beled setting, in Figure 8 and Figure 9 respectively.\n",
      "We visualize the results by plotting different resul-\n",
      "tant models by (1) the number of training exam-\n",
      "ples used (x-axis), (2) the final task performance\n",
      "achieved (y-axis), and (3) the size of the model\n",
      "(visualized by the size of the shaded area).\n",
      "Distilling step-by-step outperforms LLMs with\n",
      "much smaller models by using less data.\n",
      "On\n",
      "all datasets in Figure 8, we see that Distilling step-\n",
      "by-step outperforms PaLM’s Few-shot CoT with\n",
      "much smaller T5 models using only a subset of\n",
      "the available training examples. Specifically, on\n",
      "e-SNLI, Distilling step-by-step can achieve bet-\n",
      "ter performance than Few-shot CoT with a model\n",
      "over 2000× smaller (220M T5) and only 0.1% of\n",
      "the full dataset. In Figure 9 where only unlabeled\n",
      "datasets are available, we observe the same trend\n",
      "that Distilling step-by-step can, at most time, out-\n",
      "perform Few-shot CoT with smaller model as well\n",
      "as less data. For instance, on ANLI, Distilling step-\n",
      "by-step outperforms the LLM with a 45× smaller\n",
      "model and 50% of the full unlabeled set.\n",
      "Standard finetuning and distillation require\n",
      "more data and larger model.\n",
      "Finally, in Fig-\n",
      "ure 8 and Figure 9, we see that standard finetuning\n",
      "and distillation often need either more data or larger\n",
      "models to match LLM’s performance. For instance,\n",
      "on e-SNLI in Figure 8, we observe that Distilling\n",
      "step-by-step outperform the LLM using only 0.1%\n",
      "of the dataset while standard finetuning requires\n",
      "more data to match the performance. Furthermore,\n",
      "on ANLI in Figure 8, we observe that Distilling\n",
      "step-by-step can outperform PaLM using 770M\n",
      "model with only 80% of the training set while stan-\n",
      "dard finetuning struggles to match the LLM even\n",
      "Table 1: Distilling step-by-step works with different\n",
      "sizes of LLMs. When rationales are extracted from a\n",
      "20B GPT-NeoX model, Distilling step-by-step is still\n",
      "able to provide performance lift compared to standard\n",
      "finetuning on 220M T5 models.\n",
      "Dataset\n",
      "Method\n",
      "LLM\n",
      "e-SNLI\n",
      "ANLI\n",
      "CQA\n",
      "SVAMP\n",
      "STANDARD FINETUNING\n",
      "N/A\n",
      "88.38\n",
      "43.58\n",
      "62.19\n",
      "62.63\n",
      "DISTILLING STEP-BY-STEP\n",
      "20B\n",
      "89.12\n",
      "48.15\n",
      "63.25\n",
      "63.00\n",
      "DISTILLING STEP-BY-STEP\n",
      "540B\n",
      "89.51\n",
      "49.58\n",
      "63.29\n",
      "65.50\n",
      "using the full dataset and thus requires larger model\n",
      "to close the performance gap.\n",
      "4.4\n",
      "Further ablation studies\n",
      "So far, we have focused on showing the effective-\n",
      "ness of Distilling step-by-step on reducing the train-\n",
      "ing data required for finetuning or distilling smaller\n",
      "task-specific models. In this section, we perform\n",
      "further studies to understand the influence of dif-\n",
      "ferent components in the Distilling step-by-step\n",
      "framework. Specifically, we study (1) how differ-\n",
      "ent LLMs, from which the rationales are extracted,\n",
      "affect the effectiveness of Distilling step-by-step,\n",
      "and (2) how the multi-task training approach com-\n",
      "pares to other potential design choices in training\n",
      "small task-specific models with LLM rationales.\n",
      "Here, we fix the small task-specific models to be\n",
      "220M T5 models, and utilize 100% of the data on\n",
      "all datasets.\n",
      "Distilling step-by-step works with different sizes\n",
      "of decently trained LLMs.\n",
      "In addition to using\n",
      "540B PaLM as the LLM, here we consider a rela-\n",
      "tively smaller LLM, 20B GPT-NeoX model (Black\n",
      "et al., 2022), from which we extract rationales for\n",
      "Distilling step-by-step. In Table 1, we see that\n",
      "when coupled with LLMs of different sizes, Distill-\n",
      "ing step-by-step can still provide performance im-\n",
      "provements compared to standard finetuning. How-\n",
      "ever, the performance lift is smaller when rationales\n",
      "are extracted from the 20B GPT-NeoX model in-\n",
      "stead of from the 540B PaLM. This can be due\n",
      "to the fact that the larger PaLM model provides\n",
      "higher-quality rationales that are more beneficial\n",
      "for learning the task.\n",
      "Multi-task training is much more effective than\n",
      "single-task rationale and label joint prediction.\n",
      "There are different possible ways to train task-\n",
      "specific models with LLM-rationales as output su-\n",
      "pervisions. One straightforward approach is to con-\n",
      "catenate the rationale ˆri and label ˆyi into a single\n",
      "Table 2: Our proposed multi-task training framework\n",
      "consistently leads to better performances than treating\n",
      "rationale and label predictions as a single task. Single-\n",
      "task training can at times lead to worse performance\n",
      "than standard finetuning.\n",
      "Dataset\n",
      "Method\n",
      "e-SNLI\n",
      "ANLI\n",
      "CQA\n",
      "SVAMP\n",
      "STANDARD FINETUNING\n",
      "88.38\n",
      "43.58\n",
      "62.19\n",
      "62.63\n",
      "SINGLE-TASK TRAINING\n",
      "88.88\n",
      "43.50\n",
      "61.37\n",
      "63.00\n",
      "MULTI-TASK TRAINING\n",
      "89.51\n",
      "49.58\n",
      "63.29\n",
      "65.50\n",
      "sequence [ˆri, ˆyi] and treat the entire sequence as\n",
      "the target output in training small models, as con-\n",
      "sidered in (Magister et al., 2022; Ho et al., 2022):\n",
      "Lsingle = 1\n",
      "N\n",
      "N\n",
      "�\n",
      "i=1\n",
      "ℓ(f(xi), [ˆri, ˆyi]).\n",
      "(5)\n",
      "In Table 2, we compare this single-task training\n",
      "approach to our proposed multi-task training ap-\n",
      "proach for utilizing LLM-rationales. We see that\n",
      "not only multi-task training consistently leads to\n",
      "better performance, single-task training with LLM-\n",
      "rationales can at times leads to worse performance\n",
      "than standard finetuning, e.g., on ANLI and CQA.\n",
      "In fact, similar results have also been observed\n",
      "in (Wiegreffe et al., 2021; Magister et al., 2022;\n",
      "Ho et al., 2022) that simply treating rationale and\n",
      "label predictions as a single joint task may harm the\n",
      "model’s performance on label prediction. This val-\n",
      "idates our use of the multi-task training approach,\n",
      "and highlights the need to treat the rationales care-\n",
      "fully so as to unleash their actual benefits.\n",
      "5\n",
      "Discussion\n",
      "We propose Distilling step-by-step to extract ra-\n",
      "tionales from LLMs as informative supervision in\n",
      "training small task-specific models. We show that\n",
      "Distilling step-by-step reduces the training dataset\n",
      "required to curate task-specific smaller models; it\n",
      "also reduces the model size required to achieve,\n",
      "and even surpass, the original LLM’s performance.\n",
      "Distilling step-by-step proposes a resource-efficient\n",
      "training-to-deployment paradigm compared to ex-\n",
      "isting methods. Further studies demonstrate the\n",
      "generalizability and the design choices made in\n",
      "Distilling step-by-step. Finally, we discuss the lim-\n",
      "itations, future directions and ethics statement of\n",
      "our work below.\n",
      "Limitations\n",
      "There are a number of limitations with our ap-\n",
      "proach. First, we require users to produce a few\n",
      "example demonstrations (∼ 10-shot for all tasks)\n",
      "in order to use the few-shot CoT (Wei et al., 2022)\n",
      "prompting mechanism.\n",
      "This limitation can be\n",
      "improved by using recent advances that suggest\n",
      "that rationales can be elicited without any user-\n",
      "annotated demonstrations (Kojima et al., 2022).\n",
      "Second, training task-specific models with ratio-\n",
      "nales incur slight training-time computation over-\n",
      "head. However, at test time, our multi-task design\n",
      "naturally avoids the computation overhead since it\n",
      "allows one to only predict labels without generat-\n",
      "ing the rationales. Finally, while we observe suc-\n",
      "cess using LLM rationales, there is evidence that\n",
      "LLMs exhibit limited reasoning capability on more\n",
      "complex reasoning and planning tasks (Valmeekam\n",
      "et al., 2022). Future work should characterize how\n",
      "rationale quality affects Distilling step-by-step.\n",
      "Ethics statement\n",
      "It is worth noting that the behavior of the our down-\n",
      "stream smaller models is subject to biases inherited\n",
      "from the larger teacher LLM. We envision that the\n",
      "same research progress in reducing anti-social be-\n",
      "haviors in LLMs can also be applied to improve\n",
      "smaller language models.\n",
      "References\n",
      "Priyanka Agrawal, Chris Alberti, Fantine Huot, Joshua\n",
      "Maynez, Ji Ma, Sebastian Ruder, Kuzman Ganchev,\n",
      "Dipanjan Das, and Mirella Lapata. 2022. Qameleon:\n",
      "Multilingual qa with only 5 examples. arXiv preprint\n",
      "arXiv:2211.08264.\n",
      "Simran Arora, Avanika Narayan, Mayee F Chen, Lau-\n",
      "rel J Orr, Neel Guha, Kush Bhatia, Ines Chami, Fred-\n",
      "eric Sala, and Christopher Ré. 2022. Ask me any-\n",
      "thing: A simple strategy for prompting language mod-\n",
      "els. arXiv preprint arXiv:2210.02441.\n",
      "Lucas Beyer, Xiaohua Zhai, Amélie Royer, Larisa Mar-\n",
      "keeva, Rohan Anil, and Alexander Kolesnikov. 2022.\n",
      "Knowledge distillation: A good teacher is patient and\n",
      "consistent. In Proceedings of the IEEE/CVF Confer-\n",
      "ence on Computer Vision and Pattern Recognition,\n",
      "pages 10925–10934.\n",
      "Sid Black, Stella Biderman, Eric Hallahan, Quentin An-\n",
      "thony, Leo Gao, Laurence Golding, Horace He, Con-\n",
      "nor Leahy, Kyle McDonell, Jason Phang, Michael\n",
      "Pieler, USVSN Sai Prashanth, Shivanshu Purohit,\n",
      "Laria Reynolds, Jonathan Tow, Ben Wang, and\n",
      "Samuel Weinbach. 2022. GPT-NeoX-20B: An open-\n",
      "source autoregressive language model. In Proceed-\n",
      "ings of the ACL Workshop on Challenges & Perspec-\n",
      "tives in Creating Large Language Models.\n",
      "Tom Brown, Benjamin Mann, Nick Ryder, Melanie\n",
      "Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\n",
      "Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n",
      "Askell, et al. 2020. Language models are few-shot\n",
      "learners. Advances in neural information processing\n",
      "systems, 33:1877–1901.\n",
      "Cristian Buciluˇa,\n",
      "Rich Caruana,\n",
      "and Alexandru\n",
      "Niculescu-Mizil. 2006. Model compression. In Pro-\n",
      "ceedings of the 12th ACM SIGKDD international\n",
      "conference on Knowledge discovery and data mining,\n",
      "pages 535–541.\n",
      "Oana-Maria Camburu, Tim Rocktäschel, Thomas\n",
      "Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natu-\n",
      "ral language inference with natural language expla-\n",
      "nations. Advances in Neural Information Processing\n",
      "Systems, 31.\n",
      "Ting Chen, Simon Kornblith, Kevin Swersky, Moham-\n",
      "mad Norouzi, and Geoffrey E Hinton. 2020. Big\n",
      "self-supervised models are strong semi-supervised\n",
      "learners. Advances in neural information processing\n",
      "systems, 33:22243–22255.\n",
      "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\n",
      "Maarten Bosma, Gaurav Mishra, Adam Roberts,\n",
      "Paul Barham, Hyung Won Chung, Charles Sutton,\n",
      "Sebastian Gehrmann, et al. 2022. Palm: Scaling\n",
      "language modeling with pathways. arXiv preprint\n",
      "arXiv:2204.02311.\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\n",
      "Kristina Toutanova. 2018. Bert: Pre-training of deep\n",
      "bidirectional transformers for language understand-\n",
      "ing. arXiv preprint arXiv:1810.04805.\n",
      "Jacob Eisenstein, Daniel Andor, Bernd Bohnet, Michael\n",
      "Collins, and David Mimno. 2022. Honest students\n",
      "from untrusted teachers: Learning an interpretable\n",
      "question-answering pipeline from a pretrained lan-\n",
      "guage model. arXiv preprint arXiv:2210.02498.\n",
      "Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and\n",
      "Tushar Khot. 2023. Specializing smaller language\n",
      "models towards multi-step reasoning. arXiv preprint\n",
      "arXiv:2301.12726.\n",
      "Braden Hancock, Antoine Bordes, Pierre-Emmanuel\n",
      "Mazare, and Jason Weston. 2019. Learning from\n",
      "dialogue after deployment: Feed yourself, chatbot!\n",
      "arXiv preprint arXiv:1901.05415.\n",
      "Peter Hase and Mohit Bansal. 2021. When can models\n",
      "learn from explanations? a formal framework for\n",
      "understanding the roles of explanation data. arXiv\n",
      "preprint arXiv:2102.02201.\n",
      "Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015.\n",
      "Distilling the knowledge in a neural network. arXiv\n",
      "preprint arXiv:1503.02531, 2(7).\n",
      "Namgyu Ho, Laura Schmid, and Se-Young Yun. 2022.\n",
      "Large language models are reasoning teachers. arXiv\n",
      "preprint arXiv:2212.10071.\n",
      "Jordan Hoffmann, Sebastian Borgeaud, Arthur Men-\n",
      "sch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\n",
      "ford, Diego de Las Casas, Lisa Anne Hendricks,\n",
      "Johannes Welbl, Aidan Clark, et al. 2022. Train-\n",
      "ing compute-optimal large language models. arXiv\n",
      "preprint arXiv:2203.15556.\n",
      "Jeremy Howard and Sebastian Ruder. 2018. Universal\n",
      "language model fine-tuning for text classification.\n",
      "In Proceedings of the 56th Annual Meeting of the\n",
      "Association for Computational Linguistics (Volume 1:\n",
      "Long Papers), pages 328–339, Melbourne, Australia.\n",
      "Association for Computational Linguistics.\n",
      "Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu,\n",
      "Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.\n",
      "Large language models can self-improve.\n",
      "arXiv\n",
      "preprint arXiv:2210.11610.\n",
      "Fotis Iliopoulos, Vasilis Kontonis, Cenk Baykal, Gau-\n",
      "rav Menghani, Khoa Trinh, and Erik Vee. 2022.\n",
      "Weighted distillation with unlabeled examples. In\n",
      "Advances in Neural Information Processing Systems.\n",
      "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\n",
      "taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\n",
      "guage models are zero-shot reasoners. arXiv preprint\n",
      "arXiv:2205.11916.\n",
      "Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.\n",
      "The power of scale for parameter-efficient prompt\n",
      "tuning. arXiv preprint arXiv:2104.08691.\n",
      "Liunian Harold Li, Jack Hessel, Youngjae Yu, Xi-\n",
      "ang Ren, Kai-Wei Chang, and Yejin Choi. 2023.\n",
      "Symbolic chain-of-thought distillation: Small mod-\n",
      "els can also\" think\" step-by-step.\n",
      "arXiv preprint\n",
      "arXiv:2306.14050.\n",
      "Kevin J Liang, Weituo Hao, Dinghan Shen, Yufan\n",
      "Zhou, Weizhu Chen, Changyou Chen, and Lawrence\n",
      "Carin. 2020.\n",
      "Mixkd: Towards efficient distilla-\n",
      "tion of large-scale language models. arXiv preprint\n",
      "arXiv:2011.00593.\n",
      "Lucie Charlotte Magister, Jonathan Mallinson, Jakub\n",
      "Adamek, Eric Malmi, and Aliaksei Severyn. 2022.\n",
      "Teaching small language models to reason. arXiv\n",
      "preprint arXiv:2212.08410.\n",
      "Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.\n",
      "2020. A diverse corpus for evaluating and developing\n",
      "english math word problem solvers. In Proceedings\n",
      "of the 58th Annual Meeting of the Association for\n",
      "Computational Linguistics, pages 975–984.\n",
      "Smitha Milli, Ludwig Schmidt, Anca D Dragan, and\n",
      "Moritz Hardt. 2019.\n",
      "Model reconstruction from\n",
      "model explanations. In Proceedings of the Confer-\n",
      "ence on Fairness, Accountability, and Transparency,\n",
      "pages 1–9.\n",
      "Sharan Narang, Colin Raffel, Katherine Lee, Adam\n",
      "Roberts, Noah Fiedel, and Karishma Malkan. 2020.\n",
      "Wt5?! training text-to-text models to explain their\n",
      "predictions. arXiv preprint arXiv:2004.14546.\n",
      "Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\n",
      "Jason Weston, and Douwe Kiela. 2020. Adversarial\n",
      "NLI: A new benchmark for natural language under-\n",
      "standing. In Proceedings of the 58th Annual Meeting\n",
      "of the Association for Computational Linguistics. As-\n",
      "sociation for Computational Linguistics.\n",
      "Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\n",
      "Henryk Michalewski, Jacob Austin, David Bieber,\n",
      "David Dohan, Aitor Lewkowycz, Maarten Bosma,\n",
      "David Luan, et al. 2021. Show your work: Scratch-\n",
      "pads for intermediate computation with language\n",
      "models. arXiv preprint arXiv:2112.00114.\n",
      "Arkil Patel, Satwik Bhattamishra, and Navin Goyal.\n",
      "2021. Are NLP models really able to solve simple\n",
      "math word problems? In Proceedings of the 2021\n",
      "Conference of the North American Chapter of the\n",
      "Association for Computational Linguistics: Human\n",
      "Language Technologies, pages 2080–2094, Online.\n",
      "Association for Computational Linguistics.\n",
      "Danish Pruthi,\n",
      "Rachit Bansal,\n",
      "Bhuwan Dhingra,\n",
      "Livio Baldini Soares, Michael Collins, Zachary C\n",
      "Lipton, Graham Neubig, and William W Cohen.\n",
      "2022. Evaluating explanations: How much do ex-\n",
      "planations from the teacher aid students? Transac-\n",
      "tions of the Association for Computational Linguis-\n",
      "tics, 10:359–375.\n",
      "Colin Raffel, Noam Shazeer, Adam Roberts, Kather-\n",
      "ine Lee, Sharan Narang, Michael Matena, Yanqi\n",
      "Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the\n",
      "limits of transfer learning with a unified text-to-text\n",
      "transformer. Journal of Machine Learning Research,\n",
      "21(140):1–67.\n",
      "Nazneen Fatema Rajani, Bryan McCann, Caiming\n",
      "Xiong, and Richard Socher. 2019.\n",
      "Explain your-\n",
      "self! leveraging language models for commonsense\n",
      "reasoning. In Proceedings of the 57th Annual Meet-\n",
      "ing of the Association for Computational Linguistics,\n",
      "pages 4932–4942, Florence, Italy. Association for\n",
      "Computational Linguistics.\n",
      "Andrew Slavin Ross, Michael C Hughes, and Finale\n",
      "Doshi-Velez. 2017. Right for the right reasons: Train-\n",
      "ing differentiable models by constraining their expla-\n",
      "nations. arXiv preprint arXiv:1703.03717.\n",
      "Ryan Smith, Jason A Fries, Braden Hancock, and\n",
      "Stephen H Bach. 2022a. Language models in the\n",
      "loop: Incorporating prompting into weak supervision.\n",
      "arXiv preprint arXiv:2205.02318.\n",
      "Shaden Smith, Mostofa Patwary, Brandon Norick,\n",
      "Patrick LeGresley, Samyam Rajbhandari, Jared\n",
      "Casper, Zhun Liu, Shrimai Prabhumoye, George\n",
      "Zerveas, Vijay Korthikanti, et al. 2022b.\n",
      "Using\n",
      "deepspeed and megatron to train megatron-turing nlg\n",
      "530b, a large-scale generative language model. arXiv\n",
      "preprint arXiv:2201.11990.\n",
      "Suraj Srinivas and François Fleuret. 2018. Knowledge\n",
      "transfer with jacobian matching. In International\n",
      "Conference on Machine Learning, pages 4723–4731.\n",
      "PMLR.\n",
      "Alon Talmor, Jonathan Herzig, Nicholas Lourie, and\n",
      "Jonathan Berant. 2019. CommonsenseQA: A ques-\n",
      "tion answering challenge targeting commonsense\n",
      "knowledge. In Proceedings of the 2019 Conference\n",
      "of the North American Chapter of the Association for\n",
      "Computational Linguistics: Human Language Tech-\n",
      "nologies, Volume 1 (Long and Short Papers), pages\n",
      "4149–4158, Minneapolis, Minnesota. Association for\n",
      "Computational Linguistics.\n",
      "Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga\n",
      "Vechtomova, and Jimmy Lin. 2019. Distilling task-\n",
      "specific knowledge from bert into simple neural net-\n",
      "works. arXiv preprint arXiv:1903.12136.\n",
      "Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\n",
      "Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\n",
      "Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n",
      "2022. Lamda: Language models for dialog applica-\n",
      "tions. arXiv preprint arXiv:2201.08239.\n",
      "Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan,\n",
      "and Subbarao Kambhampati. 2022. Large language\n",
      "models still can’t plan (a benchmark for llms on plan-\n",
      "ning and reasoning about change). arXiv preprint\n",
      "arXiv:2206.10498.\n",
      "Peifeng Wang, Aaron Chan, Filip Ilievski, Muhao Chen,\n",
      "and Xiang Ren. 2022a. Pinto: Faithful language\n",
      "reasoning using prompt-generated rationales. arXiv\n",
      "preprint arXiv:2211.01562.\n",
      "Shuohang Wang, Yang Liu, Yichong Xu, Chenguang\n",
      "Zhu, and Michael Zeng. 2021.\n",
      "Want to reduce\n",
      "labeling cost?\n",
      "gpt-3 can help.\n",
      "arXiv preprint\n",
      "arXiv:2108.13487.\n",
      "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\n",
      "Ed Chi, and Denny Zhou. 2022b. Self-consistency\n",
      "improves chain of thought reasoning in language\n",
      "models. arXiv preprint arXiv:2203.11171.\n",
      "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\n",
      "Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\n",
      "Chain of thought prompting elicits reasoning in large\n",
      "language models. arXiv preprint arXiv:2201.11903.\n",
      "Peter West, Chandra Bhagavatula, Jack Hessel, Jena D\n",
      "Hwang, Liwei Jiang, Ronan Le Bras, Ximing\n",
      "Lu, Sean Welleck, and Yejin Choi. 2021.\n",
      "Sym-\n",
      "bolic knowledge distillation: from general language\n",
      "models to commonsense models.\n",
      "arXiv preprint\n",
      "arXiv:2110.07178.\n",
      "Sarah Wiegreffe, Ana Marasovi´c, and Noah A. Smith.\n",
      "2021.\n",
      "Measuring association between labels and\n",
      "free-text rationales. In Proceedings of the 2021 Con-\n",
      "ference on Empirical Methods in Natural Language\n",
      "Processing, pages 10266–10284, Online and Punta\n",
      "Cana, Dominican Republic. Association for Compu-\n",
      "tational Linguistics.\n",
      "Omar Zaidan, Jason Eisner, and Christine Piatko. 2007.\n",
      "Using “annotator rationales” to improve machine\n",
      "learning for text categorization. In Human Language\n",
      "Technologies 2007: The Conference of the North\n",
      "American Chapter of the Association for Computa-\n",
      "tional Linguistics; Proceedings of the Main Confer-\n",
      "ence, pages 260–267, Rochester, New York. Associa-\n",
      "tion for Computational Linguistics.\n",
      "Eric Zelikman, Yuhuai Wu, and Noah D Goodman.\n",
      "2022. Star: Bootstrapping reasoning with reason-\n",
      "ing. arXiv preprint arXiv:2203.14465.\n",
      "Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas\n",
      "Guibas, and Jitendra Malik. 2020. Side-tuning: a\n",
      "baseline for network adaptation via additive side net-\n",
      "works. In European Conference on Computer Vision,\n",
      "pages 698–714. Springer.\n",
      "Susan Zhang, Stephen Roller, Naman Goyal, Mikel\n",
      "Artetxe, Moya Chen, Shuohui Chen, Christopher De-\n",
      "wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\n",
      "Opt: Open pre-trained transformer language models.\n",
      "arXiv preprint arXiv:2205.01068.\n",
      "Ye Zhang, Iain Marshall, and Byron C. Wallace. 2016.\n",
      "Rationale-augmented convolutional neural networks\n",
      "for text classification. In Proceedings of the 2016\n",
      "Conference on Empirical Methods in Natural Lan-\n",
      "guage Processing, pages 795–804, Austin, Texas.\n",
      "Association for Computational Linguistics.\n",
      "Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao\n",
      "Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang,\n",
      "Yuanzhong Xu, Danyang Zhuo, Joseph E Gonza-\n",
      "lez, et al. 2022. Alpa: Automating inter-and intra-\n",
      "operator parallelism for distributed deep learning.\n",
      "arXiv preprint arXiv:2201.12023.\n",
      "A\n",
      "Experiment detail\n",
      "A.1\n",
      "Implementation\n",
      "We perform our experiments on cloud A100×16\n",
      "GPU instances.\n",
      "We train the T5 models with\n",
      "the following hyperparameters, using publicly\n",
      "available packages from https://github.com/\n",
      "huggingface/transformers:\n",
      "• T5-Base (220M) and T5-Large (770M): We\n",
      "train the models with learning rate = 5 ×\n",
      "10−5, batch size = 64, max input length =\n",
      "1024, for a maximum of 10000 steps.\n",
      "• T5-XXL (11B): We train the models with\n",
      "learning rate = 5 × 10−5, batch size = 32,\n",
      "max input length = 1024, for a maximum of\n",
      "4000 steps.\n",
      "We report all the results over 4 random runs, and\n",
      "include the standard error in the presented plots.\n",
      "A.2\n",
      "Datasets\n",
      "We provide more detailed descriptions on the\n",
      "datasets used in our experiments. We include the\n",
      "sources from which we obtain the datasets as well\n",
      "as their original sources released from the authors.\n",
      "We refer readers to these sources for their license or\n",
      "terms for use and/or distribution. To the best of our\n",
      "knowledge, the datasets used do not contain infor-\n",
      "mation that names or uniquely identifies individual\n",
      "people or offensive content.\n",
      "• e-SNLI: The dataset was originally re-\n",
      "leased in (Camburu et al., 2018), and made\n",
      "publicly available at https://github.com/\n",
      "OanaMariaCamburu/e-SNLI.\n",
      "We obtain\n",
      "the dataset from https://huggingface.co/\n",
      "datasets/esnli.\n",
      "• ANLI: The dataset was originally released\n",
      "in (Nie et al., 2020),\n",
      "and made pub-\n",
      "licly available at\n",
      "https://github.com/\n",
      "facebookresearch/anli.\n",
      "We obtain the\n",
      "dataset\n",
      "from\n",
      "https://huggingface.co/\n",
      "datasets/anli. We use the R1 split in our\n",
      "experiments.\n",
      "• CQA: The dataset was originally released\n",
      "in (Talmor et al., 2019), and made publicly\n",
      "available at https://www.tau-nlp.sites.\n",
      "tau.ac.il/commonsenseqa.\n",
      "It was then\n",
      "augmented with human-labeled explanations\n",
      "Table 3: Dataset statistics used in our experiments.\n",
      "Dataset\n",
      "Train\n",
      "Validation\n",
      "Test\n",
      "e-SNLI\n",
      "549,367\n",
      "9,842\n",
      "9,824\n",
      "ANLI\n",
      "16,946\n",
      "1,000\n",
      "1,000\n",
      "CQA\n",
      "8,766\n",
      "975\n",
      "1,221\n",
      "SVAMP\n",
      "720\n",
      "80\n",
      "200\n",
      "by (Rajani et al., 2019), which is avail-\n",
      "able at https://github.com/salesforce/\n",
      "cos-e. We obtain the dataset used in our ex-\n",
      "periments from https://huggingface.co/\n",
      "datasets/cos_e.\n",
      "• SVAMP: The dataset was originally re-\n",
      "leased in (Patel et al., 2021).\n",
      "We ob-\n",
      "tain the dataset from https://github.com/\n",
      "arkilpatel/SVAMP.\n",
      "• ASDiv:\n",
      "The dataset was originally re-\n",
      "leased in (Miao et al., 2020).\n",
      "We ob-\n",
      "tain the dataset from https://github.com/\n",
      "chaochun/nlu-asdiv-dataset.\n",
      "For each dataset, we randomly subsample 10%\n",
      "of the original training set to serve as validation set\n",
      "when validation set is not originally provided. For\n",
      "CQA, we use the original validation set to serve\n",
      "as our test set since the ground-truth labels are not\n",
      "available for the original test set. We provide the\n",
      "dataset statistics in Table 3.\n"
     ]
    }
   ],
   "source": [
    "print(doc_content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-17T04:41:39.501811Z",
     "start_time": "2023-08-17T04:41:39.446402Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes Cheng-Yu Hsieh1∗, Chun-Liang Li2, Chih-Kuan Yeh3, Hootan Nakhost2, Yasuhisa Fujii3, Alexander Ratner1, Ranjay Krishna1, Chen-Yu Lee2, Tomas Pfister2 1University of Washington, 2Google Cloud AI Research, 3Google Research cydhsieh@cs.washington.edu Abstract Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLMgenerated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for training small models within a multi-task framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to fewshot prompted LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our finetuned 770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80% of available data on a benchmark, whereas standard finetuning the same T5 model struggles to match even by using 100% of the dataset.1 1 Introduction Despite the impressive few-shot ability offered by large language models (LLMs) (Brown et al., 2020; Chowdhery et al., 2022; Thoppilan et al., 2022; Hoffmann et al., 2022; Smith et al., 2022b; Zhang et al., 2022), these models are challenging to deploy in real world applications due to their sheer ∗Work done while the author was a student researcher at Google Cloud AI Research. 1Source code is available at: https://github.com/ google-research/distilling-step-by-step. Figure 1: While large language models (LLMs) offer strong zero/few-shot performance, they are challenging to serve in practice. Traditional ways of training small task-specific models, on the other hand, requires large amount of training data. We propose Distilling stepby-step, a new paradigm that extracts rationales from LLMs as informative task knowledge into training small models, which reduces both the deployed model size as well as the data required for training. size. Serving a single 175 billion LLM requires at least 350GB GPU memory using specialized infrastructure (Zheng et al., 2022). To make matters worse, today’s state-of-the-art LLMs are composed of over 500B parameters (Chowdhery et al., 2022), requiring significantly more memory and compute. Such computational requirements are far beyond affordable for most product teams, especially for applications that require low latency performance. To circumvent these deployment challenges of large models, practitioners often choose to deploy smaller specialized models instead. These smaller models are trained using one of two common paradigms: finetuning or distillation. Finetuning updates a pretrained smaller model (e.g. BERT (Devlin et al., 2018) or T5 (Raffel et al., 2020)) using downstream human annotated data (Howard and Ruder, 2018). Distillation trains the same smaller models with labels generated by a larger LLM (Tang et al., 2019; Wang et al., 2021; Smith et al., 2022a; Arora et al., 2022). Unfortunately, these paradigms reduce model size at a cost: to achieve comparable performance to LLMs, finetuning requires expensive human labels, and disarXiv:2305.02301v2 [cs.CL] 5 Jul 2023 tillation requires large amounts of unlabeled data which can be hard to obtain (Tang et al., 2019; Liang et al., 2020). In this work, we introduce Distilling step-bystep, a new simple mechanism for training smaller models with less training data. Our mechanism reduces the amount of training data required for both finetuning and distillation of LLMs into smaller model sizes. Core to our mechanism is changing our perspective from viewing LLMs as a source of noisy labels to viewing them as agents that can reason: LLMs can produce natural language rationales justifying their predicted labels (Wei et al., 2022; Kojima et al., 2022). For example, when asked “Jesse’s room is 11 feet long and 15 feet wide. If she already has 16 square feet of carpet. How much more carpet does she need to cover the whole floor?”, an LLM can be prompted by chain-of-thought (CoT) technique (Wei et al., 2022) to provide intermediate rationales “Area = length × width. Jesse’s room has 11 × 15 square feet.” that better connects the input to the final answer “(11 × 15) − 16”. These rationales can contain relevant task knowledge, such as “Area = length × width”, that may originally require many data for small task-specific models to learn. We thus utilize these extracted rationales as additional, richer information to train small models through a multi-task training setup, with both label prediction and rationale prediction tasks (Raffel et al., 2020; Narang et al., 2020). Distilling step-by-step allows us to learn taskspecific smaller models that outperform LLMs using over 500× less model parameters, and it does so with far fewer training examples compared to traditional finetuning or distillation (Figure 1). Our results show three promising empirical conclusions across 4 NLP benchmarks. First, compared to both finetuning and distillation, our resulting models achieve better performance with over 50% less training examples on average across datasets (and up to over 85% reduction). Second, our models outperform LLMs with much smaller model sizes (up to 2000× smaller), drastically reducing the computation cost required for model deployment. Third, we simultaneously reduce the model size as well as the amount of data required to outperform LLMs. We surpass the performance of 540B parameter LLMs using a 770M T5 model; this smaller model only uses 80% of a labeled dataset that would otherwise be required if using an existing finetuning method. When only unlabeled data is present, our small models still perform on par or better than LLMs. We outperform 540B PaLM’s performance with only a 11B T5 model. We further show that when a smaller model performs worse than an LLM, Distilling step-by-step can more efficiently leverage additional unlabeled data to match the LLM performance compared to the standard distillation approach. 2 Related work Our work distills task-specific knowledge of LLMs into smaller specialist models by leveraging the emergent reasoning capabilities of today’s LLMs. We draw on knowledge distillation research and methods that learn from both human-generated rationales and LLM-generated rationales. Knowledge distillation from large models. Knowledge distillation has been successfully used to transfer knowledge from larger, more competent teacher models into smaller student models affordable for practical applications (Buciluˇa et al., 2006; Hinton et al., 2015; Beyer et al., 2022; West et al., 2021; Fu et al., 2023). It supports learning from limited labeled data, since the larger teacher model is often used to generate a training dataset with noisy pseudo labels (Chen et al., 2020; Iliopoulos et al., 2022; Wang et al., 2021; Smith et al., 2022a; Arora et al., 2022; Agrawal et al., 2022). The one limitation that knowledge distillation often faces is its reliance on large amounts of unlabelled data required to create a useful noisy training dataset. Although prior work has explored using data augmentation techniques to reduce this hunger for data (Tang et al., 2019; Liang et al., 2020; Srinivas and Fleuret, 2018; Milli et al., 2019), we propose an alternative approach: we reduce the need for large unlabeled data by distilling not just labels but also the teacher’s rationales. Learning with human rationales. While utilizing LLM-generated rationales is a new exciting area of investigation, using human-generated rationales has a rich history (Hase and Bansal, 2021). For instance, human rationales can be used to regularize model behavior (Ross et al., 2017); it can be used as additional inputs to guide a model’s predictions (Rajani et al., 2019); it can be used to improve overall model performance (Zaidan et al., 2007; Zhang et al., 2016; Camburu et al., 2018; Hancock et al., 2019; Pruthi et al., 2022); and huFigure 2: Overview on Distilling step-by-step. We first utilize CoT prompting to extract rationales from an LLM (Section 3.1). We then use the generated rationales to train small task-specific models within a multi-task learning framework where we prepend task prefixes to the input examples and train the model to output differently based on the given task prefix (Section 3.2). man rationales can be used as gold standard labels to make models more interpretable by generating similar rationales (Wiegreffe et al., 2021; Narang et al., 2020; Eisenstein et al., 2022). Unfortunately, human rationales are expensive. Learning with LLM generated rationales. Today’s LLMs are capable of explaining their predictions by generating high-quality reasoning steps (Wei et al., 2022; Kojima et al., 2022). These reasoning steps have been used to augment input prompts to LLMs, improving their few-shot or zeroshot performance (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022b); reasoning steps have also been used as additional finetuning data “selfimprove” LLMs (Zelikman et al., 2022; Huang et al., 2022). Unfortunately, regardless of how LLMs are improved, their large size limits their utility in most test-time applications. By contrast, we leverage generated rationales as informative supervision to train smaller taskspecific models, i.e. models that can be deployed without incurring large computation or memory costs. Several concurrent works have also proposed a similar idea to ours – that of using extracted rationales as supervision (Wang et al., 2022a; Ho et al., 2022; Magister et al., 2022; Li et al., 2023). Amongst them, PINTO (Wang et al., 2022a) relies on an LLM to generate rationales at test-time, and thus does not fully solve deployment challenges. Compared with Ho et al. (2022) and Magister et al. (2022), we go beyond their experiments to provide a granular study by varying training dataset size, exploring downstream model sizes, and demonstrating the effectiveness of our method on fully unlabeled datasets. 3 Distilling step-by-step We propose a new paradigm, Distilling step-bystep, that leverages the ability of LLMs to reason about their predictions to train smaller models in a data-efficient way. Our overall framework is illustrated in Figure 2. Our paradigm has two simple steps: First, given an LLM and an unlabeled dataset, we prompt the LLM to generate output labels along with rationales to justify the labels. Rationales are natural language explanations that provide support for the model’s predicted label (see Figure 2). Second, we leverage these rationales in addition to the task labels to train smaller downstream models. Intuitively, rationales provide richer, more detailed information about why an input is mapped to a specific output label, and often contain relevant task knowledge that may be hard to infer solely from the original inputs. 3.1 Extracting rationales from LLMs Recent studies observe one intriguing emerging property of LLMs: their ability to generate rationales that support their predictions (Wei et al., 2022; Kojima et al., 2022). While the studies have largely focused on how to elicit such reasoning capability from LLMs (Nye et al., 2021; Wei et al., 2022; Kojima et al., 2022), we use them in training smaller downstream models. Specifically, we utilize Chain-of-Thought (CoT) prompting (Wei et al., 2022) to elicit and extract Figure 3: We use few-shot CoT prompting that contains both an example rationale (highlighted in green) and a label (highlighted in blue) to elicit rationales from an LLM on new input examples. rationales from LLMs. As illustrated in Figure 3, given an unlabeled dataset xi ∈ D, we first curate a prompt template p that articulates how the task should be solved. Each prompt is a triplet (xp, rp, yp), where xp is an example input, yp is its corresponding label and rp is a user-provided rationale that explains why xp can be categorized as yp. We append each input xi to p and use it as an input to prompt the LLM to generate rationales and labels for each xi ∈ D. With the demonstrations seen in p, the LLM is able to mimic the triplet demonstration to generate the rationale ˆri and output ˆyi for xi. 3.2 Training smaller models with rationales We first describe the current framework for learning task-specific models. With this framework in place, we extend it to incorporate rationales into the training process. Formally, we denote a dataset as D = {(xi, yi)}N i=1 where each xi represents an input and yi is the corresponding desired output label. While our framework supports inputs and outputs of any modality, our experiments limits x and y to be natural language. This text-to-text framework (Raffel et al., 2020) encompasses a variety of NLP tasks: classification, natural language inference, question answering and more. Standard finetuning and task distillation. The most common practice to train a task-specific model is to finetune a pretrained model with supervised data (Howard and Ruder, 2018). In the absence of human-annotated labels, task-specific distillation (Hinton et al., 2015; Tang et al., 2019) uses LLM teachers to generates pseudo noisy training labels, ˆyi in place of yi (Wang et al., 2021; Smith et al., 2022a; Arora et al., 2022). For both scenarios, the smaller model f is trained to minimize the label prediction loss: Llabel = 1 N N � i=1 ℓ(f(xi), ˆyi), (1) where ℓ is the cross-entropy loss between the predicted and target tokens. Note that for ease of exposition, we overload ˆyi in Eq. 1 to be either human-annotated labels yi for the standard finetuning case, or LLM-predicted labels ˆyi for the model distillation case. Multi-task learning with rationales. To create a more explicit connection between xi’s to ˆyi’s, we use extracted rationales ˆri as additional supervision. There are several ways to incorporate rationales into the downstream model’s training process. One straightforward approach is feed ˆri as an additional input—as proposed by other concurrent research (Rajani et al., 2019; Wang et al., 2022a). In other words, the f(xi, ˆri) → ˆyi is trained with both text and rationale [x, r] as inputs: L = 1 N N � i=1 ℓ(f(xi, ˆri), ˆyi). (2) Unfortunately, this design requires an LLM to first generate a rationale before the f can make a prediction. The LLM is still necessary during deployment, limited its deployability. In this work, instead of using rationales as additional model inputs, we frame learning with rationales as a multi-task problem. Specifically, we train the model f(xi) → (ˆyi, ˆri) to not only predict the task labels but also generate the corresponding rationales given the text inputs: L = Llabel + λLrationale, (3) where Llabel is the label prediction loss in Eq. 1 and Lrationale is the rationale generation loss: Lrationale = 1 N N � i=1 ℓ(f(xi), ˆri). (4) The rationale generation loss enables the model to learn to generate the intermediate reasoning steps for the prediction, and could therefore guide the model in better predicting the resultant label. This is our proposed Distilling step-by-step. Compared with Eq. 2, the rationale ˆri is not required in the test time, which removes the need for an LLM at test-time. We prepend “task prefixes” ([label], [rationale]) to the input examples and train the smaller model to output ˆyi when [label] is provided and to produce ˆri with [rationale] (Raffel et al., 2020). 4 Experiments We empirically validate the effectiveness of Distilling step-by-step. First, we show that when compared to standard finetuning and task distillation approaches, Distilling step-by-step achieves better performance with much fewer number of training examples, substantially improving the data efficiency to learn small task-specific models (Sec. 4.1). Second, we show that Distilling step-by-step surpasses the performance of LLMs with much smaller model size, drastically lowering the deployment cost compared to LLMs (Sec. 4.2). Third, we investigate the minimum resources required, w.r.t. both number of training examples and model size, for Distilling step-by-step to outperform LLMs. We show that Distilling step-by-step outperforms LLMs by using less data and smaller model, simultaneously improving both data- and deployability-efficiency (Sec. 4.3). Finally, we perform ablation studies to understand the influence of different components and design choices in the Distilling step-by-step framework (Sec. 4.4). Setup. In the experiments, we consider the 540B PaLM model (Chowdhery et al., 2022) as the LLM. For task-specific downstream models, we use T5 models (Raffel et al., 2020) where we initialize the models with pretrained weights obtained from publicly available sources2. For CoT prompting, we follow Wei et al. (2022) when available, and curate our own examples for new datasets. We include more implementation details in Appendix A.1. Datasets. We conduct the experiments on 4 popular benchmark datasets across 3 different NLP tasks: e-SNLI (Camburu et al., 2018) and ANLI (Nie et al., 2020) for natural language inference; CQA (Talmor et al., 2019; Rajani et al., 2019) for commonsense question answering; SVAMP (Patel et al., 2021) for arithmetic math word problems. We include more dataset details in Appendix A.2. 4.1 Reducing training data We compare Distilling step-by-step to two most common methods in learning task-specific models: 2https://huggingface.co/ (1) STANDARD FINETUNING when human-labeled examples are available, and (2) STANDARD TASK DISTILLATION when only unlabeled examples are available. Specifically, standard finetuning refers to the prevailing pretrain-then-finetune paradigm that finetunes a model with ground-truth labels via standard label supervision (Howard and Ruder, 2018). On the other hand, when only unlabeled examples are available, standard task distillation learns the task-specific model by treating a teacher LLM’s predicted labels as ground-truths (Hinton et al., 2015; Chen et al., 2020; Wang et al., 2021; Smith et al., 2022a; Arora et al., 2022). In the following set of experiments, we fix the task-specific models to be 220M T5-Base models, and compare the task performances achieved by different methods under varying number of available training examples. Distilling step-by-step outperforms standard finetuning with much less labeled examples. When finetuned with human-labeled examples, Figure 4 shows that Distilling step-by-step consistently achieves better performance than standard finetuning across varying numbers of labeled examples used. Furthermore, we see that Distilling step-bystep can achieve the same performance as standard finetuning with much less labeled examples. In particular, by using only 12.5% of the full eSNLI dataset, Distilling step-by-step can outperform standard finetuning trained with 100% of the full dataset. Similarly, we achieve 75%, 25%, and 20% reduction in training examples required to outperform standard finetuning on ANLI, CQA, and SVAMP respectively. Distilling step-by-step outperforms standard distillation with much less unlabeled examples. When only unlabeled data is available, we compare Distilling step-by-step to standard task distillation. In Figure 5, we observe an overall similar trend to the finetuning setup. Specifically, we see that Distilling step-by-step outperforms standard task distillation on all 4 datasets under different numbers of unlabeled data used. We as well see that Distilling step-by-step requires much less unlabeled data to outperform standard task distillation. For instance, we need only 12.5% of the full unlabeled dataset to outperform the performance achieved by standard task distillation using 100% of the training examples on e-SNLI dataset. Figure 4: We compare Distilling step-by-step and Standard finetuning using 220M T5 models on varying sizes of human-labeled datasets. On all datasets, Distilling step-by-step is able to outperform Standard finetuning, trained on the full dataset, by using much less training examples (e.g., 12.5% of the full e-SNLI dataset). Figure 5: Similar to the plots above, we compare Distilling step-by-step and Standard task distillation using 220M T5 models on varying sizes of unlabeled datasets. Distilling step-by-step is able to outperform Standard task distillation by using only a small subset of the full unlabeled dataset (e.g., 12.5% on ANLI dataset). 4.2 Reducing model size In the following set of experiments, we hold the training set size fixed (using 100% of the datasets), and compare varying sizes of small T5 models trained with Distilling step-by-step and standard approaches to LLMs. Specifically, we consider 3 different sizes of T5 models, i.e., 220M T5-Base, 770M T5-Large, and 11B T5-XXL. For LLMs, we include two baseline methods: (1) FEW-SHOT COT (Wei et al., 2022), and (2) PINTO TUNING (Wang et al., 2022a). Few-shot CoT directly utilizes CoT demonstrations to prompt the 540B PaLM to generate intermediate steps before predicting the final labels without any further finetuning of the LLM. PINTO tuning refers to our extension of Wang et al. (2022a) to handle tasks beyond question-answering, which are not studied by Wang et al. (2022a). Here, we finetune a 220M T5-Base model on top of the outputs generated from the PaLM model, which can be viewed as a finetuning method for LLMs with additional parameters (Zhang et al., 2020; Lester et al., 2021). We present the experimental results under the two broad scenarios of having access to labeled datasets or unlabeled datasets in Figure 6 and Figure 7, respectively. We plot each method by their deployed model sizes for prediction (x-axis), and their corresponding task performances (y-axis). Distilling step-by-step improves over standard baselines across varying model sizes used. In Figure 6 and Figure 7 respectively, we see that Distilling step-by-step consistently improves over standard finetuning and standard distillation across all sizes of T5 models. The improvements are most pronounced on ANLI, where Distilling step-bystep outperforms standard finetuning and distillation by an average of 8% and 13% on task accuracy respectively. Distilling step-by-step outperforms LLMs by using much smaller task-specific models. In Figure 6 when human-labeled datasets are available, Distilling step-by-step can always outperform Few-shot CoT and PINTO tuning on all 4 datasets considered, by using much smaller T5 models. For instance, we can achieve better performances than 540B PaLM model’s Few-shot CoT Figure 6: We perform Distilling step-by-step and Standard finetuning, using the full human-labeled datasets, on varying sizes of T5 models and compare their performance to LLM baselines, i.e., Few-shot CoT and PINTO Tuning. Distilling step-by-step is able to outperform LLM baselines by using much smaller models, e.g., over 700× smaller model on ANLI. Standard finetuning fails to match LLM’s performance using the same model size. Figure 7: Using unlabeled datasets, we perform Distilling step-by-step and Standard task distillation on varying sizes of T5 models and compare them to Few-shot CoT. Distilling step-by-step outperforms Few-shot CoT by using 2000× smaller models on e-SNLI and 45× smaller models on ANLI and CQA. On SVAMP, by adding unlabeled examples from ASDiv, we close the gap to Few-shot CoT whereas Standard distillation still struggles to catch up. with 220M (over 2000× smaller) T5 model on eSNLI, 770M (over 700× smaller) T5 models on ANLI and SVAMP, and 11B (over 45× smaller) T5 model on CQA. These results hold true even by further finetuning the 540B PaLM model on available labeled data with PINTO tuning3. In Figure 7, by only utilizing unlabeled examples, Distilling step-by-step also outperforms the teacher LLM on 3 out of 4 datasets. Specifically, Distilling step-by-step surpasses the 540B PaLM model’s Few-shot CoT performance by using 11B T5 with less than 3% of PaLM’s size. On SVAMP where the distilled model underperforms, we hypothesize that the performance gap is due to the relatively small number of data points in the dataset (i.e., 800). In reaction, we propose to augment the dataset with additional unlabeled examples to close the performance gap as shown in next. 3We note that PETuning methods may outperform PINTO tuning. However, they require massive resource in both training and deployment, which is not the focus of this work. Unlabeled data augmentation further improves Distilling step-by-step. We augment the SVAMP training set with unlabeled examples from the ASDiv dataset (Miao et al., 2020). ASDiv dataset contains a total of 2, 305 examples, where each example is a math word problem similar to the ones in SVAMP. In Figure 7 on SVAMP, we show the performances of Distilling step-by-step and standard task distillation using 11B T5 model after augmenting the training set with ASDiv. We see the data augmentation much improves the performance for both Distilling step-by-step and standard task distillation. However, even with the added unlabeled examples, standard task distillation still underperforms Few-shot CoT. On the other hand, Distilling step-by-step is able to much more efficiently exploit the value of the added examples to achieve the same performance level of Few-shot CoT, again, using a T5 model of size less than 3% of the 540B PaLM. Figure 8: We show the minimum size of T5 models and the least amount of human-labeled examples required for Distilling step-by-step to outperform LLM’s Few-shot CoT by a coarse-grained search. Distilling step-by-step is able to outperform Few-shot CoT using not only much smaller models, but it also achieves so with much less training examples compared to Standard finetuning. On ANLI, we outperform the LLM CoT with a 770M model using only 80% of the dataset, whereas Standard finetuning struggles to match even using 100% of the dataset. Figure 9: Similar to Figure 8 but using only unlabeled examples, Distilling step-by-step is able to outperform Few-shot CoT using much smaller models and with much less examples compared to Standard task distillation. On SVAMP, the x-axis corresponds to the size of ASDiv dataset used for augmenting the original SVAMP dataset, i.e., x = 0 is without augmentation and x = 100 corresponds to adding the full ASDiv dataset. 4.3 Outperforming LLMs using minimum model size and least training data Here, using the LLM’s performance as an anchor point, we explore the most efficient resource requirements in terms of both number of training examples and deployed model size, that Distilling step-by-step and standard finetuning/distillation need to outperform the LLM. We present the results, again under human-labeled setting and unlabeled setting, in Figure 8 and Figure 9 respectively. We visualize the results by plotting different resultant models by (1) the number of training examples used (x-axis), (2) the final task performance achieved (y-axis), and (3) the size of the model (visualized by the size of the shaded area). Distilling step-by-step outperforms LLMs with much smaller models by using less data. On all datasets in Figure 8, we see that Distilling stepby-step outperforms PaLM’s Few-shot CoT with much smaller T5 models using only a subset of the available training examples. Specifically, on e-SNLI, Distilling step-by-step can achieve better performance than Few-shot CoT with a model over 2000× smaller (220M T5) and only 0.1% of the full dataset. In Figure 9 where only unlabeled datasets are available, we observe the same trend that Distilling step-by-step can, at most time, outperform Few-shot CoT with smaller model as well as less data. For instance, on ANLI, Distilling stepby-step outperforms the LLM with a 45× smaller model and 50% of the full unlabeled set. Standard finetuning and distillation require more data and larger model. Finally, in Figure 8 and Figure 9, we see that standard finetuning and distillation often need either more data or larger models to match LLM’s performance. For instance, on e-SNLI in Figure 8, we observe that Distilling step-by-step outperform the LLM using only 0.1% of the dataset while standard finetuning requires more data to match the performance. Furthermore, on ANLI in Figure 8, we observe that Distilling step-by-step can outperform PaLM using 770M model with only 80% of the training set while standard finetuning struggles to match the LLM even Table 1: Distilling step-by-step works with different sizes of LLMs. When rationales are extracted from a 20B GPT-NeoX model, Distilling step-by-step is still able to provide performance lift compared to standard finetuning on 220M T5 models. Dataset Method LLM e-SNLI ANLI CQA SVAMP STANDARD FINETUNING N/A 88.38 43.58 62.19 62.63 DISTILLING STEP-BY-STEP 20B 89.12 48.15 63.25 63.00 DISTILLING STEP-BY-STEP 540B 89.51 49.58 63.29 65.50 using the full dataset and thus requires larger model to close the performance gap. 4.4 Further ablation studies So far, we have focused on showing the effectiveness of Distilling step-by-step on reducing the training data required for finetuning or distilling smaller task-specific models. In this section, we perform further studies to understand the influence of different components in the Distilling step-by-step framework. Specifically, we study (1) how different LLMs, from which the rationales are extracted, affect the effectiveness of Distilling step-by-step, and (2) how the multi-task training approach compares to other potential design choices in training small task-specific models with LLM rationales. Here, we fix the small task-specific models to be 220M T5 models, and utilize 100% of the data on all datasets. Distilling step-by-step works with different sizes of decently trained LLMs. In addition to using 540B PaLM as the LLM, here we consider a relatively smaller LLM, 20B GPT-NeoX model (Black et al., 2022), from which we extract rationales for Distilling step-by-step. In Table 1, we see that when coupled with LLMs of different sizes, Distilling step-by-step can still provide performance improvements compared to standard finetuning. However, the performance lift is smaller when rationales are extracted from the 20B GPT-NeoX model instead of from the 540B PaLM. This can be due to the fact that the larger PaLM model provides higher-quality rationales that are more beneficial for learning the task. Multi-task training is much more effective than single-task rationale and label joint prediction. There are different possible ways to train taskspecific models with LLM-rationales as output supervisions. One straightforward approach is to concatenate the rationale ˆri and label ˆyi into a single Table 2: Our proposed multi-task training framework consistently leads to better performances than treating rationale and label predictions as a single task. Singletask training can at times lead to worse performance than standard finetuning. Dataset Method e-SNLI ANLI CQA SVAMP STANDARD FINETUNING 88.38 43.58 62.19 62.63 SINGLE-TASK TRAINING 88.88 43.50 61.37 63.00 MULTI-TASK TRAINING 89.51 49.58 63.29 65.50 sequence [ˆri, ˆyi] and treat the entire sequence as the target output in training small models, as considered in (Magister et al., 2022; Ho et al., 2022): Lsingle = 1 N N � i=1 ℓ(f(xi), [ˆri, ˆyi]). (5) In Table 2, we compare this single-task training approach to our proposed multi-task training approach for utilizing LLM-rationales. We see that not only multi-task training consistently leads to better performance, single-task training with LLMrationales can at times leads to worse performance than standard finetuning, e.g., on ANLI and CQA. In fact, similar results have also been observed in (Wiegreffe et al., 2021; Magister et al., 2022; Ho et al., 2022) that simply treating rationale and label predictions as a single joint task may harm the model’s performance on label prediction. This validates our use of the multi-task training approach, and highlights the need to treat the rationales carefully so as to unleash their actual benefits. 5 Discussion We propose Distilling step-by-step to extract rationales from LLMs as informative supervision in training small task-specific models. We show that Distilling step-by-step reduces the training dataset required to curate task-specific smaller models; it also reduces the model size required to achieve, and even surpass, the original LLM’s performance. Distilling step-by-step proposes a resource-efficient training-to-deployment paradigm compared to existing methods. Further studies demonstrate the generalizability and the design choices made in Distilling step-by-step. Finally, we discuss the limitations, future directions and ethics statement of our work below. Limitations There are a number of limitations with our approach. First, we require users to produce a few example demonstrations (∼ 10-shot for all tasks) in order to use the few-shot CoT (Wei et al., 2022) prompting mechanism. This limitation can be improved by using recent advances that suggest that rationales can be elicited without any userannotated demonstrations (Kojima et al., 2022). Second, training task-specific models with rationales incur slight training-time computation overhead. However, at test time, our multi-task design naturally avoids the computation overhead since it allows one to only predict labels without generating the rationales. Finally, while we observe success using LLM rationales, there is evidence that LLMs exhibit limited reasoning capability on more complex reasoning and planning tasks (Valmeekam et al., 2022). Future work should characterize how rationale quality affects Distilling step-by-step. Ethics statement It is worth noting that the behavior of the our downstream smaller models is subject to biases inherited from the larger teacher LLM. We envision that the same research progress in reducing anti-social behaviors in LLMs can also be applied to improve smaller language models. References Priyanka Agrawal, Chris Alberti, Fantine Huot, Joshua Maynez, Ji Ma, Sebastian Ruder, Kuzman Ganchev, Dipanjan Das, and Mirella Lapata. 2022. Qameleon: Multilingual qa with only 5 examples. arXiv preprint arXiv:2211.08264. Simran Arora, Avanika Narayan, Mayee F Chen, Laurel J Orr, Neel Guha, Kush Bhatia, Ines Chami, Frederic Sala, and Christopher Ré. 2022. Ask me anything: A simple strategy for prompting language models. arXiv preprint arXiv:2210.02441. Lucas Beyer, Xiaohua Zhai, Amélie Royer, Larisa Markeeva, Rohan Anil, and Alexander Kolesnikov. 2022. Knowledge distillation: A good teacher is patient and consistent. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10925–10934. Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An opensource autoregressive language model. In Proceedings of the ACL Workshop on Challenges & Perspectives in Creating Large Language Models. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901. Cristian Buciluˇa, Rich Caruana, and Alexandru Niculescu-Mizil. 2006. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535–541. Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natural language inference with natural language explanations. Advances in Neural Information Processing Systems, 31. Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. 2020. Big self-supervised models are strong semi-supervised learners. Advances in neural information processing systems, 33:22243–22255. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Jacob Eisenstein, Daniel Andor, Bernd Bohnet, Michael Collins, and David Mimno. 2022. Honest students from untrusted teachers: Learning an interpretable question-answering pipeline from a pretrained language model. arXiv preprint arXiv:2210.02498. Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. 2023. Specializing smaller language models towards multi-step reasoning. arXiv preprint arXiv:2301.12726. Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazare, and Jason Weston. 2019. Learning from dialogue after deployment: Feed yourself, chatbot! arXiv preprint arXiv:1901.05415. Peter Hase and Mohit Bansal. 2021. When can models learn from explanations? a formal framework for understanding the roles of explanation data. arXiv preprint arXiv:2102.02201. Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7). Namgyu Ho, Laura Schmid, and Se-Young Yun. 2022. Large language models are reasoning teachers. arXiv preprint arXiv:2212.10071. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556. Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 328–339, Melbourne, Australia. Association for Computational Linguistics. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large language models can self-improve. arXiv preprint arXiv:2210.11610. Fotis Iliopoulos, Vasilis Kontonis, Cenk Baykal, Gaurav Menghani, Khoa Trinh, and Erik Vee. 2022. Weighted distillation with unlabeled examples. In Advances in Neural Information Processing Systems. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691. Liunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, and Yejin Choi. 2023. Symbolic chain-of-thought distillation: Small models can also\" think\" step-by-step. arXiv preprint arXiv:2306.14050. Kevin J Liang, Weituo Hao, Dinghan Shen, Yufan Zhou, Weizhu Chen, Changyou Chen, and Lawrence Carin. 2020. Mixkd: Towards efficient distillation of large-scale language models. arXiv preprint arXiv:2011.00593. Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. 2022. Teaching small language models to reason. arXiv preprint arXiv:2212.08410. Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020. A diverse corpus for evaluating and developing english math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 975–984. Smitha Milli, Ludwig Schmidt, Anca D Dragan, and Moritz Hardt. 2019. Model reconstruction from model explanations. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 1–9. Sharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan. 2020. Wt5?! training text-to-text models to explain their predictions. arXiv preprint arXiv:2004.14546. Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020. Adversarial NLI: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114. Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080–2094, Online. Association for Computational Linguistics. Danish Pruthi, Rachit Bansal, Bhuwan Dhingra, Livio Baldini Soares, Michael Collins, Zachary C Lipton, Graham Neubig, and William W Cohen. 2022. Evaluating explanations: How much do explanations from the teacher aid students? Transactions of the Association for Computational Linguistics, 10:359–375. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67. Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain yourself! leveraging language models for commonsense reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4932–4942, Florence, Italy. Association for Computational Linguistics. Andrew Slavin Ross, Michael C Hughes, and Finale Doshi-Velez. 2017. Right for the right reasons: Training differentiable models by constraining their explanations. arXiv preprint arXiv:1703.03717. Ryan Smith, Jason A Fries, Braden Hancock, and Stephen H Bach. 2022a. Language models in the loop: Incorporating prompting into weak supervision. arXiv preprint arXiv:2205.02318. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. 2022b. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990. Suraj Srinivas and François Fleuret. 2018. Knowledge transfer with jacobian matching. In International Conference on Machine Learning, pages 4723–4731. PMLR. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149–4158, Minneapolis, Minnesota. Association for Computational Linguistics. Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. 2019. Distilling taskspecific knowledge from bert into simple neural networks. arXiv preprint arXiv:1903.12136. Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239. Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. 2022. Large language models still can’t plan (a benchmark for llms on planning and reasoning about change). arXiv preprint arXiv:2206.10498. Peifeng Wang, Aaron Chan, Filip Ilievski, Muhao Chen, and Xiang Ren. 2022a. Pinto: Faithful language reasoning using prompt-generated rationales. arXiv preprint arXiv:2211.01562. Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu, and Michael Zeng. 2021. Want to reduce labeling cost? gpt-3 can help. arXiv preprint arXiv:2108.13487. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022b. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903. Peter West, Chandra Bhagavatula, Jack Hessel, Jena D Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, and Yejin Choi. 2021. Symbolic knowledge distillation: from general language models to commonsense models. arXiv preprint arXiv:2110.07178. Sarah Wiegreffe, Ana Marasovi´c, and Noah A. Smith. 2021. Measuring association between labels and free-text rationales. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10266–10284, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using “annotator rationales” to improve machine learning for text categorization. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 260–267, Rochester, New York. Association for Computational Linguistics. Eric Zelikman, Yuhuai Wu, and Noah D Goodman. 2022. Star: Bootstrapping reasoning with reasoning. arXiv preprint arXiv:2203.14465. Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, and Jitendra Malik. 2020. Side-tuning: a baseline for network adaptation via additive side networks. In European Conference on Computer Vision, pages 698–714. Springer. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068. Ye Zhang, Iain Marshall, and Byron C. Wallace. 2016. Rationale-augmented convolutional neural networks for text classification. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 795–804, Austin, Texas. Association for Computational Linguistics. Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Joseph E Gonzalez, et al. 2022. Alpa: Automating inter-and intraoperator parallelism for distributed deep learning. arXiv preprint arXiv:2201.12023. A Experiment detail A.1 Implementation We perform our experiments on cloud A100×16 GPU instances. We train the T5 models with the following hyperparameters, using publicly available packages from https://github.com/ huggingface/transformers: • T5-Base (220M) and T5-Large (770M): We train the models with learning rate = 5 × 10−5, batch size = 64, max input length = 1024, for a maximum of 10000 steps. • T5-XXL (11B): We train the models with learning rate = 5 × 10−5, batch size = 32, max input length = 1024, for a maximum of 4000 steps. We report all the results over 4 random runs, and include the standard error in the presented plots. A.2 Datasets We provide more detailed descriptions on the datasets used in our experiments. We include the sources from which we obtain the datasets as well as their original sources released from the authors. We refer readers to these sources for their license or terms for use and/or distribution. To the best of our knowledge, the datasets used do not contain information that names or uniquely identifies individual people or offensive content. • e-SNLI: The dataset was originally released in (Camburu et al., 2018), and made publicly available at https://github.com/ OanaMariaCamburu/e-SNLI. We obtain the dataset from https://huggingface.co/ datasets/esnli. • ANLI: The dataset was originally released in (Nie et al., 2020), and made publicly available at https://github.com/ facebookresearch/anli. We obtain the dataset from https://huggingface.co/ datasets/anli. We use the R1 split in our experiments. • CQA: The dataset was originally released in (Talmor et al., 2019), and made publicly available at https://www.tau-nlp.sites. tau.ac.il/commonsenseqa. It was then augmented with human-labeled explanations Table 3: Dataset statistics used in our experiments. Dataset Train Validation Test e-SNLI 549,367 9,842 9,824 ANLI 16,946 1,000 1,000 CQA 8,766 975 1,221 SVAMP 720 80 200 by (Rajani et al., 2019), which is available at https://github.com/salesforce/ cos-e. We obtain the dataset used in our experiments from https://huggingface.co/ datasets/cos_e. • SVAMP: The dataset was originally released in (Patel et al., 2021). We obtain the dataset from https://github.com/ arkilpatel/SVAMP. • ASDiv: The dataset was originally released in (Miao et al., 2020). We obtain the dataset from https://github.com/ chaochun/nlu-asdiv-dataset. For each dataset, we randomly subsample 10% of the original training set to serve as validation set when validation set is not originally provided. For CQA, we use the original validation set to serve as our test set since the ground-truth labels are not available for the original test set. We provide the dataset statistics in Table 3. \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def reformat_text(doc_content):\n",
    "    content = doc_content.replace('-\\n', '')\n",
    "    content = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', content)\n",
    "    content = re.sub(' +', ' ', content)\n",
    "    return content\n",
    "\n",
    "print(reformat_text(doc_content))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-17T04:46:00.962943Z",
     "start_time": "2023-08-17T04:46:00.931452Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-17T05:01:20.839940Z",
     "start_time": "2023-08-17T05:01:20.812384Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "splits = text_splitter.split_text(reformat_text(doc_content))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-17T05:01:21.253959Z",
     "start_time": "2023-08-17T05:01:21.218231Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-17T05:01:21.562007Z",
     "start_time": "2023-08-17T05:01:21.529965Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-17T05:01:16.741645Z",
     "start_time": "2023-08-17T05:01:16.714277Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "7371"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reformat_text(doc_content).split(\" \"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-17T04:58:17.245016Z",
     "start_time": "2023-08-17T04:58:17.207212Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
