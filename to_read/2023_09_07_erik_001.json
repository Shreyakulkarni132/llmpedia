{"Published": "2023-09-07", "Title": "XGen-7B Technical Report", "Authors": "Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam, Tong Niu, Wojciech Kry\u015bci\u0144ski, Lidiya Murakhovs'ka, Prafulla Kumar Choubey, Alex Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq Joty, Caiming Xiong", "Summary": "Large Language Models (LLMs) have become ubiquitous across various domains, transforming the way we interact with information and conduct research. However, most high-performing LLMs remain confined behind proprietary walls, hindering scientific progress. Most open-source LLMs, on the other hand, are limited in their ability to support longer sequence lengths, which is a key requirement for many tasks that require inference over an input context. To address this, we have trained XGen, a series of 7B parameter models on up to 8K sequence length for up to 1.5T tokens. We have also finetuned the XGen models on public-domain instructional data, creating their instruction-tuned counterparts (XGen-Inst). We open-source our models for both research advancements and commercial applications. Our evaluation on standard benchmarks shows that XGen models achieve comparable or better results when compared with state-of-the-art open-source LLMs. Our targeted evaluation on long sequence modeling tasks shows the benefits of our 8K-sequence models over 2K-sequence open-source LLMs.", "main_contribution": {"headline": "XGen-7B: A Large Language Model Supporting Longer Sequence Lengths", "description": "The paper presents XGen-7B, a series of Large Language Models (LLMs) trained to support longer sequence lengths of up to 8K tokens. This addresses a key limitation in most open-source LLMs, which typically support a maximum of 2K token sequence length. The XGen-7B models are trained on up to 1.5 trillion tokens and are also fine-tuned on public-domain instructional data, creating instruction-tuned counterparts (XGen-7B-Inst). The authors have open-sourced these models to facilitate both research advancements and commercial applications. Evaluations on standard benchmarks show that XGen-7B models achieve comparable or better results when compared with state-of-the-art open-source LLMs. Furthermore, the models demonstrate benefits in long sequence modeling tasks over 2K-sequence open-source LLMs."}, "takeaways": {"headline": "XGen-7B: A New Benchmark for Long Sequence Modeling", "description": "The XGen-7B models, with their ability to support up to 8K sequence lengths, set a new benchmark for long sequence modeling tasks. This capability is particularly beneficial for tasks that require inference over an extensive input context, such as writing code, summarizing text, and predicting protein sequences. The models' performance on standard benchmarks and long sequence modeling tasks demonstrates their potential for both research and commercial applications. The open-sourcing of these models also provides a valuable resource for the AI community to further explore and improve upon long sequence modeling.", "example": "For instance, in a task that requires summarizing a long document, an LLM like XGen-7B can process the entire document in one go, rather than breaking it down into smaller chunks. This allows the model to better understand the overall context and produce a more coherent and accurate summary."}, "category": "TRAINING", "novelty_analysis": "The development of XGen-7B models represents a significant advancement in the field of LLMs. While the concept of training LLMs on longer sequence lengths is not entirely new, the scale at which the authors have achieved this - up to 8K sequence length for up to 1.5 trillion tokens - is novel. Furthermore, the fine-tuning of these models on public-domain instructional data adds another layer of novelty to this work.", "novelty_score": 3, "technical_analysis": "The paper delves into the technical details of training the XGen-7B models, including the stage-wise training process, the data mixtures used, and the training recipe. It also discusses the challenges encountered during training, such as loss spikes and the computational cost of training with longer sequences. While the paper is technical in nature, the authors do a commendable job of explaining these complex concepts in an accessible manner.", "technical_score": 3, "enjoyable_analysis": "The paper is well-structured and presents a clear narrative, making it an enjoyable read. The authors effectively balance technical details with broader discussions on the implications of their work, ensuring that the paper is engaging for both technical and non-technical readers. The inclusion of comprehensive evaluations and comparisons with other LLMs adds further value to the paper.", "enjoyable_score": 3}