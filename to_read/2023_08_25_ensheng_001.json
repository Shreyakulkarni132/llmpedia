{"Published": "2023-08-25", "Title": "SoTaNa: The Open-Source Software Development Assistant", "Authors": "Ensheng Shi, Fengji Zhang, Yanlin Wang, Bei Chen, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, Hongbin Sun", "Summary": "Software development plays a crucial role in driving innovation and efficiency across modern societies. To meet the demands of this dynamic field, there is a growing need for an effective software development assistant. However, existing large language models represented by ChatGPT suffer from limited accessibility, including training data and model weights. Although other large open-source models like LLaMA have shown promise, they still struggle with understanding human intent. In this paper, we present SoTaNa, an open-source software development assistant. SoTaNa utilizes ChatGPT to generate high-quality instruction-based data for the domain of software engineering and employs a parameter-efficient fine-tuning approach to enhance the open-source foundation model, LLaMA. We evaluate the effectiveness of \\our{} in answering Stack Overflow questions and demonstrate its capabilities. Additionally, we discuss its capabilities in code summarization and generation, as well as the impact of varying the volume of generated data on model performance. Notably, SoTaNa can run on a single GPU, making it accessible to a broader range of researchers. Our code, model weights, and data are public at \\url{https://github.com/DeepSoftwareAnalytics/SoTaNa}.", "main_contribution": {"headline": "SoTaNa: An Open-Source Software Development Assistant Utilizing Large Language Models", "description": "The paper presents SoTaNa, an open-source software development assistant that leverages large language models (LLMs) to generate high-quality instruction-based data for software engineering tasks. SoTaNa uses ChatGPT to generate this data and employs a parameter-efficient fine-tuning approach to enhance the LLaMA open-source foundation model. The model is evaluated on its ability to answer Stack Overflow questions, demonstrating its effectiveness in understanding developers' intent and generating relevant responses. The authors also discuss SoTaNa's capabilities in code summarization and generation, and the impact of varying the volume of generated data on model performance. Notably, SoTaNa can run on a single GPU, making it accessible to a broader range of researchers."}, "takeaways": {"headline": "SoTaNa: A Powerful Tool for Software Development Assistance", "description": "SoTaNa's ability to understand developers' intent and generate relevant responses makes it a powerful tool for software development assistance. It can be used to answer Stack Overflow questions, summarize code, and generate code, demonstrating its wide range of applications in the software engineering domain. The model's performance can be influenced by the volume of generated data, highlighting the importance of data quality and quantity in training effective LLMs. The fact that SoTaNa can run on a single GPU also makes it a practical tool for researchers with limited computational resources.", "example": "For instance, a developer could use SoTaNa to answer a complex programming query on Stack Overflow. The developer would input the question into SoTaNa, which would then generate a relevant and accurate response based on its training data. This could significantly speed up the problem-solving process and improve the developer's productivity."}, "category": "USE CASES", "novelty_analysis": "SoTaNa represents a novel application of large language models in the field of software development assistance. While previous models have struggled with understanding human intent, SoTaNa demonstrates improved performance in this area by leveraging ChatGPT to generate high-quality instruction-based data for software engineering tasks. The use of a parameter-efficient fine-tuning approach to enhance the LLaMA open-source foundation model is also a unique aspect of this work.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, discussing the process of generating instruction-based data using ChatGPT and the fine-tuning approach used to enhance the LLaMA model. However, the authors explain these concepts clearly and provide sufficient context, making the paper accessible to readers with a basic understanding of large language models and software engineering.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and clearly presents the methodology and results of the study. The authors' discussion of SoTaNa's capabilities and the impact of varying the volume of generated data on model performance provides interesting insights into the practical applications of large language models in software development. Overall, the paper offers a compelling read for those interested in the intersection of artificial intelligence and software engineering.", "enjoyable_score": 2}