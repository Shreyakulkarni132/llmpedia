{"Published": "2023-09-12", "Title": "LLaSM: Large Language and Speech Model", "Authors": "Yu Shu, Siwei Dong, Guangyao Chen, Wenhao Huang, Ruihua Zhang, Daochen Shi, Qiqi Xiang, Yemin Shi", "Summary": "Multi-modal large language models have garnered significant interest recently. Though, most of the works focus on vision-language multi-modal models providing strong capabilities in following vision-and-language instructions. However, we claim that speech is also an important modality through which humans interact with the world. Hence, it is crucial for a general-purpose assistant to be able to follow multi-modal speech-and-language instructions. In this work, we propose Large Language and Speech Model (LLaSM). LLaSM is an end-to-end trained large multi-modal speech-language model with cross-modal conversational abilities, capable of following speech-and-language instructions. Our early experiments show that LLaSM demonstrates a more convenient and natural way for humans to interact with artificial intelligence. Specifically, we also release a large Speech Instruction Following dataset LLaSM-Audio-Instructions. Code and demo are available at https://github.com/LinkSoul-AI/LLaSM and https://huggingface.co/spaces/LinkSoul/LLaSM. The LLaSM-Audio-Instructions dataset is available at https://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions.", "main_contribution": {"headline": "LLaSM: A Large Language and Speech Model for Multi-modal Conversations", "description": "The paper introduces the Large Language and Speech Model (LLaSM), a multi-modal model capable of understanding and following speech-and-language instructions. Unlike most large language models that only receive text input, LLaSM can process speech inputs directly, making it more efficient and natural for human interaction. The model is trained in two stages: modality adaptation pre-training and cross-modal instruction fine-tuning. The authors also release a large Speech Instruction Following dataset, LLaSM-Audio-Instructions, to address the scarcity of cross-modal speech-and-language instructions data. The dataset is the largest of its kind for Chinese and English speech-text cross-modal instruction-following."}, "takeaways": {"headline": "LLaSM: A Step Towards More Natural Human-AI Interaction", "description": "LLaSM's ability to process speech inputs directly makes it a promising model for more convenient and interactive human-AI communication. It can be used to build general-purpose assistants that can follow multi-modal speech-and-language instructions. The LLaSM-Audio-Instructions dataset, released alongside the model, provides a valuable resource for training and fine-tuning similar models. The authors also suggest that by adopting a visual modal encoder, LLaSM could be extended to include visual capabilities in the future.", "example": "For instance, an AI assistant built using LLaSM could receive a spoken instruction like 'Find the nearest coffee shop' and respond appropriately. The assistant could also handle more complex instructions involving multiple modalities, such as 'Show me the route to the nearest coffee shop and read out the directions.'"}, "category": "ARCHITECTURES", "novelty_analysis": "The introduction of LLaSM represents a significant advancement in the field of multi-modal large language models. While previous works have focused primarily on vision-language models, this paper emphasizes the importance of speech as a modality and introduces a model capable of processing both speech and text inputs. The release of the LLaSM-Audio-Instructions dataset also contributes to the novelty of this work.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, detailing the architecture of LLaSM and the two-stage training process. It also discusses the construction of the LLaSM-Audio-Instructions dataset. However, the concepts are explained clearly, making the paper accessible to readers with a basic understanding of large language models and multi-modal learning.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and provides a clear explanation of the proposed model and its training process. The inclusion of early experimental results and the discussion of potential future work make the paper an engaging read for those interested in multi-modal large language models.", "enjoyable_score": 2}