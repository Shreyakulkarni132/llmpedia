{"Published": "2023-08-21", "Title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models", "Authors": "Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, Torsten Hoefler", "Summary": "We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information (\"LLM thoughts\") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by >31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.", "main_contribution": {"headline": "Graph of Thoughts (GoT): A Novel Framework for Enhancing LLM Prompting Capabilities", "description": "The paper introduces Graph of Thoughts (GoT), a novel framework that significantly enhances the prompting capabilities of Large Language Models (LLMs). GoT models the information generated by an LLM as an arbitrary graph, where each unit of information, or 'LLM thought', is a vertex and the edges represent dependencies between these vertices. This approach allows for the combination of arbitrary LLM thoughts into synergistic outcomes, the distillation of entire networks of thoughts, and the enhancement of thoughts using feedback loops. GoT outperforms existing paradigms such as Chain-of-Thought and Tree of Thoughts (ToT), increasing the quality of sorting tasks by 62% over ToT while simultaneously reducing costs by over 31%. The framework is designed to be extensible, allowing for the incorporation of new thought transformations and the use of different LLM models."}, "takeaways": {"headline": "GoT: A Powerful Tool for Complex Problem Solving with LLMs", "description": "GoT's graph-based model of reasoning offers a powerful tool for solving complex problems with LLMs. By allowing for the combination and refinement of thoughts, GoT brings LLM reasoning closer to human thinking and brain mechanisms, which form complex networks. The framework is particularly well-suited for tasks that can be naturally decomposed into smaller subtasks that are solved individually and then merged for a final solution. GoT's modular architecture allows for rapid prototyping of novel prompting ideas and experimenting with different LLM models, making it a versatile tool for LLM practitioners.", "example": "For example, in a sorting task, GoT could decompose the task into sorting smaller subarrays of numbers. Each subarray could be sorted individually as a separate 'thought', and then these sorted subarrays could be merged into the final sorted array. This approach could significantly improve the quality of sorting while reducing costs compared to other prompting schemes."}, "category": "PROMPTING", "novelty_analysis": "GoT introduces a novel approach to prompting in LLMs by modeling the LLM reasoning process as an arbitrary graph. This allows for more complex thought patterns and transformations that are not possible with existing paradigms such as Chain-of-Thought or Tree of Thoughts. The framework's ability to improve the quality of tasks while reducing costs represents a significant advancement in the field of LLM prompting.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, detailing the design and implementation of the GoT framework and its underlying graph-based model of reasoning. It discusses the various thought transformations enabled by GoT and presents a modular architecture for implementing the framework. However, the concepts are explained clearly and should be accessible to readers with a background in AI and machine learning.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents a novel and intriguing contribution to the field of LLM prompting. The clear explanation of the GoT framework and its advantages, along with the detailed evaluation of its performance on various tasks, make for an engaging and informative read.", "enjoyable_score": 3}