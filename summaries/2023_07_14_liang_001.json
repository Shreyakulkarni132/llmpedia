{"Published": "2023-07-14", "Title": "Learning to Retrieve In-Context Examples for Large Language Models", "Authors": "Liang Wang, Nan Yang, Furu Wei", "Summary": "Large language models (LLMs) have demonstrated their ability to learn in-context, allowing them to perform various tasks based on a few input-output examples. However, the effectiveness of in-context learning is heavily reliant on the quality of the selected examples. In this paper, we propose a novel framework to iteratively train dense retrievers that can identify high-quality in-context examples for LLMs. Our framework initially trains a reward model based on LLM feedback to evaluate the quality of candidate examples, followed by knowledge distillation to train a bi-encoder based dense retriever. Our experiments on a suite of 30 tasks demonstrate that our framework significantly enhances in-context learning performance. Furthermore, we show the generalization ability of our framework to unseen tasks during training. An in-depth analysis reveals that our model improves performance by retrieving examples with similar patterns, and the gains are consistent across LLMs of varying sizes.", "main_contribution": {"headline": "LLM-R: A Framework for High-Quality In-Context Example Retrieval for Large Language Models", "description": "The paper introduces a novel framework, LLM-R (LLM Retriever), designed to retrieve high-quality in-context examples for large language models (LLMs). The framework operates in an iterative manner, initially training a reward model based on LLM feedback to evaluate the quality of candidate examples. This is followed by knowledge distillation to train a bi-encoder based dense retriever. The framework significantly enhances in-context learning performance across a suite of 30 tasks and demonstrates the ability to generalize to unseen tasks during training. The model improves performance by retrieving examples with similar patterns, and the gains are consistent across LLMs of varying sizes."}, "takeaways": {"headline": "LLM-R Framework Enhances In-Context Learning Performance", "description": "The LLM-R framework can be used to improve the performance of large language models by retrieving high-quality in-context examples. The iterative training process, which involves training a reward model and a dense retriever, can be applied to a wide range of tasks. The framework's ability to generalize to unseen tasks during training makes it a versatile tool for LLM practitioners. The model's performance improvement is consistent across LLMs of varying sizes, making it a scalable solution.", "example": "For instance, given a task of question answering, the LLM-R framework can be used to retrieve high-quality in-context examples from a pool of training data. These examples are then used to train the LLM, improving its performance on the task. The process can be iterated multiple times, each time retrieving a new set of candidates based on the latest dense retriever, leading to continuous improvement in performance."}, "category": "TRAINING", "novelty_analysis": "The LLM-R framework introduces a novel approach to retrieving high-quality in-context examples for large language models. The iterative training process, which involves training a reward model and a dense retriever, is a unique contribution to the field. The framework's ability to generalize to unseen tasks during training is also a significant advancement.", "novelty_score": 3, "technical_analysis": "The paper is highly technical, delving into the specifics of the LLM-R framework, including the training of the reward model and the dense retriever. It also provides a detailed analysis of the framework's performance across a suite of 30 tasks, requiring a solid understanding of large language models and machine learning concepts.", "technical_score": 3, "enjoyable_analysis": "The paper is well-structured and provides a comprehensive overview of the LLM-R framework. The detailed analysis of the framework's performance across various tasks and its ability to generalize to unseen tasks provides insightful and engaging content. However, the high level of technical detail may make it a challenging read for those without a strong background in the field.", "enjoyable_score": 2}