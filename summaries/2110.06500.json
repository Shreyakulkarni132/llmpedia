{"Published": "2022-07-14", "Title": "Differentially Private Fine-tuning of Language Models", "Authors": "Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A. Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, Sergey Yekhanin, Huishuai Zhang", "Summary": "We give simpler, sparser, and faster algorithms for differentially private fine-tuning of large-scale pre-trained language models, which achieve the state-of-the-art privacy versus utility tradeoffs on many standard NLP tasks. We propose a meta-framework for this problem, inspired by the recent success of highly parameter-efficient methods for fine-tuning. Our experiments show that differentially private adaptations of these approaches outperform previous private algorithms in three important dimensions: utility, privacy, and the computational and memory cost of private training. On many commonly studied datasets, the utility of private models approaches that of non-private models. For example, on the MNLI dataset we achieve an accuracy of $87.8\\%$ using RoBERTa-Large and $83.5\\%$ using RoBERTa-Base with a privacy budget of $\\epsilon = 6.7$. In comparison, absent privacy constraints, RoBERTa-Large achieves an accuracy of $90.2\\%$. Our findings are similar for natural language generation tasks. Privately fine-tuning with DART, GPT-2-Small, GPT-2-Medium, GPT-2-Large, and GPT-2-XL achieve BLEU scores of 38.5, 42.0, 43.1, and 43.8 respectively (privacy budget of $\\epsilon = 6.8,\\delta=$ 1e-5) whereas the non-private baseline is $48.1$. All our experiments suggest that larger models are better suited for private fine-tuning: while they are well known to achieve superior accuracy non-privately, we find that they also better maintain their accuracy when privacy is introduced.", "main_contribution": {"headline": "Simpler, sparser, and faster algorithms for differentially private fine-tuning of LLMs", "description": "The paper presents a meta-framework for differentially private fine-tuning of large-scale pre-trained language models. The authors propose simpler, sparser, and faster algorithms that achieve state-of-the-art privacy versus utility trade-offs on many standard NLP tasks. The framework is inspired by highly parameter-efficient methods for fine-tuning. The authors demonstrate that their differentially private adaptations outperform previous private algorithms in terms of utility, privacy, and the computational and memory cost of private training. They also show that larger models maintain their accuracy better when privacy is introduced."}, "takeaways": {"headline": "Differentially private fine-tuning of LLMs can achieve near non-private performance", "description": "The paper's findings suggest that differentially private fine-tuning of large language models can achieve performance close to that of non-private models. This is particularly useful for LLM practitioners working with sensitive data, as it allows them to maintain high utility while ensuring privacy. The authors' approach also reduces the computational and memory cost of private training, making it more efficient. The paper also suggests that larger models are better suited for private fine-tuning, as they maintain their accuracy better when privacy is introduced.", "example": "For instance, an LLM practitioner could use the authors' approach to fine-tune a large model like RoBERTa-Large or GPT-2-XL on a sensitive dataset, achieving high accuracy while ensuring differential privacy. The approach would also be more efficient in terms of computational and memory cost compared to previous private algorithms."}, "category": "FINE-TUNING", "novelty_analysis": "The paper presents a novel approach to differentially private fine-tuning of large language models, achieving state-of-the-art privacy versus utility trade-offs. The authors' approach outperforms previous private algorithms in terms of utility, privacy, and computational and memory cost, marking a significant advancement in the field.", "novelty_score": 3, "technical_analysis": "The paper is highly technical, discussing the details of the authors' approach to differentially private fine-tuning of large language models. It requires a deep understanding of differential privacy, fine-tuning of language models, and the computational and memory cost of training models.", "technical_score": 3, "enjoyable_analysis": "The paper is well-written and organized, presenting a novel and intriguing contribution to the field of differentially private fine-tuning of large language models. However, the high level of technical detail may make it challenging for readers without a strong background in the field.", "enjoyable_score": 2}