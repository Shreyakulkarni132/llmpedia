{"Published": "2023-05-23", "Title": "LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond", "Authors": "Philippe Laban, Wojciech Kry\u015bci\u0144ski, Divyansh Agarwal, Alexander R. Fabbri, Caiming Xiong, Shafiq Joty, Chien-Sheng Wu", "Summary": "With the recent appearance of LLMs in practical settings, having methods that can effectively detect factual inconsistencies is crucial to reduce the propagation of misinformation and improve trust in model outputs. When testing on existing factual consistency benchmarks, we find that a few large language models (LLMs) perform competitively on classification benchmarks for factual inconsistency detection compared to traditional non-LLM methods. However, a closer analysis reveals that most LLMs fail on more complex formulations of the task and exposes issues with existing evaluation benchmarks, affecting evaluation precision. To address this, we propose a new protocol for inconsistency detection benchmark creation and implement it in a 10-domain benchmark called SummEdits. This new benchmark is 20 times more cost-effective per sample than previous benchmarks and highly reproducible, as we estimate inter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with performance close to random chance. The best-performing model, GPT-4, is still 8\\% below estimated human performance, highlighting the gaps in LLMs' ability to reason about facts and detect inconsistencies when they occur.", "main_contribution": {"headline": "New benchmark SUMMEDITS for evaluating LLMs' factual reasoning abilities", "description": "The paper introduces a new benchmark called SUMMEDITS for evaluating the factual reasoning abilities of Large Language Models (LLMs). This benchmark is designed to be more cost-effective and reproducible than previous benchmarks, with an estimated inter-annotator agreement of about 0.9. The authors found that most LLMs struggle on SUMMEDITS, with performance close to random chance, indicating a gap in LLMs' ability to reason about facts and detect inconsistencies."}, "takeaways": {"headline": "SUMMEDITS benchmark exposes limitations in LLMs' factual reasoning", "description": "The SUMMEDITS benchmark provides a more rigorous and cost-effective method for evaluating the factual reasoning abilities of LLMs. It can be used to identify weaknesses in LLMs' ability to detect factual inconsistencies, which is crucial for applications that rely on accurate information extraction or summarization. The benchmark could guide the development of more robust LLMs and help improve the trustworthiness of model outputs.", "example": "For instance, an LLM could be trained and then evaluated using the SUMMEDITS benchmark. The results could then be used to identify areas where the model struggles with factual reasoning, guiding further fine-tuning or training efforts to improve the model's performance."}, "category": "BEHAVIOR", "novelty_analysis": "The introduction of the SUMMEDITS benchmark represents a significant contribution to the field of LLM evaluation. While the concept of evaluating LLMs' factual reasoning abilities is not new, the creation of a more cost-effective and reproducible benchmark is a novel development.", "novelty_score": 2, "technical_analysis": "The paper is somewhat technical, discussing the creation and implementation of a new benchmark for evaluating LLMs. However, it does not delve into complex mathematical theories or algorithms, making it accessible to readers with a basic understanding of LLMs and evaluation benchmarks.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting and important problem in the field of LLMs. The introduction of the SUMMEDITS benchmark and the discussion of its implications for LLM development make for an engaging read.", "enjoyable_score": 2}