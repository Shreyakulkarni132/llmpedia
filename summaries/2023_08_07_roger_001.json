{"Published": "2023-08-07", "Title": "Studying Large Language Model Generalization with Influence Functions", "Authors": "Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger, Kamil\u0117 Luko\u0161i\u016bt\u0117, Karina Nguyen, Nicholas Joseph, Sam McCandlish, Jared Kaplan, Samuel R. Bowman", "Summary": "When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs.", "main_contribution": {"headline": "Scaling Influence Functions to Large Language Models", "description": "The paper introduces a method to scale influence functions to Large Language Models (LLMs) with up to 52 billion parameters. Influence functions are used to understand which training examples most contribute to a given behavior of a model. However, they are difficult to scale to LLMs due to the computational challenge of calculating an inverse-Hessian-vector product (IHVP). The authors propose using the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to overcome this challenge. They also introduce two algorithmic techniques, TF-IDF filtering and query batching, to reduce the cost of computing gradients of candidate training sequences. The proposed method allows for the investigation of the generalization patterns of LLMs."}, "takeaways": {"headline": "Influence Functions Provide Insight into LLM Generalization", "description": "The use of influence functions, scaled to LLMs using the EK-FAC approximation, provides a powerful tool for studying the generalization properties of LLMs. This method can help researchers and practitioners understand which training examples most contribute to a given behavior of a model, thereby providing insights into the model's learning process. The proposed method can be used to investigate various aspects of LLMs, such as the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior.", "example": "For instance, to understand how an LLM has learned to solve a particular type of math problem, one could use the proposed method to identify the training examples that most contributed to this behavior. This could provide insights into the learning process of the model and potentially help in improving its training."}, "category": "TRAINING", "novelty_analysis": "The paper presents a novel method for scaling influence functions to LLMs, which has not been done before due to computational challenges. The use of the EK-FAC approximation and the introduction of TF-IDF filtering and query batching techniques represent significant advancements in the field.", "novelty_score": 3, "technical_analysis": "The paper is highly technical, requiring a deep understanding of influence functions, the EK-FAC approximation, and the computational challenges associated with scaling these to LLMs. The paper is filled with technical details and mathematical equations, making it a challenging read for non-experts.", "technical_score": 3, "enjoyable_analysis": "The paper is well-structured and presents a novel and intriguing contribution to the field. However, the high level of technical detail and complexity may make it a challenging read for those without a strong background in the field.", "enjoyable_score": 2}