{"Published": "2023-01-16", "Title": "Dissociating language and thought in large language models: a cognitive perspective", "Authors": "Kyle Mahowald, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, Evelina Fedorenko", "Summary": "Today's large language models (LLMs) routinely generate coherent, grammatical and seemingly meaningful paragraphs of text. This achievement has led to speculation that these networks are -- or will soon become -- \"thinking machines\", capable of performing tasks that require abstract knowledge and reasoning. Here, we review the capabilities of LLMs by considering their performance on two different aspects of language use: 'formal linguistic competence', which includes knowledge of rules and patterns of a given language, and 'functional linguistic competence', a host of cognitive abilities required for language understanding and use in the real world. Drawing on evidence from cognitive neuroscience, we show that formal competence in humans relies on specialized language processing mechanisms, whereas functional competence recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition. In line with this distinction, LLMs show impressive (although imperfect) performance on tasks requiring formal linguistic competence, but fail on many tests requiring functional competence. Based on this evidence, we argue that (1) contemporary LLMs should be taken seriously as models of formal linguistic skills; (2) models that master real-life language use would need to incorporate or develop not only a core language module, but also multiple non-language-specific cognitive capacities required for modeling thought. Overall, a distinction between formal and functional linguistic competence helps clarify the discourse surrounding LLMs' potential and provides a path toward building models that understand and use language in human-like ways.", "main_contribution": {"headline": "Distinguishing Formal and Functional Linguistic Competence in LLMs", "description": "The paper provides a comprehensive review of the capabilities of Large Language Models (LLMs) by distinguishing between 'formal linguistic competence' and 'functional linguistic competence'. The authors argue that while LLMs have nearly mastered formal competence, which involves knowledge of linguistic rules and patterns, they still struggle with functional competence, which requires understanding and using language in real-world contexts. The paper grounds this distinction in human neuroscience, showing that these skills recruit different cognitive mechanisms in humans. The authors suggest that to master real-life language use, models would need to develop not only a core language module but also multiple non-language-specific cognitive capacities."}, "takeaways": {"headline": "LLMs Excel at Formal Linguistic Competence but Struggle with Functional Competence", "description": "The paper's distinction between formal and functional linguistic competence provides a clear framework for understanding the strengths and limitations of LLMs. While LLMs have shown impressive performance in tasks requiring formal linguistic competence, they fail on many tests requiring functional competence. This insight can guide LLM practitioners in the development and application of these models, highlighting the need for incorporating or developing non-language-specific cognitive capacities to improve their performance in real-world tasks.", "example": "For instance, while an LLM might excel at generating grammatically correct sentences (formal competence), it might struggle to understand the context or social implications of a conversation (functional competence). This distinction can guide the development of LLMs, emphasizing the need to incorporate cognitive capacities like formal reasoning, world knowledge, situation modeling, and social cognition."}, "category": "BEHAVIOR", "novelty_analysis": "The paper's main contribution is the distinction it draws between formal and functional linguistic competence in LLMs, providing a new perspective on evaluating their capabilities. While the concepts of formal and functional competence are not new, their application to LLMs offers a fresh lens to understand and improve these models.", "novelty_score": 2, "technical_analysis": "The paper is moderately technical, discussing concepts from cognitive neuroscience and their application to LLMs. However, it does not delve into complex mathematical theories or algorithms, making it accessible to readers with a basic understanding of AI and neuroscience.", "technical_score": 2, "enjoyable_analysis": "The paper is well-written and provides a novel perspective on LLMs, making it an interesting read. The use of neuroscience to ground the discussion adds depth to the paper and makes it engaging for readers interested in the intersection of AI and cognitive science.", "enjoyable_score": 3}