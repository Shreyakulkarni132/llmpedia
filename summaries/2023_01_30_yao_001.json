{"Published": "2023-01-30", "Title": "Complexity-Based Prompting for Multi-Step Reasoning", "Authors": "Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot", "Summary": "We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multi-step reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3 and Codex, our approach substantially improves multi-step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks (GSM8K, MultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and Penguins), with an average +5.3 and up to +18 accuracy improvements. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.", "main_contribution": {"headline": "Complexity-Based Prompting Enhances Multi-Step Reasoning in Large Language Models", "description": "The paper introduces a novel example selection scheme for multi-step reasoning in large language models (LLMs) called complexity-based prompting. This method selects prompts with higher reasoning complexity, i.e., chains with more reasoning steps, to achieve better performance on multi-step reasoning tasks. The authors extend this complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where multiple reasoning chains are sampled from the model, and the majority of generated answers from complex reasoning chains are chosen. This approach significantly improves multi-step reasoning accuracy and achieves state-of-the-art performance on several benchmarks."}, "takeaways": {"headline": "Complexity-Based Prompting Offers Improved Performance and Efficiency in Multi-Step Reasoning Tasks", "description": "The complexity-based prompting method can be used to enhance the performance of LLMs in multi-step reasoning tasks. By selecting prompts with higher reasoning complexity and extending this criteria to decoding, practitioners can achieve better results and efficiency. This approach is intuitive, easy to implement, and annotation-efficient, making it a valuable tool for LLM practitioners working on complex reasoning tasks.", "example": "For instance, when working on a complex reasoning task, instead of using simple prompts, an LLM practitioner can use complexity-based prompting to select prompts with more reasoning steps. The model can then generate multiple reasoning chains, and the majority of generated answers from complex reasoning chains can be selected as the final output, leading to improved accuracy."}, "category": "PROMPTING", "novelty_analysis": "The paper presents a novel approach to prompting in LLMs, introducing the concept of complexity-based prompting for multi-step reasoning. This method represents a significant advancement in the field of prompting techniques for LLMs, offering a new way to improve performance on complex reasoning tasks.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, discussing the intricacies of complexity-based prompting and its application in multi-step reasoning tasks. However, the concepts are explained clearly, making it accessible to readers with a basic understanding of LLMs and prompting techniques.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting and novel approach to prompting in LLMs. The clear explanation of concepts and the demonstration of the method's effectiveness make it an engaging read for those interested in LLMs and multi-step reasoning.", "enjoyable_score": 3}