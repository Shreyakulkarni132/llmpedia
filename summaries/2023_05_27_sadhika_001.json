{"Published": "2023-05-27", "Title": "Fine-Tuning Language Models with Just Forward Passes", "Authors": "Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen, Sanjeev Arora", "Summary": "Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12x memory reduction; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise.", "main_contribution": {"headline": "Memory-Efficient Zeroth-Order Optimizer for Fine-Tuning Large Language Models", "description": "The paper introduces a memory-efficient zeroth-order optimizer (MeZO) that adapts the classical zeroth-order stochastic gradient descent (ZO-SGD) method to operate in-place, thereby fine-tuning large language models (LLMs) with the same memory footprint as inference. This approach allows for the training of a 30-billion parameter model on a single A100 80GB GPU, a task that would only be feasible for a 2.7B model using traditional backpropagation. The authors conduct extensive experiments across different model types, scales, and downstream tasks, demonstrating that MeZO outperforms in-context learning and linear probing, achieves comparable performance to fine-tuning with backpropagation, is compatible with both full-parameter and parameter-efficient tuning techniques, and can effectively optimize non-differentiable objectives."}, "takeaways": {"headline": "MeZO Enables Efficient Fine-Tuning of Large Language Models", "description": "The MeZO approach introduced in this paper offers a practical solution for fine-tuning large language models without the prohibitive memory requirements of backpropagation. This opens up new possibilities for training larger models on limited hardware, making it a valuable tool for researchers and practitioners working with LLMs. Furthermore, MeZO's compatibility with both full-parameter and parameter-efficient tuning techniques, as well as its ability to optimize non-differentiable objectives, adds to its versatility and potential for a wide range of applications.", "example": "For instance, using MeZO, a researcher could fine-tune a 30-billion parameter language model on a single A100 80GB GPU, a task that would typically require a much larger memory budget. This could be done as follows: \n\n\nfrom MeZO import MeZO\n\n# Initialize MeZO optimizer\noptimizer = MeZO(model.parameters())\n\n# Training loop\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        optimizer.zero_grad()\n        output = model(batch)\n        loss = criterion(output, batch.labels)\n        loss.backward()\n        optimizer.step()\n\n\nThis code snippet provides a simplified example of how MeZO could be used in practice, although the actual implementation would likely involve additional steps and considerations."}, "category": "FINE-TUNING", "novelty_analysis": "The introduction of MeZO represents a significant advancement in the field of fine-tuning large language models. While zeroth-order methods have been explored in the past, their application to large-scale models has been limited due to theoretical and practical challenges. The authors' adaptation of the classical ZO-SGD method to operate in-place, thereby reducing memory consumption to the same level as inference, is a novel and impactful contribution.", "novelty_score": 3, "technical_analysis": "The paper is highly technical, delving into the details of the zeroth-order optimization method and its adaptation to large language models. It requires a solid understanding of optimization methods, backpropagation, and the architecture of large language models. The authors also provide a theoretical analysis to support their empirical findings, further adding to the technical depth of the paper.", "technical_score": 3, "enjoyable_analysis": "The paper is well-structured and presents a clear narrative, starting with the problem of memory-intensive backpropagation for large models, introducing the proposed solution (MeZO), and then validating it through comprehensive experiments. The combination of theoretical insights and empirical results makes for an engaging read for those with a technical background in the field.", "enjoyable_score": 2}