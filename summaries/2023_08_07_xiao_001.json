{"Published": "2023-08-07", "Title": "AgentBench: Evaluating LLMs as Agents", "Authors": "Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, Jie Tang", "Summary": "Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks. As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. Our extensive test over 25 LLMs (including APIs and open-sourced models) shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and open-sourced competitors. It also serves as a component of an ongoing project with wider coverage and deeper consideration towards systematic LLM evaluation. Datasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench", "main_contribution": {"headline": "AgentBench: A Benchmark for Evaluating LLMs as Agents", "description": "The paper introduces AgentBench, a multi-dimensional evolving benchmark designed to evaluate Large Language Models (LLMs) as agents in interactive environments. AgentBench comprises eight distinct environments that assess an LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. The authors conducted extensive tests on 25 LLMs, including both APIs and open-sourced models. The results revealed a significant performance disparity between top commercial LLMs and their open-sourced counterparts. AgentBench is part of a broader project aimed at providing a more comprehensive and systematic evaluation of LLMs."}, "takeaways": {"headline": "AgentBench Provides a Comprehensive Evaluation Framework for LLMs", "description": "AgentBench provides a valuable tool for evaluating the performance of LLMs as agents in interactive environments. It can be used to identify the strengths and weaknesses of different LLMs, which can guide the development and fine-tuning of these models. The benchmark can also help in identifying the most suitable LLM for a specific task or application. For example, if an application requires an LLM to act as an agent in a complex environment, the results from AgentBench can provide insights into which LLMs are most capable of handling such tasks.", "example": "For instance, if you are developing a chatbot that needs to interact with users in a multi-turn conversation, you can use AgentBench to evaluate different LLMs and choose the one that performs best in this type of interactive environment."}, "category": "BEHAVIOR", "novelty_analysis": "AgentBench represents a significant contribution to the field of LLM evaluation. While there have been previous attempts to evaluate LLMs, the introduction of a multi-dimensional evolving benchmark that assesses LLMs as agents in interactive environments is a novel approach.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, providing a detailed description of the AgentBench benchmark and the methodology used to evaluate the LLMs. However, the concepts and methodologies are explained clearly, making it accessible to readers with a basic understanding of LLMs and AI.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and provides a comprehensive overview of the AgentBench benchmark and its application in evaluating LLMs. The results are presented clearly, making it easy for readers to understand the performance of different LLMs in various environments.", "enjoyable_score": 2}