{"Published": "2023-02-20", "Title": "Adaptive Test Generation Using a Large Language Model", "Authors": "Max Sch\u00e4fer, Sarah Nadi, Aryaz Eghbali, Frank Tip", "Summary": "Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task, motivating the need for automation. This paper presents TestPilot, an adaptive test generation technique that leverages Large Language Models (LLMs). TestPilot uses Codex, an off-the-shelf LLM, to automatically generate unit tests for a given program without requiring additional training or few-shot learning on examples of existing tests. In our approach, Codex is provided with prompts that include the signature and implementation of a function under test, along with usage examples extracted from documentation. If a generated test fails, TestPilot's adaptive component attempts to generate a new test that fixes the problem by re-prompting the model with the failing test and error message. We created an implementation of TestPilot for JavaScript and evaluated it on 25 npm packages with a total of 1,684 API functions to generate tests for. Our results show that the generated tests achieve up to 93.1% statement coverage (median 68.2%). Moreover, on average, 58.5% of the generated tests contain at least one assertion that exercises functionality from the package under test. Our experiments with excluding parts of the information included in the prompts show that all components contribute towards the generation of effective test suites. Finally, we find that TestPilot does not generate memorized tests: 92.7% of our generated tests have $\\leq$ 50% similarity with existing tests (as measured by normalized edit distance), with none of them being exact copies.", "main_contribution": {"headline": "TestPilot: An Adaptive Test Generation Technique Using Large Language Models", "description": "The paper introduces TestPilot, an adaptive test generation technique that leverages Large Language Models (LLMs) to automate the creation of unit tests for software. TestPilot uses Codex, an off-the-shelf LLM, to generate tests based on prompts that include the function signature, implementation, and usage examples. If a test fails, TestPilot's adaptive component generates a new test by re-prompting the model with the failing test and error message. The technique was implemented for JavaScript and evaluated on 25 npm packages, achieving up to 93.1% statement coverage and demonstrating that the generated tests were not memorized but unique."}, "takeaways": {"headline": "TestPilot offers a promising approach to automated unit test generation", "description": "TestPilot's approach to automated test generation can significantly reduce the manual labor involved in creating unit tests, improving efficiency in software development. By leveraging the capabilities of LLMs, TestPilot can generate meaningful and unique tests, contributing to the robustness of the software. The adaptive component of TestPilot, which generates new tests based on failing ones, adds a layer of intelligence to the test generation process.", "example": "For instance, given a JavaScript function, TestPilot would generate a unit test based on the function's signature and implementation. If the test fails, TestPilot would re-prompt the LLM with the failing test and error message, generating a new test that addresses the failure."}, "category": "USE CASES", "novelty_analysis": "TestPilot introduces a novel approach to automated test generation by leveraging the capabilities of LLMs. The adaptive component, which generates new tests based on failing ones, is a unique feature that sets TestPilot apart from other test generation techniques.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, discussing the implementation of TestPilot and its evaluation on npm packages. However, it does not delve into complex mathematical theories or algorithms, making it accessible to readers with a basic understanding of software testing and LLMs.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents a novel and intriguing contribution to the field of automated test generation. The use of LLMs in this context is particularly interesting, and the results of the evaluation provide valuable insights.", "enjoyable_score": 3}