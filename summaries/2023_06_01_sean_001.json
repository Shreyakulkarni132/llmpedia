{"Published": "2023-06-01", "Title": "Do Large Language Models know what humans know?", "Authors": "Sean Trott, Cameron Jones, Tyler Chang, James Michaelov, Benjamin Bergen", "Summary": "Humans can attribute beliefs to others. However, it is unknown to what extent this ability results from an innate biological endowment or from experience accrued through child development, particularly exposure to language describing others' mental states. We test the viability of the language exposure hypothesis by assessing whether models exposed to large quantities of human language display sensitivity to the implied knowledge states of characters in written passages. In pre-registered analyses, we present a linguistic version of the False Belief Task to both human participants and a Large Language Model, GPT-3. Both are sensitive to others' beliefs, but while the language model significantly exceeds chance behavior, it does not perform as well as the humans, nor does it explain the full extent of their behavior -- despite being exposed to more language than a human would in a lifetime. This suggests that while statistical learning from language exposure may in part explain how humans develop the ability to reason about the mental states of others, other mechanisms are also responsible.", "main_contribution": "The paper investigates the ability of Large Language Models (LLMs), specifically GPT-3, to attribute beliefs to others, a key aspect of human cognition. The authors use a linguistic version of the False Belief Task to test whether LLMs can infer the knowledge states of characters in written passages. The results show that while GPT-3 can perform above chance, it does not match human performance, suggesting that other mechanisms beyond language exposure are responsible for humans' ability to reason about others' mental states.", "takeaways": "The findings suggest that while LLMs like GPT-3 can exhibit some level of understanding of others' beliefs, they do not fully replicate human performance in this area. This implies that practitioners should be cautious in assuming that LLMs can fully understand or represent human cognition, even when they have been exposed to vast amounts of language data. The study also highlights the importance of considering other mechanisms beyond language exposure when developing AI models that aim to mimic human cognition.", "novelty_analysis": "The paper presents a novel approach to testing the cognitive abilities of LLMs by using a linguistic version of the False Belief Task, a well-established tool in cognitive psychology. The findings contribute to the ongoing debate about the extent to which LLMs can replicate human cognition, providing new insights into the limitations of these models.", "novelty_score": 3, "category": "BEHAVIOR", "technical_analysis": "The paper is somewhat technical, as it involves concepts from cognitive psychology and AI. However, the authors explain these concepts in a clear and accessible manner, making the paper understandable for readers with a basic knowledge of these fields.", "technical_score": 2, "enjoyable_analysis": "The paper is well-written and presents an interesting and novel approach to testing the cognitive abilities of LLMs. The findings are insightful and contribute to our understanding of the capabilities and limitations of these models, making the paper enjoyable for readers interested in AI and cognitive psychology.", "enjoyable_score": 3}