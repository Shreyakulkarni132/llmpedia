{"Published": "2023-04-08", "Title": "Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change)", "Authors": "Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati", "Summary": "Recent advances in large language models (LLMs) have transformed the field of natural language processing (NLP). From GPT-3 to PaLM, the state-of-the-art performance on natural language tasks is being pushed forward with every new large language model. Along with natural language abilities, there has been a significant interest in understanding whether such models exhibit reasoning capabilities with the use of reasoning benchmarks. However, even though results are seemingly positive, these benchmarks prove to be simplistic in nature and the performance of LLMs on these benchmarks cannot be used as evidence to support, many a times outlandish, claims being made about LLMs' reasoning capabilities. Further, these only represent a very limited set of simple reasoning tasks and we need to look at more sophisticated reasoning problems if we are to measure the true limits of such LLM-based systems. Motivated by this, we propose an extensible assessment framework to test the capabilities of LLMs on reasoning about actions and change, a central aspect of human intelligence. We provide multiple test cases that are more involved than any of the previously established benchmarks and each test case evaluates a different aspect of reasoning about actions and change. Results on GPT-3 (davinci), Instruct-GPT3 (text-davinci-002) and BLOOM (176B), showcase subpar performance on such reasoning tasks.", "main_contribution": "The paper introduces an extensible assessment framework to evaluate the reasoning capabilities of Large Language Models (LLMs) about actions and change. This framework provides multiple test cases that are more complex than previously established benchmarks, with each test case evaluating a different aspect of reasoning about actions and change. The authors apply this framework to GPT-3, Instruct-GPT3, and BLOOM, revealing subpar performance on such reasoning tasks.", "takeaways": "The proposed framework provides a more sophisticated measure of LLMs' reasoning capabilities, moving beyond the simplistic nature of existing benchmarks. This allows for a more accurate and nuanced understanding of the limits of LLM-based systems. However, the results indicate that current LLMs still struggle with complex reasoning tasks, suggesting a need for further improvements in this area.", "novelty_analysis": "The introduction of a more sophisticated assessment framework for evaluating LLMs' reasoning capabilities represents a significant advancement in the field. The focus on reasoning about actions and change, a central aspect of human intelligence, is a unique and meaningful contribution.", "novelty_score": 3, "category": "BEHAVIOR", "technical_analysis": "The paper is somewhat technical, as it introduces a new assessment framework and applies it to evaluate the reasoning capabilities of LLMs. However, the concepts are explained clearly, making it accessible to those with a background in computer science or AI.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents a novel and interesting contribution to the field. However, the technical nature of the content and the focus on evaluation rather than application may limit its appeal to a broader audience.", "enjoyable_score": 2}