{"Published": "2023-04-08", "Title": "Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change)", "Authors": "Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati", "Summary": "Recent advances in large language models (LLMs) have transformed the field of natural language processing (NLP). From GPT-3 to PaLM, the state-of-the-art performance on natural language tasks is being pushed forward with every new large language model. Along with natural language abilities, there has been a significant interest in understanding whether such models exhibit reasoning capabilities with the use of reasoning benchmarks. However, even though results are seemingly positive, these benchmarks prove to be simplistic in nature and the performance of LLMs on these benchmarks cannot be used as evidence to support, many a times outlandish, claims being made about LLMs' reasoning capabilities. Further, these only represent a very limited set of simple reasoning tasks and we need to look at more sophisticated reasoning problems if we are to measure the true limits of such LLM-based systems. Motivated by this, we propose an extensible assessment framework to test the capabilities of LLMs on reasoning about actions and change, a central aspect of human intelligence. We provide multiple test cases that are more involved than any of the previously established benchmarks and each test case evaluates a different aspect of reasoning about actions and change. Results on GPT-3 (davinci), Instruct-GPT3 (text-davinci-002) and BLOOM (176B), showcase subpar performance on such reasoning tasks.", "main_contribution": {"headline": "New Assessment Framework for Evaluating Reasoning Capabilities of LLMs", "description": "The paper introduces a novel, extensible assessment framework designed to evaluate the reasoning capabilities of Large Language Models (LLMs), particularly their ability to reason about actions and change. This framework provides multiple test cases, each evaluating a different aspect of reasoning about actions and change. These test cases are more complex than previously established benchmarks, aiming to measure the true limits of LLM-based systems. The authors argue that existing benchmarks are too simplistic and cannot be used to support claims about LLMs' reasoning capabilities."}, "takeaways": {"headline": "LLMs Struggle with Complex Reasoning Tasks, Need for More Sophisticated Benchmarks", "description": "The paper highlights the limitations of LLMs in handling complex reasoning tasks, particularly those involving actions and change. It underscores the need for more sophisticated benchmarks to truly evaluate the reasoning capabilities of these models. The introduced assessment framework can be a valuable tool for LLM practitioners to evaluate and improve the reasoning capabilities of their models. However, the results indicate that current LLMs, including GPT-3 and BLOOM, show subpar performance on such complex reasoning tasks.", "example": "For instance, using the proposed framework, an LLM could be tested on a complex reasoning task such as predicting the outcome of a series of actions in a given scenario. The model's performance on this task would provide insights into its ability to reason about actions and change."}, "category": "BEHAVIOR", "novelty_analysis": "The paper presents a novel assessment framework for evaluating the reasoning capabilities of LLMs, which is a significant contribution to the field. The focus on complex reasoning tasks involving actions and change is a departure from the simplistic benchmarks commonly used, adding a new dimension to the evaluation of LLMs.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical as it introduces a new assessment framework and discusses its application in evaluating the reasoning capabilities of LLMs. However, it does not delve into complex mathematical theories or algorithms, making it accessible to readers with a basic understanding of LLMs.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting perspective on the limitations of LLMs and the need for more sophisticated benchmarks. However, the technical nature of the content and the focus on evaluation rather than application might limit its appeal to a broader audience.", "enjoyable_score": 2}