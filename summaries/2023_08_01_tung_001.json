{"Published": "2023-08-01", "Title": "Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors", "Authors": "Tung Phung, Victor-Alexandru P\u0103durean, Jos\u00e9 Cambronero, Sumit Gulwani, Tobias Kohn, Rupak Majumdar, Adish Singla, Gustavo Soares", "Summary": "Generative AI and large language models hold great promise in enhancing computing education by powering next-generation educational technologies for introductory programming. Recent works have studied these models for different scenarios relevant to programming education; however, these works are limited for several reasons, as they typically consider already outdated models or only specific scenario(s). Consequently, there is a lack of a systematic study that benchmarks state-of-the-art models for a comprehensive set of programming education scenarios. In our work, we systematically evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human tutors for a variety of scenarios. We evaluate using five introductory Python programming problems and real-world buggy programs from an online platform, and assess performance using expert-based annotations. Our results show that GPT-4 drastically outperforms ChatGPT (based on GPT-3.5) and comes close to human tutors' performance for several scenarios. These results also highlight settings where GPT-4 still struggles, providing exciting future directions on developing techniques to improve the performance of these models.", "main_contribution": {"headline": "Benchmarking Large Language Models for Programming Education", "description": "The paper presents a systematic evaluation of two large language models (LLMs), ChatGPT (based on GPT-3.5) and GPT-4, in the context of programming education. The authors compare the performance of these models with human tutors across six different scenarios: program repair, hint generation, grading feedback, pair programming, contextualized explanation, and task synthesis. The evaluation is conducted using five introductory Python programming problems and real-world buggy programs from an online platform. The results show that GPT-4 significantly outperforms ChatGPT and comes close to the performance of human tutors in several scenarios. However, the study also identifies areas where GPT-4 struggles, suggesting future research directions for improving these models."}, "takeaways": {"headline": "Large Language Models Show Promise in Programming Education", "description": "The study demonstrates the potential of large language models, particularly GPT-4, in enhancing programming education. The models can act as digital tutors, assistants, or peers, providing assistance in various scenarios such as program repair, hint generation, and grading feedback. However, the performance of these models still falls short in more challenging scenarios like grading feedback and task synthesis. The findings suggest that while LLMs can be powerful tools in programming education, there is still room for improvement. For practitioners, this means that while LLMs can be used to assist in teaching programming, human intervention is still necessary in certain scenarios.", "example": "For instance, in a programming class, an LLM like GPT-4 could be used to provide hints to students when they are stuck on a problem or to help repair buggy programs. However, when it comes to providing grading feedback or synthesizing new tasks, human tutors might still be needed to ensure the quality and appropriateness of the feedback or tasks."}, "category": "USE CASES", "novelty_analysis": "The paper provides a comprehensive benchmarking of state-of-the-art LLMs in the context of programming education, which is a novel contribution. While previous works have studied LLMs for different scenarios relevant to programming education, they typically considered outdated models or only specific scenarios. This study fills this gap by systematically evaluating and comparing the performance of ChatGPT and GPT-4 with human tutors across a variety of scenarios.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, as it involves a detailed evaluation of LLMs in the context of programming education. It requires a basic understanding of LLMs and programming concepts. However, the authors do a good job of explaining the evaluation process and the scenarios used in the study, making it accessible to readers with a basic understanding of AI and programming.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting application of LLMs in programming education. The systematic evaluation and comparison of the models across different scenarios provide valuable insights. However, the paper might be a bit dense for readers without a background in AI or programming.", "enjoyable_score": 2}