{"Published": "2023-08-14", "Title": "OctoPack: Instruction Tuning Code Large Language Models", "Authors": "Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, Shayne Longpre", "Summary": "Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.", "main_contribution": {"headline": "OctoPack: Leveraging Git Commits for Instruction Tuning of Large Language Models", "description": "The paper introduces OctoPack, a novel approach to fine-tuning large language models (LLMs) using code instructions derived from Git commits. The authors compile a massive dataset, CommitPack, consisting of 4 terabytes of Git commits across 350 programming languages. This dataset is used to instruction-tune LLMs, which leads to significant performance improvements on natural language tasks. The authors also introduce HumanEvalPack, an expanded benchmark for coding tasks. The models OctoCoder and OctoGeeX, trained using OctoPack, demonstrate superior performance across this benchmark, indicating the effectiveness of CommitPack in generalizing to a wide range of languages and coding tasks."}, "takeaways": {"headline": "Instruction Tuning with Git Commits Enhances LLM Performance on Coding Tasks", "description": "The OctoPack approach can be used to improve the performance of LLMs on coding tasks by leveraging the natural structure of Git commits for instruction tuning. This method can be particularly useful for tasks that require code generation or explanation. The models OctoCoder and OctoGeeX, trained using OctoPack, have shown superior performance on the HumanEvalPack benchmark, indicating their potential for real-world applications. The freely available code, models, and data can be used by practitioners to replicate or build upon this work.", "example": "For instance, an LLM trained using OctoPack could be used to develop an intelligent code review tool that not only identifies potential issues in the code but also suggests fixes or improvements based on the patterns learned from the Git commits."}, "category": "FINE-TUNING", "novelty_analysis": "The paper presents a novel approach to instruction tuning of LLMs using code instructions derived from Git commits. This method, encapsulated in OctoPack, represents a significant departure from traditional methods of fine-tuning LLMs and introduces a new way of leveraging existing code repositories for model training.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, discussing the process of compiling the CommitPack dataset and the methodology for instruction tuning of LLMs. However, it does not delve into complex mathematical theories or algorithms, making it accessible to readers with a basic understanding of machine learning and coding.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting approach to fine-tuning LLMs. The use of Git commits for instruction tuning is a novel idea, and the impressive results achieved by the OctoCoder and OctoGeeX models make for an engaging read.", "enjoyable_score": 2}