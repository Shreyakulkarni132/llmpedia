{"Published": "2022-05-17", "Title": "SKILL: Structured Knowledge Infusion for Large Language Models", "Authors": "Fedor Moiseev, Zhe Dong, Enrique Alfonseca, Martin Jaggi", "Summary": "Large language models (LLMs) have demonstrated human-level performance on a vast spectrum of natural language tasks. However, it is largely unexplored whether they can better internalize knowledge from a structured data, such as a knowledge graph, or from text. In this work, we propose a method to infuse structured knowledge into LLMs, by directly training T5 models on factual triples of knowledge graphs (KGs). We show that models pre-trained on Wikidata KG with our method outperform the T5 baselines on FreebaseQA and WikiHop, as well as the Wikidata-answerable subset of TriviaQA and NaturalQuestions. The models pre-trained on factual triples compare competitively with the ones on natural language sentences that contain the same knowledge. Trained on a smaller size KG, WikiMovies, we saw 3x improvement of exact match score on MetaQA task compared to T5 baseline. The proposed method has an advantage that no alignment between the knowledge graph and text corpus is required in curating training data. This makes our method particularly useful when working with industry-scale knowledge graphs.", "main_contribution": {"headline": "SKILL: A method for infusing structured knowledge into Large Language Models", "description": "The paper introduces SKILL (Structured Knowledge Infusion for Large Language Models), a novel method for infusing structured knowledge from knowledge graphs (KGs) into LLMs. Unlike previous methods that require alignment between the KG and text corpus, SKILL directly trains T5 models on factual triples of KGs. This approach is particularly beneficial when working with industry-scale KGs. The authors demonstrate that models pre-trained with SKILL on Wikidata KG outperform T5 baselines on several tasks and compare competitively with models trained on natural language sentences containing the same knowledge."}, "takeaways": {"headline": "SKILL provides a direct and efficient way to infuse structured knowledge into LLMs", "description": "SKILL offers a new way to enhance the performance of LLMs by directly training them on factual triples from KGs. This method eliminates the need for alignment between the KG and text corpus, simplifying the training process and making it more efficient, especially for industry-scale KGs. Practitioners can use SKILL to improve the performance of LLMs on tasks that require structured knowledge, such as question answering and fact checking.", "example": "For instance, to train an LLM for a question answering task, one could use SKILL to pre-train the model on a KG like Wikidata. The factual triples from the KG would provide the model with a wealth of structured knowledge, potentially improving its ability to answer questions accurately."}, "category": "TRAINING", "novelty_analysis": "The paper presents a novel method, SKILL, for infusing structured knowledge into LLMs. This method is unique in that it directly trains models on factual triples from KGs, eliminating the need for alignment between the KG and text corpus. This represents a significant advancement in the field of LLM training.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, as it introduces a new method for training LLMs and provides detailed experimental results. However, it does not delve into complex mathematical theories or algorithms, making it accessible to readers with a background in machine learning.", "technical_score": 2, "enjoyable_analysis": "The paper is well-written and presents a novel and interesting approach to LLM training. The experimental results provide compelling evidence for the effectiveness of the proposed method, making the paper an engaging read for those interested in LLMs and KGs.", "enjoyable_score": 2}