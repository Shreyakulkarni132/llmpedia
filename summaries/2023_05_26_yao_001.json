{"Published": "2023-05-26", "Title": "Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models", "Authors": "Yao Yao, Zuchao Li, Hai Zhao", "Summary": "With the widespread use of large language models (LLMs) in NLP tasks, researchers have discovered the potential of Chain-of-thought (CoT) to assist LLMs in accomplishing complex reasoning tasks by generating intermediate steps. However, human thought processes are often non-linear, rather than simply sequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT) reasoning, which models human thought processes not only as a chain but also as a graph. By representing thought units as nodes and connections between them as edges, our approach captures the non-sequential nature of human thinking and allows for a more realistic modeling of thought processes. Similar to Multimodal-CoT, we modeled GoT reasoning as a two-stage framework, generating rationales first and then producing the final answer. Specifically, we employ an additional graph-of-thoughts encoder for GoT representation learning and fuse the GoT representation with the original input representation through a gated fusion mechanism. We implement a GoT reasoning model on the T5 pre-trained model and evaluate its performance on a text-only reasoning task (GSM8K) and a multimodal reasoning task (ScienceQA). Our model achieves significant improvement over the strong CoT baseline with 3.41% and 5.08% on the GSM8K test set with T5-base and T5-large architectures, respectively. Additionally, our model boosts accuracy from 84.91% to 91.54% using the T5-base model and from 91.68% to 92.77% using the T5-large model over the state-of-the-art Multimodal-CoT on the ScienceQA test set. Experiments have shown that GoT achieves comparable results to Multimodal-CoT(large) with over 700M parameters, despite having fewer than 250M backbone model parameters, demonstrating the effectiveness of GoT.", "main_contribution": {"headline": "Graph-of-Thought (GoT) reasoning enhances LLMs' complex reasoning capabilities", "description": "The paper introduces Graph-of-Thought (GoT) reasoning, a novel approach to model human thought processes in Large Language Models (LLMs). Unlike the Chain-of-Thought (CoT) method, which models thought processes as sequential chains, GoT models them as a graph, capturing the non-sequential nature of human thinking. The authors implement a GoT reasoning model on the T5 pre-trained model and demonstrate its effectiveness in complex reasoning tasks. The GoT model outperforms the CoT baseline and the state-of-the-art Multimodal-CoT, despite having fewer model parameters."}, "takeaways": {"headline": "GoT reasoning offers a more realistic modeling of thought processes in LLMs", "description": "The Graph-of-Thought (GoT) reasoning approach provides a more realistic modeling of human thought processes, enhancing the complex reasoning capabilities of LLMs. This approach could be particularly useful in tasks that require non-linear reasoning, such as problem-solving or creative tasks. The GoT model's superior performance, despite having fewer parameters, also suggests potential efficiency gains in terms of computational resources.", "example": "For instance, in a problem-solving task, an LLM using GoT reasoning could generate a graph of interconnected thoughts, rather than a linear chain. This could allow the model to explore multiple possible solutions simultaneously and identify the most promising one, potentially leading to more effective problem-solving."}, "category": "PROMPTING", "novelty_analysis": "The introduction of Graph-of-Thought (GoT) reasoning represents a significant advancement in the field of LLMs. By modeling human thought processes as a graph rather than a linear chain, the authors provide a more realistic and effective approach to complex reasoning tasks.", "novelty_score": 3, "technical_analysis": "The paper is highly technical, detailing the implementation of the GoT reasoning model on the T5 pre-trained model and its evaluation on complex reasoning tasks. It requires a solid understanding of LLMs and their applications in natural language processing tasks.", "technical_score": 3, "enjoyable_analysis": "The paper presents a novel and intriguing contribution to the field of LLMs, making it an interesting read. However, the high level of technical detail might make it challenging for readers without a strong background in the field.", "enjoyable_score": 2}