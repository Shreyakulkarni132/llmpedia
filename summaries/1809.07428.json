{"Published": "2018-09-19", "Title": "Ranking Distillation: Learning Compact Ranking Models With High Performance for Recommender System", "Authors": "Jiaxi Tang, Ke Wang", "Summary": "We propose a novel way to train ranking models, such as recommender systems, that are both effective and efficient. Knowledge distillation (KD) was shown to be successful in image recognition to achieve both effectiveness and efficiency. We propose a KD technique for learning to rank problems, called \\emph{ranking distillation (RD)}. Specifically, we train a smaller student model to learn to rank documents/items from both the training data and the supervision of a larger teacher model. The student model achieves a similar ranking performance to that of the large teacher model, but its smaller model size makes the online inference more efficient. RD is flexible because it is orthogonal to the choices of ranking models for the teacher and student. We address the challenges of RD for ranking problems. The experiments on public data sets and state-of-the-art recommendation models showed that RD achieves its design purposes: the student model learnt with RD has a model size less than half of the teacher model while achieving a ranking performance similar to the teacher model and much better than the student model learnt without RD.", "main_contribution": {"headline": "Ranking Distillation: A novel method for efficient and effective ranking models", "description": "The paper introduces a new technique called Ranking Distillation (RD) for training ranking models, such as recommender systems, that are both effective and efficient. RD is a knowledge distillation (KD) technique specifically designed for learning to rank problems. It involves training a smaller student model to learn to rank items from both the training data and the supervision of a larger teacher model. The student model achieves a similar ranking performance to the teacher model but with a smaller model size, making online inference more efficient. RD is flexible and can be applied regardless of the choice of ranking models for the teacher and student."}, "takeaways": {"headline": "Ranking Distillation offers a path to more efficient recommender systems", "description": "The Ranking Distillation technique can be used to create more efficient recommender systems without sacrificing performance. By training a smaller student model under the supervision of a larger teacher model, the student model can achieve similar ranking performance with a smaller model size. This can be particularly useful in scenarios where computational resources are limited or where quick online inference is required. The flexibility of RD also means it can be applied to a variety of ranking models.", "example": "For instance, an e-commerce platform could use RD to train a compact recommender system that suggests products to customers based on their browsing history. The student model would learn from both the training data and the larger teacher model, resulting in a system that performs similarly to the teacher model but with faster online inference."}, "category": "TRAINING", "novelty_analysis": "The paper presents a novel approach to training ranking models, introducing the concept of Ranking Distillation. This technique, which applies knowledge distillation to learning to rank problems, is a unique contribution to the field.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, discussing the details of the Ranking Distillation technique and its application to recommender systems. However, it does not delve into complex mathematical theories or algorithms, making it accessible to readers with a basic understanding of machine learning.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents a novel and interesting approach to training ranking models. The practical implications of the research are clearly outlined, making it an engaging read for those interested in recommender systems and machine learning.", "enjoyable_score": 2}