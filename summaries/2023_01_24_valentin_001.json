{"Published": "2023-01-24", "Title": "Can large language models reason about medical questions?", "Authors": "Valentin Li\u00e9vin, Christoffer Egeberg Hother, Ole Winther", "Summary": "Although large language models (LLMs) often produce impressive outputs, it remains unclear how they perform in real-world scenarios requiring strong reasoning skills and expert domain knowledge. We set out to investigate whether GPT-3.5 (Codex and InstructGPT) can be applied to answer and reason about difficult real-world-based questions. We utilize two multiple-choice medical exam questions (USMLE and MedMCQA) and a medical reading comprehension dataset (PubMedQA). We investigate multiple prompting scenarios: Chain-of-Thought (CoT, think step-by-step), zero- and few-shot (prepending the question with question-answer exemplars) and retrieval augmentation (injecting Wikipedia passages into the prompt). For a subset of the USMLE questions, a medical expert reviewed and annotated the model's CoT. We found that InstructGPT can often read, reason and recall expert knowledge. Failure are primarily due to lack of knowledge and reasoning errors and trivial guessing heuristics are observed, e.g.\\ too often predicting labels A and D on USMLE. Sampling and combining many completions overcome some of these limitations. Using 100 samples, Codex 5-shot CoT not only gives close to well-calibrated predictive probability but also achieves human-level performances on the three datasets. USMLE: 60.2%, MedMCQA: 62.7% and PubMedQA: 78.2%.", "main_contribution": "The paper investigates the ability of GPT-3.5 (Codex and InstructGPT) to answer and reason about difficult real-world-based medical questions. The authors utilize two multiple-choice medical exam questions (USMLE and MedMCQA) and a medical reading comprehension dataset (PubMedQA). They explore multiple prompting scenarios, including Chain-of-Thought (CoT), zero- and few-shot, and retrieval augmentation. The study finds that InstructGPT can often read, reason, and recall expert knowledge, with failures primarily due to lack of knowledge and reasoning errors.", "takeaways": "The study demonstrates that LLMs can be used to answer complex medical questions, achieving human-level performances on three datasets. This suggests that LLMs could be used in real-world scenarios requiring strong reasoning skills and expert domain knowledge. However, the study also highlights the limitations of LLMs, such as their tendency to make reasoning errors and their reliance on trivial guessing heuristics.", "novelty_analysis": "The paper presents a novel investigation into the use of LLMs for answering complex medical questions. While LLMs have been used in various domains, their application in the medical field, especially in the context of reasoning about complex questions, is relatively unexplored. The study's findings contribute to our understanding of the capabilities and limitations of LLMs in this domain.", "novelty_score": 2, "category": "USE CASES", "technical_analysis": "The paper is somewhat technical, as it involves the use of LLMs for complex reasoning tasks and the exploration of various prompting scenarios. However, the concepts are explained clearly, making it accessible to readers with a background in AI and machine learning.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting investigation into the use of LLMs for medical reasoning. The findings are insightful and have practical implications, making the paper enjoyable for readers interested in the application of AI in the medical field.", "enjoyable_score": 3}