{"Published": "2023-01-24", "Title": "Can large language models reason about medical questions?", "Authors": "Valentin Li\u00e9vin, Christoffer Egeberg Hother, Ole Winther", "Summary": "Although large language models (LLMs) often produce impressive outputs, it remains unclear how they perform in real-world scenarios requiring strong reasoning skills and expert domain knowledge. We set out to investigate whether GPT-3.5 (Codex and InstructGPT) can be applied to answer and reason about difficult real-world-based questions. We utilize two multiple-choice medical exam questions (USMLE and MedMCQA) and a medical reading comprehension dataset (PubMedQA). We investigate multiple prompting scenarios: Chain-of-Thought (CoT, think step-by-step), zero- and few-shot (prepending the question with question-answer exemplars) and retrieval augmentation (injecting Wikipedia passages into the prompt). For a subset of the USMLE questions, a medical expert reviewed and annotated the model's CoT. We found that InstructGPT can often read, reason and recall expert knowledge. Failure are primarily due to lack of knowledge and reasoning errors and trivial guessing heuristics are observed, e.g.\\ too often predicting labels A and D on USMLE. Sampling and combining many completions overcome some of these limitations. Using 100 samples, Codex 5-shot CoT not only gives close to well-calibrated predictive probability but also achieves human-level performances on the three datasets. USMLE: 60.2%, MedMCQA: 62.7% and PubMedQA: 78.2%.", "main_contribution": {"headline": "Large Language Models can reason about medical questions with human-level performance", "description": "The paper investigates the ability of Large Language Models (LLMs), specifically GPT-3.5 (Codex and InstructGPT), to answer and reason about real-world-based medical questions. The authors use multiple-choice medical exam questions and a medical reading comprehension dataset for this purpose. They explore various prompting scenarios, including Chain-of-Thought (CoT), zero- and few-shot, and retrieval augmentation. The study finds that InstructGPT can often read, reason, and recall expert knowledge, with failures primarily due to lack of knowledge and reasoning errors. By sampling and combining many completions, some of these limitations can be overcome. The paper demonstrates that Codex 5-shot CoT achieves human-level performances on the three datasets used."}, "takeaways": {"headline": "LLMs can be effectively used for medical reasoning tasks", "description": "The paper's findings suggest that LLMs can be effectively used for medical reasoning tasks, achieving human-level performance. This opens up possibilities for the use of LLMs in medical applications, such as automated medical question answering systems, medical exam preparation tools, and potentially even diagnostic aids. The study also highlights the importance of appropriate prompting and the use of techniques like CoT and retrieval augmentation to improve the performance of LLMs.", "example": "For instance, an LLM like InstructGPT could be used to develop a medical question answering system. The system could use a combination of CoT, zero- and few-shot prompting, and retrieval augmentation to answer complex medical questions. For example, given a question about a specific medical condition, the system could use CoT to reason through the answer step-by-step, use zero- and few-shot prompting to provide examples of similar questions and their answers, and use retrieval augmentation to inject relevant medical information into the prompt."}, "category": "USE CASES", "novelty_analysis": "The paper's main contribution lies in its application of LLMs to the field of medical reasoning, demonstrating that these models can achieve human-level performance on medical questions. While the use of LLMs in various domains is not new, their application in the medical field, especially in the context of reasoning about complex medical questions, is a significant contribution.", "novelty_score": 2, "technical_analysis": "The paper is somewhat technical, as it delves into the specifics of how different prompting scenarios are used and how they affect the performance of the LLMs. However, it does not delve into the underlying algorithms or mathematical theories, making it accessible to readers with a basic understanding of LLMs.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting application of LLMs in the medical field. The use of real-world medical questions and datasets adds a practical dimension to the study, making it an engaging read for those interested in the application of AI in healthcare.", "enjoyable_score": 2}