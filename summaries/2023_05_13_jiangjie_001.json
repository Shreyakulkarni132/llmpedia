{"Published": "2023-05-13", "Title": "Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge", "Authors": "Jiangjie Chen, Wei Shi, Ziquan Fu, Sijie Cheng, Lei Li, Yanghua Xiao", "Summary": "Large language models (LLMs) have been widely studied for their ability to store and utilize positive knowledge. However, negative knowledge, such as \"lions don't live in the ocean\", is also ubiquitous in the world but rarely mentioned explicitly in the text. What do LLMs know about negative knowledge? This work examines the ability of LLMs to negative commonsense knowledge. We design a constrained keywords-to-sentence generation task (CG) and a Boolean question-answering task (QA) to probe LLMs. Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions. We term this phenomenon the belief conflict of LLMs. Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.", "main_contribution": "The paper investigates the ability of Large Language Models (LLMs) to handle negative commonsense knowledge. The authors designed two tasks, a constrained keywords-to-sentence generation task (CG) and a Boolean question-answering task (QA), to probe the LLMs. The results showed that while LLMs can correctly answer yes-or-no questions, they often fail to generate valid sentences grounded in negative commonsense knowledge. The authors termed this phenomenon as the 'belief conflict' of LLMs.", "takeaways": "The findings of this paper highlight a significant limitation in the current LLMs' ability to handle negative commonsense knowledge. This could have implications for the development and application of LLMs, particularly in tasks that require understanding and generating sentences based on negative commonsense knowledge. The identification of the 'belief conflict' phenomenon could guide future research in addressing this limitation.", "novelty_analysis": "The paper presents a novel investigation into the handling of negative commonsense knowledge by LLMs. The introduction of the 'belief conflict' phenomenon and the design of the probing tasks provide new insights into the limitations of LLMs.", "novelty_score": 3, "category": "BEHAVIOR", "technical_analysis": "The paper is somewhat technical, discussing the design of probing tasks and the analysis of LLMs' performance. However, the concepts are explained clearly, making it accessible to readers with a background in AI and machine learning.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting investigation into a less-explored aspect of LLMs. The findings are insightful and could stimulate further research in the field.", "enjoyable_score": 2}