{"Published": "2022-11-10", "Title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale", "Authors": "Tim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer", "Summary": "Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our software.", "main_contribution": {"headline": "LLM.int8(): A novel 8-bit quantization procedure for transformers that retains full precision performance", "description": "The paper introduces LLM.int8(), a novel two-part quantization procedure for transformers that reduces the memory needed for inference by half while retaining full precision performance. The procedure first uses vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. For the emergent outliers, a new mixed-precision decomposition scheme is introduced, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9% of values are multiplied in 8-bit. This method allows a 175B parameter 16/32-bit checkpoint to be loaded, converted to Int8, and used immediately without performance degradation."}, "takeaways": {"headline": "LLM.int8() makes large language models more accessible by reducing memory requirements", "description": "The LLM.int8() procedure can be a game-changer for practitioners working with large language models (LLMs). By reducing the memory needed for inference by half, it makes such models much more accessible, even on a single server with consumer GPUs. This could potentially lead to cost savings and efficiency improvements in various applications of LLMs. Furthermore, the procedure does not degrade performance, ensuring that the benefits of large models are retained.", "example": "For instance, if you have a 175B parameter model, you can load it, convert it to Int8 using LLM.int8(), and use it immediately for inference. This can be done even on a single server with consumer GPUs, making it more accessible and cost-effective."}, "category": "TRAINING", "novelty_analysis": "The paper presents a novel approach to quantization in transformers, addressing the challenge of memory requirements in large language models. The introduction of a mixed-precision decomposition scheme for handling outlier features is a unique contribution that sets this work apart from previous studies.", "novelty_score": 3, "technical_analysis": "The paper is highly technical, delving into the intricacies of the proposed quantization procedure and its implementation. It requires a solid understanding of transformers, matrix multiplication, and quantization techniques.", "technical_score": 3, "enjoyable_analysis": "The paper is well-structured and presents a significant contribution to the field. However, the high level of technical detail and the focus on a specific aspect of LLMs may limit its appeal to a broader audience.", "enjoyable_score": 2}