{"Published": "2023-06-23", "Title": "Long-range Language Modeling with Self-retrieval", "Authors": "Ohad Rubin, Jonathan Berant", "Summary": "Retrieval-augmented language models (LMs) have received much attention recently. However, typically the retriever is not trained jointly as a native component of the LM, but added to an already-pretrained LM, which limits the ability of the LM and the retriever to adapt to one another. In this work, we propose the Retrieval-Pretrained Transformer (RPT), an architecture and training procedure for jointly training a retrieval-augmented LM from scratch for the task of modeling long texts. Given a recently generated text chunk in a long document, the LM computes query representations, which are then used to retrieve earlier chunks in the document, located potentially tens of thousands of tokens before. Information from retrieved chunks is fused into the LM representations to predict the next target chunk. We train the retriever component with a semantic objective, where the goal is to retrieve chunks that increase the probability of the next chunk, according to a reference LM. We evaluate RPT on four long-range language modeling tasks, spanning books, code, and mathematical writing, and demonstrate that RPT improves retrieval quality and subsequently perplexity across the board compared to strong baselines.", "main_contribution": {"headline": "Introduction of Retrieval-Pretrained Transformer (RPT) for long-range language modeling", "description": "The paper introduces the Retrieval-Pretrained Transformer (RPT), a novel architecture and training procedure for jointly training a retrieval-augmented language model from scratch. Unlike previous models where the retriever is added to an already-pretrained language model, RPT integrates the retriever as a native component, allowing it to adapt alongside the language model. The RPT takes a chunk of text, retrieves semantically relevant chunks from the past, and fuses these retrieved chunks into its representations to predict the next chunk. The retriever component is trained with a semantic objective, aiming to retrieve chunks that increase the probability of the next chunk according to a reference language model."}, "takeaways": {"headline": "RPT offers improved retrieval quality and perplexity for long-range language modeling", "description": "The RPT model presents a significant advancement in the field of long-range language modeling. By jointly training the retriever and language model, RPT improves retrieval quality and subsequently perplexity across various tasks. This approach can be particularly beneficial in applications dealing with long texts, such as book summarization, code generation, or mathematical writing. The joint training approach also allows for better adaptation between the retriever and the language model, potentially leading to more accurate and contextually relevant predictions.", "example": "For instance, when working with a long document, RPT can be used to generate a query representation from a recently generated text chunk. This query is then used to retrieve earlier chunks in the document, potentially located tens of thousands of tokens before. The information from these retrieved chunks is then fused into the language model representations to predict the next target chunk."}, "category": "ARCHITECTURES", "novelty_analysis": "The paper presents a novel approach to retrieval-augmented language modeling by introducing the Retrieval-Pretrained Transformer (RPT). This model differs from previous works by jointly training the retriever and language model from scratch, allowing for better adaptation between the two components. This approach is a significant departure from previous models where the retriever is added to an already-pretrained language model.", "novelty_score": 3, "technical_analysis": "The paper is highly technical, detailing the architecture and training procedure of the Retrieval-Pretrained Transformer (RPT). It requires a solid understanding of language models, retrieval-augmented language models, and the concept of joint training. The paper also delves into the semantic objective used to train the retriever component, adding another layer of complexity.", "technical_score": 3, "enjoyable_analysis": "The paper is well-structured and presents a novel approach to long-range language modeling. The introduction of the Retrieval-Pretrained Transformer (RPT) and its potential applications make for an interesting read. However, the high level of technical detail may make it challenging for readers without a strong background in the field.", "enjoyable_score": 2}