{"Published": "2023-06-13", "Title": "Large Language Models Are Reasoning Teachers", "Authors": "Namgyu Ho, Laura Schmid, Se-Young Yun", "Summary": "Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper, we use these large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude. We propose Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tune smaller models. We evaluate our method on a wide range of public models and complex tasks. We find that Fine-tune-CoT enables substantial reasoning capability in small models, far outperforming prompt-based baselines and even the teacher model in many tasks. Additionally, we extend our method by leveraging the teacher model's ability to generate multiple distinct rationales for each original sample. Enriching the fine-tuning data with such diverse reasoning results in a substantial performance boost across datasets, even for very small models. We conduct ablations and sample studies to understand the emergence of reasoning capabilities of student models. Our code implementation and data are available at https://github.com/itsnamgyu/reasoning-teacher.", "main_contribution": {"headline": "Fine-tune-CoT: A method to enable complex reasoning in smaller models", "description": "The paper introduces Fine-tune-CoT, a novel method that uses large language models as reasoning teachers to enable complex reasoning in smaller models. This method generates reasoning samples from very large teacher models to fine-tune smaller models, reducing model size requirements by several orders of magnitude. The authors demonstrate that Fine-tune-CoT significantly enhances the reasoning capability of small models, outperforming prompt-based baselines and even the teacher model in many tasks. The method is further extended by leveraging the teacher model's ability to generate multiple distinct rationales for each original sample, which when used to enrich the fine-tuning data, results in a substantial performance boost across datasets, even for very small models."}, "takeaways": {"headline": "Fine-tune-CoT offers a scalable solution for complex reasoning in LLMs", "description": "Fine-tune-CoT presents a practical solution for LLM practitioners looking to deploy complex reasoning capabilities at scale. By using large models as reasoning teachers, smaller models can be fine-tuned to perform complex tasks, reducing the computational requirements and inference costs associated with deploying large models. The method's ability to leverage multiple distinct rationales for each original sample further enhances the performance of the fine-tuned models. This approach could be particularly useful in applications where reasoning capabilities are required but computational resources are limited.", "example": "For instance, a small model could be fine-tuned using Fine-tune-CoT to perform complex reasoning tasks in a real-time chatbot application, where quick response times are crucial and computational resources may be limited."}, "category": "FINE-TUNING", "novelty_analysis": "The paper presents a novel approach to enabling complex reasoning in smaller models by using large models as reasoning teachers. The introduction of Fine-tune-CoT and its extension to leverage multiple distinct rationales for each original sample represent significant advancements in the field.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, detailing the workings of the Fine-tune-CoT method and its extension. However, the concepts are explained clearly and should be accessible to readers with a background in machine learning.", "technical_score": 2, "enjoyable_analysis": "The paper is well-written and presents an interesting solution to a significant challenge in the field of large language models. The clear explanations and practical implications make it an enjoyable read for those interested in the application of LLMs.", "enjoyable_score": 3}