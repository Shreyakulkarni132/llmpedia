{"Published": "2023-02-16", "Title": "Talking About Large Language Models", "Authors": "Murray Shanahan", "Summary": "Thanks to rapid progress in artificial intelligence, we have entered an era when technology and philosophy intersect in interesting ways. Sitting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to anthropomorphism, to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as \"knows\", \"believes\", and \"thinks\", when describing these systems. To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere.", "main_contribution": {"headline": "A philosophical perspective on anthropomorphizing Large Language Models", "description": "This paper does not introduce a new algorithm or technique, but rather provides a philosophical perspective on the anthropomorphization of Large Language Models (LLMs). The author argues that as LLMs become more adept at mimicking human language, there is a tendency to view these systems as more human-like than they actually are. The paper advocates for a practice of continually reminding ourselves of how LLMs actually work to avoid this anthropomorphization. The author hopes that this increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence."}, "takeaways": {"headline": "A cautionary note on anthropomorphizing LLMs", "description": "For practitioners working with LLMs, this paper serves as a reminder to avoid anthropomorphizing these models. It emphasizes the importance of understanding the underlying workings of LLMs and not attributing human-like qualities or capabilities to them that they do not possess. This perspective can help practitioners communicate more accurately about LLMs to the public and other stakeholders, and make more informed decisions about their deployment.", "example": "For instance, when describing the behavior of an LLM, instead of saying 'the model thinks', it would be more accurate to say 'the model predicts' or 'the model generates'. This helps to avoid the misconception that the model has human-like cognitive abilities."}, "category": "BEHAVIOR", "novelty_analysis": "The paper does not present new technical contributions, but offers a novel philosophical perspective on the discourse around LLMs. It provides a fresh viewpoint on the anthropomorphization of LLMs, which is not commonly discussed in the field.", "novelty_score": 3, "technical_analysis": "The paper is not technical in nature. It does not delve into the technical workings of LLMs, but rather discusses the philosophical implications of their increasing proficiency in mimicking human language.", "technical_score": 1, "enjoyable_analysis": "The paper is well-written and provides an interesting philosophical perspective on a topic that is often discussed in purely technical terms. It offers a refreshing viewpoint and encourages thoughtful reflection on the discourse around LLMs.", "enjoyable_score": 3}