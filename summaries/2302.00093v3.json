{"Published": "2023-06-06", "Title": "Large Language Models Can Be Easily Distracted by Irrelevant Context", "Authors": "Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Sch\u00e4rli, Denny Zhou", "Summary": "Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.", "main_contribution": {"headline": "Investigation of Large Language Models' Distractibility and Mitigation Strategies", "description": "This paper investigates the distractibility of large language models (LLMs), specifically how their problem-solving accuracy can be influenced by irrelevant context. The authors introduce a new benchmark, Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. The study reveals that the performance of LLMs decreases significantly when irrelevant information is included in the input. The authors propose several mitigation strategies, such as decoding with self-consistency and adding an instruction to the prompt that tells the LLM to ignore the irrelevant information."}, "takeaways": {"headline": "Understanding LLM Distractibility Can Improve Model Robustness", "description": "The paper's findings highlight the importance of understanding and addressing the distractibility of LLMs in real-world applications where irrelevant information is often present. The proposed mitigation strategies, such as decoding with self-consistency and adding specific instructions to the prompt, can be used to improve the robustness of LLMs. For example, when using an LLM for a task like document summarization, one could add a prompt instruction to ignore irrelevant details or use a self-consistency decoding approach to ensure the model stays focused on the main points.", "example": "For instance, when using an LLM to summarize a document, you could add a prompt instruction like 'Ignore irrelevant details and focus on the main points.' Or, you could use a self-consistency decoding approach where the model checks its own output for consistency and relevance."}, "category": "BEHAVIOR", "novelty_analysis": "The paper presents a novel investigation into the distractibility of LLMs, a topic that has not been extensively studied before. The introduction of the GSM-IC benchmark and the proposed mitigation strategies represent significant contributions to the field.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, discussing the construction of a new benchmark and the implementation of various mitigation strategies. However, it does not delve into complex mathematical theories or algorithms, making it accessible to readers with a basic understanding of LLMs.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting investigation into a less-studied aspect of LLMs. The findings are insightful and have practical implications, making the paper an enjoyable read for those interested in the behavior of LLMs.", "enjoyable_score": 3}