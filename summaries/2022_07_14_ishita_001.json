{"Published": "2022-07-14", "Title": "Language models show human-like content effects on reasoning", "Authors": "Ishita Dasgupta, Andrew K. Lampinen, Stephanie C. Y. Chan, Antonia Creswell, Dharshan Kumaran, James L. McClelland, Felix Hill", "Summary": "Abstract reasoning is a key ability for an intelligent system. Large language models achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect, and depends on our knowledge and beliefs about the content of the reasoning problem. For example, humans reason much more reliably about logical rules that are grounded in everyday situations than arbitrary rules about abstract attributes. The training experiences of language models similarly endow them with prior expectations that reflect human knowledge and beliefs. We therefore hypothesized that language models would show human-like content effects on abstract reasoning problems. We explored this hypothesis across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task (Wason, 1968). We find that state of the art large language models (with 7 or 70 billion parameters; Hoffman et al., 2022) reflect many of the same patterns observed in humans across these tasks -- like humans, models reason more effectively about believable situations than unrealistic or abstract ones. Our findings have implications for understanding both these cognitive effects, and the factors that contribute to language model performance.", "main_contribution": {"headline": "Large Language Models Exhibit Human-like Content Effects on Abstract Reasoning", "description": "This paper investigates the performance of large language models (LLMs) on abstract reasoning tasks, comparing their behavior to human reasoning. The authors hypothesize that LLMs, like humans, would show content effects on abstract reasoning problems, meaning they would reason more effectively about believable situations than unrealistic or abstract ones. This hypothesis is explored across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. The findings reveal that state-of-the-art LLMs reflect many of the same patterns observed in humans across these tasks, providing insights into both cognitive effects and factors that contribute to LLM performance."}, "takeaways": {"headline": "LLMs' Human-like Reasoning Patterns Can Inform Their Training and Application", "description": "The findings of this paper can be used to better understand and predict the behavior of LLMs in abstract reasoning tasks. By recognizing that LLMs, like humans, reason more effectively about believable situations, practitioners can tailor the training and application of these models to leverage this tendency. For instance, when training an LLM for a task involving logical reasoning, it might be beneficial to use examples grounded in everyday situations rather than abstract ones. Similarly, when using an LLM to solve a problem, it might be more effective to frame the problem in a realistic context.", "example": "For instance, if we are training an LLM to perform a task involving logical reasoning, we might choose to use examples that are grounded in everyday situations rather than abstract ones. This could potentially improve the model's performance on the task."}, "category": "BEHAVIOR", "novelty_analysis": "The paper presents a novel exploration of the behavior of large language models in abstract reasoning tasks, comparing their performance to human reasoning. The findings that LLMs exhibit human-like content effects in their reasoning is a unique contribution to the understanding of these models.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, as it involves a detailed analysis of the performance of large language models on abstract reasoning tasks. However, the concepts and methods used are well explained and should be accessible to readers with a background in AI and machine learning.", "technical_score": 2, "enjoyable_analysis": "The paper is well-written and presents an interesting comparison between human reasoning and the behavior of large language models. The findings are intriguing and have significant implications for the understanding and application of these models.", "enjoyable_score": 3}