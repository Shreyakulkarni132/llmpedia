{"Published": "2022-07-14", "Title": "Language models show human-like content effects on reasoning", "Authors": "Ishita Dasgupta, Andrew K. Lampinen, Stephanie C. Y. Chan, Antonia Creswell, Dharshan Kumaran, James L. McClelland, Felix Hill", "Summary": "Abstract reasoning is a key ability for an intelligent system. Large language models achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect, and depends on our knowledge and beliefs about the content of the reasoning problem. For example, humans reason much more reliably about logical rules that are grounded in everyday situations than arbitrary rules about abstract attributes. The training experiences of language models similarly endow them with prior expectations that reflect human knowledge and beliefs. We therefore hypothesized that language models would show human-like content effects on abstract reasoning problems. We explored this hypothesis across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task (Wason, 1968). We find that state of the art large language models (with 7 or 70 billion parameters; Hoffman et al., 2022) reflect many of the same patterns observed in humans across these tasks -- like humans, models reason more effectively about believable situations than unrealistic or abstract ones. Our findings have implications for understanding both these cognitive effects, and the factors that contribute to language model performance.", "main_contribution": "The paper investigates the performance of large language models (LLMs) on abstract reasoning tasks, comparing their behavior to human reasoning. The authors hypothesize that LLMs would show human-like content effects on abstract reasoning problems, and test this hypothesis across three logical reasoning tasks. The results show that LLMs, like humans, reason more effectively about believable situations than unrealistic or abstract ones.", "takeaways": "The findings suggest that LLMs' training experiences endow them with prior expectations that reflect human knowledge and beliefs. This understanding can help in designing better LLMs and in understanding their limitations. The study also highlights the importance of considering the content of the reasoning problem when evaluating the performance of LLMs on abstract reasoning tasks.", "novelty_analysis": "The novelty of this paper lies in its exploration of the parallels between human and LLM reasoning, particularly in the context of abstract reasoning tasks. While previous studies have examined the performance of LLMs on such tasks, this paper uniquely focuses on the influence of the content of the reasoning problem on the performance of LLMs.", "novelty_score": 2, "category": "BEHAVIOR", "technical_analysis": "The paper is somewhat technical, as it requires understanding of abstract reasoning tasks and the functioning of LLMs. However, the concepts are explained clearly and the paper is accessible to those with a background in AI and machine learning.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting comparison between human and LLM reasoning. The findings are insightful and have practical implications, making the paper enjoyable for those interested in the behavior of LLMs.", "enjoyable_score": 3}