{"Published": "2023-07-05", "Title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes", "Authors": "Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, Tomas Pfister", "Summary": "Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for training small models within a multi-task framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to few-shot prompted LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our finetuned 770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80% of available data on a benchmark, whereas standard finetuning the same T5 model struggles to match even by using 100% of the dataset. We release the code at: https://github.com/google-research/distilling-step-by-step .", "main_contribution": {"headline": "Distilling step-by-step: A new mechanism for training smaller models that outperform LLMs", "description": "The paper introduces a novel mechanism, Distilling step-by-step, which trains smaller models that outperform Large Language Models (LLMs) while using less training data. This method extracts LLM rationales as additional supervision for training small models within a multi-task framework. The authors demonstrate that this mechanism achieves better performance with fewer labeled/unlabeled training examples compared to both finetuning and distillation. Furthermore, it outperforms few-shot prompted LLMs using substantially smaller model sizes and less data."}, "takeaways": {"headline": "Distilling step-by-step offers a more efficient way to train smaller models", "description": "The Distilling step-by-step mechanism provides a more efficient way to train smaller models that outperform LLMs. It reduces both the model size and the amount of data required for training, making it a valuable tool for practitioners working with LLMs, especially in scenarios where resources are limited. The method's ability to extract LLM rationales as additional supervision for training small models within a multi-task framework also offers a new perspective on leveraging LLMs for smaller model training.", "example": "For instance, if you're training a smaller model for a specific task, you can use the Distilling step-by-step mechanism to extract rationales from an LLM. These rationales can then be used as additional supervision for training your model, reducing the amount of data required and potentially improving the model's performance."}, "category": "TRAINING", "novelty_analysis": "The Distilling step-by-step mechanism is a novel approach to training smaller models that outperform LLMs. It introduces a new way of leveraging LLMs for smaller model training, which is a significant contribution to the field.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical as it introduces a new mechanism for training smaller models and discusses its implementation in detail. However, it does not delve into complex mathematical theories or algorithms, making it accessible to readers with a background in AI and machine learning.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting and novel approach to training smaller models. The authors' clear explanation of the Distilling step-by-step mechanism and its benefits makes the paper an enjoyable read for those interested in LLMs and model training.", "enjoyable_score": 3}