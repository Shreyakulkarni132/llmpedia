{"Published": "2023-02-01", "Title": "Co-Writing with Opinionated Language Models Affects Users' Views", "Authors": "Maurice Jakesch, Advait Bhat, Daniel Buschek, Lior Zalmanson, Mor Naaman", "Summary": "If large language models like GPT-3 preferably produce a particular point of view, they may influence people's opinions on an unknown scale. This study investigates whether a language-model-powered writing assistant that generates some opinions more often than others impacts what users write - and what they think. In an online experiment, we asked participants (N=1,506) to write a post discussing whether social media is good for society. Treatment group participants used a language-model-powered writing assistant configured to argue that social media is good or bad for society. Participants then completed a social media attitude survey, and independent judges (N=500) evaluated the opinions expressed in their writing. Using the opinionated language model affected the opinions expressed in participants' writing and shifted their opinions in the subsequent attitude survey. We discuss the wider implications of our results and argue that the opinions built into AI language technologies need to be monitored and engineered more carefully.", "main_contribution": {"headline": "Opinionated Language Models Can Influence Users' Views", "description": "The paper investigates the impact of opinionated language models on users' views and writings. The authors conducted an online experiment where participants used a language-model-powered writing assistant configured to argue that social media is good or bad for society. The study found that the opinionated language model affected the opinions expressed in participants' writing and shifted their opinions in the subsequent attitude survey. This highlights the potential of large language models like GPT-3 to influence people's opinions, suggesting a need for careful monitoring and engineering of the opinions built into AI language technologies."}, "takeaways": {"headline": "Opinionated LLMs Can Shape User Opinions and Writings", "description": "The study's findings suggest that LLM practitioners should be aware of the potential for their models to influence users' opinions and writings. This is particularly relevant when developing applications like writing assistants, where the model's output directly influences the user's output. Practitioners should consider implementing mechanisms to monitor and control the opinions expressed by their models to prevent unintended persuasion. Additionally, the study's methodology could be replicated to assess the influence of other LLM applications.", "example": "For instance, if an LLM is being used to generate content for a news summarization app, the developers could conduct a similar experiment to assess whether the model's output is influencing users' views on the news topics. If the model is found to be opinionated, the developers could then implement mechanisms to neutralize the model's bias."}, "category": "BEHAVIOR", "novelty_analysis": "The paper presents a novel investigation into the influence of opinionated language models on users' views and writings. While the potential for AI to influence human behavior is not a new concept, this study is unique in its focus on large language models and their impact on users' opinions.", "novelty_score": 3, "technical_analysis": "The paper is not overly technical. It presents an online experiment and discusses its results, but does not delve into the technical details of the language models used or the statistical methods applied. The paper is accessible to readers with a basic understanding of AI and language models.", "technical_score": 1, "enjoyable_analysis": "The paper is well-written and presents an interesting investigation into the potential influence of language models on users' opinions. The findings are significant and have wide-ranging implications for the field of AI, making the paper an engaging read.", "enjoyable_score": 3}