{"Published": "2023-07-21", "Title": "Prompting Large Language Models with Speech Recognition Abilities", "Authors": "Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, Christian Fuegen, Mike Seltzer", "Summary": "Large language models have proven themselves highly flexible, able to solve a wide range of generative tasks, such as abstractive summarization and open-ended question answering. In this paper we extend the capabilities of LLMs by directly attaching a small audio encoder allowing it to perform speech recognition. By directly prepending a sequence of audial embeddings to the text token embeddings, the LLM can be converted to an automatic speech recognition (ASR) system, and be used in the exact same manner as its textual counterpart. Experiments on Multilingual LibriSpeech (MLS) show that incorporating a conformer encoder into the open sourced LLaMA-7B allows it to outperform monolingual baselines by 18% and perform multilingual speech recognition despite LLaMA being trained overwhelmingly on English text. Furthermore, we perform ablation studies to investigate whether the LLM can be completely frozen during training to maintain its original capabilities, scaling up the audio encoder, and increasing the audio encoder striding to generate fewer embeddings. The results from these studies show that multilingual ASR is possible even when the LLM is frozen or when strides of almost 1 second are used in the audio encoder opening up the possibility for LLMs to operate on long-form audio.", "main_contribution": {"headline": "Extending LLMs with Speech Recognition Abilities via Audio Encoder", "description": "The paper presents a novel approach to extend the capabilities of Large Language Models (LLMs) by attaching a small audio encoder, enabling them to perform speech recognition. This is achieved by prepending a sequence of audial embeddings to the text token embeddings, effectively converting the LLM into an automatic speech recognition (ASR) system. The authors demonstrate the effectiveness of this approach by incorporating a conformer encoder into the open-sourced LLaMA-7B, which outperforms monolingual baselines by 18% and performs multilingual speech recognition despite being trained predominantly on English text."}, "takeaways": {"headline": "LLMs can be extended to perform speech recognition, opening up new application avenues", "description": "The paper's approach of extending LLMs with speech recognition capabilities by attaching an audio encoder can be used to create new applications or improve existing ones that require speech-to-text conversion. This could be particularly useful in applications like transcription services, voice assistants, and any other system that needs to understand and process spoken language. The fact that this approach allows for multilingual speech recognition even when the LLM is frozen or when long strides are used in the audio encoder opens up the possibility for LLMs to operate on long-form audio.", "example": "For instance, an LLM with an attached audio encoder could be used to create a transcription service that can handle multiple languages and long audio files. The LLM would convert the audio into audial embeddings, which would then be converted into text token embeddings for the LLM to process and generate the transcription."}, "category": "ARCHITECTURES", "novelty_analysis": "The paper presents a novel approach to extending the capabilities of LLMs by attaching an audio encoder, enabling them to perform speech recognition. This is a significant advancement in the field, as it allows LLMs to process and understand spoken language, opening up new application possibilities.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, as it delves into the details of how the audio encoder is attached to the LLM and how it converts audio into audial embeddings. However, it does not require advanced mathematical knowledge and can be understood by someone with a background in machine learning.", "technical_score": 2, "enjoyable_analysis": "The paper is well-written and presents an interesting and novel approach to extending the capabilities of LLMs. The potential applications of this approach are exciting and the paper does a good job of explaining the technical details in an understandable way.", "enjoyable_score": 3}