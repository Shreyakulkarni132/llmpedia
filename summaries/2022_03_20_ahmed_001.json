{"Published": "2022-03-20", "Title": "How does the pre-training objective affect what large language models learn about linguistic properties?", "Authors": "Ahmed Alajrami, Nikolaos Aletras", "Summary": "Several pre-training objectives, such as masked language modeling (MLM), have been proposed to pre-train language models (e.g. BERT) with the aim of learning better language representations. However, to the best of our knowledge, no previous work so far has investigated how different pre-training objectives affect what BERT learns about linguistics properties. We hypothesize that linguistically motivated objectives such as MLM should help BERT to acquire better linguistic knowledge compared to other non-linguistically motivated objectives that are not intuitive or hard for humans to guess the association between the input and the label to be predicted. To this end, we pre-train BERT with two linguistically motivated objectives and three non-linguistically motivated ones. We then probe for linguistic characteristics encoded in the representation of the resulting models. We find strong evidence that there are only small differences in probing performance between the representations learned by the two different types of objectives. These surprising results question the dominant narrative of linguistically informed pre-training.", "main_contribution": "The paper investigates the impact of different pre-training objectives on the linguistic knowledge acquired by BERT. The authors pre-train BERT with two linguistically motivated objectives and three non-linguistically motivated ones, and then probe for linguistic characteristics encoded in the resulting models. The main finding is that there are only small differences in probing performance between the representations learned by the two different types of objectives, challenging the dominant narrative of linguistically informed pre-training.", "takeaways": "The findings suggest that non-linguistically motivated objectives can be as effective as linguistically motivated ones in pre-training BERT. This could potentially broaden the range of pre-training objectives that can be used in practice, and may also lead to a re-evaluation of the importance of linguistically informed pre-training in the development of language models.", "novelty_analysis": "The paper presents a novel investigation into the impact of different pre-training objectives on the linguistic knowledge acquired by BERT. While previous studies have explored how well and to what extent language models learn linguistic information, this is the first work to specifically examine the effect of the pre-training objective on this process.", "novelty_score": 3, "category": "TRAINING", "technical_analysis": "The paper is somewhat technical, as it requires a good understanding of language models, pre-training objectives, and probing tasks. However, the concepts are explained clearly and should be accessible to anyone with a background in computer science or AI research.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting and novel investigation into the impact of pre-training objectives on the linguistic knowledge acquired by BERT. The surprising findings challenge established beliefs in the field, making the paper a stimulating read.", "enjoyable_score": 3}