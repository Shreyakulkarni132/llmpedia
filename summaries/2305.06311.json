{"Published": "2023-05-10", "Title": "Automatic Evaluation of Attribution by Large Language Models", "Authors": "Xiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, Huan Sun", "Summary": "A recent focus of large language model (LLM) development, as exemplified by generative search engines, is to incorporate external references to generate and support their claims. However, evaluating the attribution, i.e., verifying whether the generated statement is indeed fully supported by the cited reference, remains an open problem. Although human evaluation is common practice, it is costly and time-consuming. In this paper, we investigate the automatic evaluation of attribution by LLMs. We begin by providing a definition of attribution and then explore two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed from related tasks, such as question answering, fact-checking, natural language inference, and summarization. To facilitate the evaluation, we manually curate a set of test examples covering 12 domains from a generative search engine, New Bing. Our results on the curated test set and simulated test examples from existing benchmark questions highlight both promising signals as well as remaining challenges for the automatic evaluation of attribution. We hope our testbed, modeling methodology, and insights will help lay the foundation for future studies on this important problem.", "main_contribution": {"headline": "Automatic Evaluation of Attribution by Large Language Models", "description": "The paper introduces an automatic evaluation method for attribution in Large Language Models (LLMs). Attribution refers to the verification of whether a generated statement is fully supported by the cited reference. The authors propose two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller Language Models (LMs). They also extend the definition of attribution to include two types of errors: contradictory errors and extrapolatory errors. The paper presents a metric called AttributionScore, which measures how attributable a model\u2019s generation is to its reference."}, "takeaways": {"headline": "Automatic evaluation of attribution can improve the trustworthiness of LLMs", "description": "The automatic evaluation of attribution can help in verifying the accuracy of the claims made by LLMs, thereby improving their trustworthiness. The proposed approaches, prompting LLMs and fine-tuning smaller LMs, can be used to evaluate the attribution of LLMs in a cost-effective and time-efficient manner. The AttributionScore metric can be used to measure the quality of the generated statements in terms of their attribution to the cited references.", "example": "For instance, an LLM generates a statement citing a specific reference. The AttributionScore can be used to measure how well the generated statement is supported by the cited reference. If the score is high, it means the statement is well-attributed to the reference, and vice versa."}, "category": "BEHAVIOR", "novelty_analysis": "The paper presents a novel approach to the automatic evaluation of attribution in LLMs. The introduction of the AttributionScore metric and the extension of the definition of attribution to include two types of errors are significant contributions to the field.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical as it introduces a new metric and discusses the concept of attribution in detail. However, it does not delve into complex mathematical theories or algorithms, making it accessible to readers with a basic understanding of LLMs.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting problem in the field of LLMs. The introduction of the AttributionScore metric and the discussion on the challenges of evaluating attribution make it an engaging read.", "enjoyable_score": 2}