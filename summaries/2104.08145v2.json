{"Published": "2021-09-03", "Title": "KI-BERT: Infusing Knowledge Context for Better Language and Domain Understanding", "Authors": "Keyur Faldu, Amit Sheth, Prashant Kikani, Hemang Akbari", "Summary": "Contextualized entity representations learned by state-of-the-art transformer-based language models (TLMs) like BERT, GPT, T5, etc., leverage the attention mechanism to learn the data context from training data corpus. However, these models do not use the knowledge context. Knowledge context can be understood as semantics about entities and their relationship with neighboring entities in knowledge graphs. We propose a novel and effective technique to infuse knowledge context from multiple knowledge graphs for conceptual and ambiguous entities into TLMs during fine-tuning. It projects knowledge graph embeddings in the homogeneous vector-space, introduces new token-types for entities, aligns entity position ids, and a selective attention mechanism. We take BERT as a baseline model and implement the \"Knowledge-Infused BERT\" by infusing knowledge context from ConceptNet and WordNet, which significantly outperforms BERT and other recent knowledge-aware BERT variants like ERNIE, SenseBERT, and BERT_CS over eight different subtasks of GLUE benchmark. The KI-BERT-base model even significantly outperforms BERT-large for domain-specific tasks like SciTail and academic subsets of QQP, QNLI, and MNLI.", "main_contribution": {"headline": "KI-BERT: A novel technique to infuse knowledge context into Transformer-based Language Models", "description": "The paper introduces KI-BERT, a novel technique that infuses knowledge context from multiple knowledge graphs into Transformer-based Language Models (TLMs) during fine-tuning. This is achieved by projecting knowledge graph embeddings into a homogeneous vector space, introducing new token-types for entities, aligning entity position ids, and implementing a selective attention mechanism. The authors demonstrate that KI-BERT, which uses knowledge context from ConceptNet and WordNet, significantly outperforms BERT and other recent knowledge-aware BERT variants over various subtasks of the GLUE benchmark."}, "takeaways": {"headline": "KI-BERT enhances TLMs' performance by leveraging knowledge context", "description": "KI-BERT presents a promising approach to improve the performance of TLMs by infusing knowledge context from knowledge graphs. This technique can be particularly useful when fine-tuning models on smaller, domain-specific datasets where the lack of explicit knowledge context can be a bottleneck. The significant performance improvement demonstrated by KI-BERT suggests that incorporating knowledge context can be a valuable strategy for LLM practitioners working on various NLP tasks.", "example": "For instance, when fine-tuning a TLM for a task like sentiment analysis on a domain-specific dataset, one could use KI-BERT to infuse knowledge context from relevant knowledge graphs. This could potentially enhance the model's understanding of the domain and improve its performance."}, "category": "FINE-TUNING", "novelty_analysis": "The introduction of KI-BERT represents a significant advancement in the field of TLMs. The technique of infusing knowledge context from multiple knowledge graphs into TLMs during fine-tuning is a novel approach that addresses a key limitation of existing models.", "novelty_score": 3, "technical_analysis": "The paper is quite technical, delving into the details of how KI-BERT infuses knowledge context into TLMs. It discusses the projection of knowledge graph embeddings, the introduction of new token-types, the alignment of entity position ids, and the implementation of a selective attention mechanism.", "technical_score": 3, "enjoyable_analysis": "The paper is well-structured and presents a novel and intriguing contribution to the field of TLMs. However, the technical nature of the content might make it a challenging read for those without a strong background in the field.", "enjoyable_score": 2}