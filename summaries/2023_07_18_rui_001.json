{"Published": "2023-07-18", "Title": "Secrets of RLHF in Large Language Models Part I: PPO", "Authors": "Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, Limao Xiong, Lu Chen, Zhiheng Xi, Nuo Xu, Wenbin Lai, Minghao Zhu, Cheng Chang, Zhangyue Yin, Rongxiang Weng, Wensen Cheng, Haoran Huang, Tianxiang Sun, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang", "Summary": "Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \\textbf{reward models} to measure human preferences, \\textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \\textbf{process supervision} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first report, we dissect the framework of RLHF, re-evaluate the inner workings of PPO, and explore how the parts comprising PPO algorithms impact policy agent training. We identify policy constraints being the key factor for the effective implementation of the PPO algorithm. Therefore, we explore the PPO-max, an advanced version of PPO algorithm, to efficiently improve the training stability of the policy model. Based on our main results, we perform a comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT. The absence of open-source implementations has posed significant challenges to the investigation of LLMs alignment. Therefore, we are eager to release technical reports, reward models and PPO codes, aiming to make modest contributions to the advancement of LLMs.", "main_contribution": {"headline": "PPO-max: An Advanced PPO Algorithm for Stable Training of Large Language Models", "description": "The paper presents an advanced version of the Proximal Policy Optimization (PPO) algorithm, named PPO-max, designed to improve the training stability of large language models (LLMs). The authors identify policy constraints as a key factor for the effective implementation of the PPO algorithm. They conduct an in-depth analysis of the inner workings of PPO and explore how different components of the PPO algorithm impact policy agent training. The PPO-max algorithm is shown to alleviate the instability of vanilla PPO training and enables longer training steps with a larger training corpus. The authors also release technical reports, reward models, and PPO codes to aid further research and development in the field."}, "takeaways": {"headline": "PPO-max Enhances Stability and Efficiency in LLM Training", "description": "The PPO-max algorithm offers a promising solution for improving the stability and efficiency of LLM training. By addressing the challenges of reward design, environment interaction, and agent training, PPO-max can potentially reduce the trial and error cost associated with LLMs. This could motivate further development of technical alignment and safe landing of LLMs. The authors' release of technical reports, reward models, and PPO codes also provides valuable resources for researchers and practitioners working with LLMs.", "example": "For instance, when training an LLM for a complex task like text summarization, using the PPO-max algorithm could potentially lead to more stable and efficient training. This could result in a model that produces more accurate and coherent summaries, thereby enhancing the utility of the LLM in real-world applications."}, "category": "TRAINING", "novelty_analysis": "The introduction of the PPO-max algorithm represents a significant contribution to the field of LLM training. While the PPO algorithm itself is not new, the authors' exploration of its inner workings and their development of an advanced version specifically designed for LLMs is a novel and significant advancement.", "novelty_score": 3, "technical_analysis": "The paper is highly technical, delving into the intricacies of the PPO algorithm and its application in LLM training. It requires a solid understanding of reinforcement learning, policy optimization, and large language models. The authors also present a detailed analysis of their experiments and results, further adding to the technical depth of the paper.", "technical_score": 3, "enjoyable_analysis": "For readers with a strong background in reinforcement learning and LLMs, the paper offers an engaging and insightful exploration of the PPO algorithm and its application in LLM training. The authors' clear presentation of their methodology and results, along with their thoughtful discussion of the implications, makes for an enjoyable and informative read.", "enjoyable_score": 2}