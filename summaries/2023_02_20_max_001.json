{"Published": "2023-02-20", "Title": "Adaptive Test Generation Using a Large Language Model", "Authors": "Max Sch\u00e4fer, Sarah Nadi, Aryaz Eghbali, Frank Tip", "Summary": "Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task, motivating the need for automation. This paper presents TestPilot, an adaptive test generation technique that leverages Large Language Models (LLMs). TestPilot uses Codex, an off-the-shelf LLM, to automatically generate unit tests for a given program without requiring additional training or few-shot learning on examples of existing tests. In our approach, Codex is provided with prompts that include the signature and implementation of a function under test, along with usage examples extracted from documentation. If a generated test fails, TestPilot's adaptive component attempts to generate a new test that fixes the problem by re-prompting the model with the failing test and error message. We created an implementation of TestPilot for JavaScript and evaluated it on 25 npm packages with a total of 1,684 API functions to generate tests for. Our results show that the generated tests achieve up to 93.1% statement coverage (median 68.2%). Moreover, on average, 58.5% of the generated tests contain at least one assertion that exercises functionality from the package under test. Our experiments with excluding parts of the information included in the prompts show that all components contribute towards the generation of effective test suites. Finally, we find that TestPilot does not generate memorized tests: 92.7% of our generated tests have $\\leq$ 50% similarity with existing tests (as measured by normalized edit distance), with none of them being exact copies.", "main_contribution": "The paper introduces TestPilot, an adaptive test generation technique that leverages Large Language Models (LLMs) to automatically generate unit tests for a given program. TestPilot uses Codex, an off-the-shelf LLM, and provides it with prompts that include the signature and implementation of a function under test, along with usage examples extracted from documentation. If a generated test fails, TestPilot's adaptive component attempts to generate a new test that fixes the problem by re-prompting the model with the failing test and error message.", "takeaways": "TestPilot offers a novel approach to automated test generation, leveraging the capabilities of LLMs to generate unit tests that are not only effective but also readable and understandable. The adaptive component of TestPilot, which attempts to fix failing tests by re-prompting the model, adds a layer of robustness to the test generation process. The practical implications of this work are significant, as it can potentially reduce the laborious task of manual test creation, thereby improving software development efficiency.", "novelty_analysis": "The paper presents a novel approach to automated test generation by leveraging LLMs, specifically Codex. The adaptive component of TestPilot, which attempts to fix failing tests, is a unique feature not commonly seen in traditional automated test generation techniques. This makes the findings of this paper unique and meaningful.", "novelty_score": 3, "category": "USE CASES", "technical_analysis": "The paper is somewhat technical, discussing the implementation of TestPilot and its evaluation on npm packages. It requires a basic understanding of software testing and LLMs, but the concepts are explained clearly enough for someone with a computer science background to understand.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents a novel and interesting contribution to the field of automated test generation. The clear explanation of the TestPilot technique and the comprehensive evaluation of its effectiveness make it an enjoyable read for those interested in the application of LLMs in software testing.", "enjoyable_score": 3}