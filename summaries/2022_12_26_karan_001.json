{"Published": "2022-12-26", "Title": "Large Language Models Encode Clinical Knowledge", "Authors": "Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, Philip Mansfield, Blaise Aguera y Arcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, Vivek Natarajan", "Summary": "Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but the quality bar for medical and clinical applications is high. Today, attempts to assess models' clinical knowledge typically rely on automated evaluations on limited benchmarks. There is no standard to evaluate model predictions and reasoning across a breadth of tasks. To address this, we present MultiMedQA, a benchmark combining six existing open question answering datasets spanning professional medical exams, research, and consumer queries; and HealthSearchQA, a new free-response dataset of medical questions searched online. We propose a framework for human evaluation of model answers along multiple axes including factuality, precision, possible harm, and bias. In addition, we evaluate PaLM (a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA, MedMCQA, PubMedQA, MMLU clinical topics), including 67.6% accuracy on MedQA (US Medical License Exam questions), surpassing prior state-of-the-art by over 17%. However, human evaluation reveals key gaps in Flan-PaLM responses. To resolve this we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, recall of knowledge, and medical reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal important limitations of today's models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLM models for clinical applications.", "main_contribution": {"headline": "Introduction of MultiMedQA and HealthSearchQA for evaluating LLMs in medical applications", "description": "This paper introduces two new benchmarks, MultiMedQA and HealthSearchQA, for evaluating the performance of Large Language Models (LLMs) in medical applications. MultiMedQA combines six existing open question answering datasets spanning professional medical exams, research, and consumer queries. HealthSearchQA is a new free-response dataset of medical questions searched online. The paper also presents a framework for human evaluation of model answers along multiple axes including factuality, precision, possible harm, and bias. Furthermore, the paper introduces instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars."}, "takeaways": {"headline": "LLMs show potential in medical applications, but human evaluation is crucial", "description": "The paper demonstrates that LLMs, specifically PaLM and its instruction-tuned variant Flan-PaLM, can achieve impressive results in medical applications. However, human evaluation reveals key gaps in model responses, emphasizing the importance of human oversight in deploying these models in clinical settings. The introduction of instruction prompt tuning provides a new method for aligning LLMs to specific domains, which could be useful for practitioners looking to adapt LLMs to new tasks or domains.", "example": "For instance, an LLM could be fine-tuned for a specific medical task using instruction prompt tuning. A few exemplar prompts and responses could be provided to the model, which would then adjust its parameters to better align with the desired task. However, the model's responses should be evaluated by humans along multiple axes, such as factuality and possible harm, to ensure safe and accurate use."}, "category": "FINE-TUNING", "novelty_analysis": "The paper introduces two new benchmarks for evaluating LLMs in medical applications, which is a significant contribution. The use of instruction prompt tuning to align LLMs to new domains is also a novel approach. However, the overall methodology of fine-tuning and evaluating LLMs is in line with existing practices in the field.", "novelty_score": 2, "technical_analysis": "The paper is somewhat technical, discussing the process of fine-tuning LLMs and introducing new evaluation benchmarks. However, it does not delve into complex mathematical theories or algorithms, making it accessible to readers with a basic understanding of machine learning and natural language processing.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents interesting findings about the potential and limitations of LLMs in medical applications. The introduction of new benchmarks and evaluation methods adds to the intrigue, making it an engaging read for those interested in the intersection of AI and healthcare.", "enjoyable_score": 2}