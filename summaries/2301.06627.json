{"Published": "2023-01-16", "Title": "Dissociating language and thought in large language models: a cognitive perspective", "Authors": "Kyle Mahowald, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, Evelina Fedorenko", "Summary": "Today's large language models (LLMs) routinely generate coherent, grammatical and seemingly meaningful paragraphs of text. This achievement has led to speculation that these networks are -- or will soon become -- \"thinking machines\", capable of performing tasks that require abstract knowledge and reasoning. Here, we review the capabilities of LLMs by considering their performance on two different aspects of language use: 'formal linguistic competence', which includes knowledge of rules and patterns of a given language, and 'functional linguistic competence', a host of cognitive abilities required for language understanding and use in the real world. Drawing on evidence from cognitive neuroscience, we show that formal competence in humans relies on specialized language processing mechanisms, whereas functional competence recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition. In line with this distinction, LLMs show impressive (although imperfect) performance on tasks requiring formal linguistic competence, but fail on many tests requiring functional competence. Based on this evidence, we argue that (1) contemporary LLMs should be taken seriously as models of formal linguistic skills; (2) models that master real-life language use would need to incorporate or develop not only a core language module, but also multiple non-language-specific cognitive capacities required for modeling thought. Overall, a distinction between formal and functional linguistic competence helps clarify the discourse surrounding LLMs' potential and provides a path toward building models that understand and use language in human-like ways.", "main_contribution": {"headline": "Distinguishing formal and functional linguistic competence in LLMs", "description": "The paper provides a comprehensive review of the capabilities of Large Language Models (LLMs) by distinguishing between 'formal linguistic competence' and 'functional linguistic competence'. Formal competence refers to the knowledge of rules and patterns of a language, while functional competence refers to the cognitive abilities required for language understanding and use in the real world. The authors argue that while LLMs have nearly mastered formal competence, they still struggle with functional competence tasks, which often require non-linguistic capacities."}, "takeaways": {"headline": "LLMs excel at formal linguistic tasks but struggle with functional ones", "description": "The paper's distinction between formal and functional linguistic competence provides a clear understanding of the strengths and limitations of LLMs. While LLMs can generate grammatically correct and coherent text, they struggle with tasks that require understanding and using language in real-world contexts. This insight can guide practitioners in selecting appropriate tasks for LLMs and in developing strategies to improve their functional competence, possibly through the integration of non-linguistic cognitive capacities.", "example": "For instance, an LLM might be excellent at generating a grammatically correct story (formal competence), but it might struggle to generate a story that makes sense in a real-world context (functional competence). To improve the LLM's functional competence, one might consider integrating additional modules that model world knowledge, social cognition, and other non-linguistic cognitive capacities."}, "category": "BEHAVIOR", "novelty_analysis": "The paper's main contribution is the distinction between formal and functional linguistic competence in the context of LLMs. This perspective, grounded in cognitive neuroscience, provides a fresh lens to evaluate the capabilities of LLMs. However, the concepts themselves are not new, and the paper does not introduce new techniques or algorithms.", "novelty_score": 2, "technical_analysis": "The paper is not overly technical. It discusses concepts from cognitive neuroscience and linguistics, but it does not delve into complex mathematical theories or algorithms. The paper should be accessible to readers with a basic understanding of LLMs and cognitive science.", "technical_score": 1, "enjoyable_analysis": "The paper is well-written and provides a fresh perspective on evaluating the capabilities of LLMs. It offers a clear and concise explanation of the concepts of formal and functional linguistic competence, making it an enjoyable read for those interested in the intersection of AI and cognitive science.", "enjoyable_score": 3}