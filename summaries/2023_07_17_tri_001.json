{"Published": "2023-07-17", "Title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning", "Authors": "Tri Dao", "Summary": "Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4$\\times$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40\\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2$\\times$ speedup compared to FlashAttention, reaching 50-73\\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72\\% model FLOPs utilization).", "main_contribution": {"headline": "FlashAttention-2: Enhanced GPU Efficiency for Scaling Transformers", "description": "The paper introduces FlashAttention-2, an improvement over the original FlashAttention algorithm, designed to address the challenge of scaling Transformers to longer sequence lengths. The authors identify that the inefficiency of FlashAttention is due to suboptimal work partitioning between different thread blocks and warps on the GPU. To address this, FlashAttention-2 introduces better work partitioning, reducing the number of non-matrix multiplication (non-matmul) FLOPs, parallelizing the attention computation across different thread blocks, and distributing work within each thread block to reduce communication through shared memory. These improvements lead to a 2x speedup compared to FlashAttention, reaching up to 73% of the theoretical maximum FLOPs/s on A100 GPUs."}, "takeaways": {"headline": "FlashAttention-2 Boosts Efficiency of Transformers on GPUs", "description": "FlashAttention-2 presents a significant advancement in the efficiency of Transformers, particularly in the context of GPU utilization. The improved work partitioning and parallelization techniques can be applied to enhance the performance of Transformers in various applications, including language modeling, high-resolution image understanding, and generation tasks in code, audio, and video. The paper also provides a valuable insight into the importance of optimizing non-matmul FLOPs and GPU resource utilization for achieving maximum throughput.", "example": "For instance, when training a GPT-style model using FlashAttention-2 on an A100 GPU, the training speed can reach up to 225 TFLOPs/s, which is a significant improvement over the original FlashAttention algorithm. This can lead to faster training times and more efficient use of computational resources."}, "category": "TRAINING", "novelty_analysis": "FlashAttention-2 builds upon the original FlashAttention algorithm, introducing several improvements to enhance GPU efficiency and speed up the training of Transformers. While the concept of optimizing GPU resource utilization is not entirely new, the specific techniques and their application to Transformers represent an incremental advancement in the field.", "novelty_score": 2, "technical_analysis": "The paper is highly technical, delving into the specifics of GPU memory hierarchy, thread blocks and warps, and the intricacies of the FlashAttention-2 algorithm. It requires a solid understanding of GPU architecture, Transformers, and matrix operations to fully comprehend the content.", "technical_score": 3, "enjoyable_analysis": "For readers with a strong technical background in GPU computing and Transformers, the paper offers an engaging exploration of optimizing these models for better performance. However, the high level of technical detail and the focus on specific aspects of GPU architecture may make it a challenging read for those without this background.", "enjoyable_score": 2}