{"Published": "2023-04-11", "Title": "Teaching Large Language Models to Self-Debug", "Authors": "Xinyun Chen, Maxwell Lin, Nathanael Sch\u00e4rli, Denny Zhou", "Summary": "Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any feedback on the code correctness or error messages, the model is able to identify its mistakes by explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3%, and improves the prediction accuracy on problems of the hardest label by 9%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.", "main_contribution": {"headline": "Self-Debugging: Teaching LLMs to Debug Their Own Code", "description": "The paper introduces Self-Debugging, a novel approach that enables Large Language Models (LLMs) to debug their own predicted code. This is achieved through few-shot demonstrations, without any additional model training. The model executes the code, then generates a feedback message based on the code and its execution result. This approach is different from previous works that relied on human feedback for code repair. The Self-Debugging method has shown significant improvements in code generation benchmarks, demonstrating its effectiveness."}, "takeaways": {"headline": "Self-Debugging Enhances LLMs' Code Generation Capabilities", "description": "The Self-Debugging approach can be a game-changer for LLM practitioners working on code generation tasks. It allows the model to identify and correct its own mistakes, thereby improving the quality of the generated code. This method can be particularly useful in scenarios where there are no unit tests to verify the correctness of predictions. Furthermore, it improves sample efficiency by leveraging feedback messages and reusing failed predictions.", "example": "For instance, an LLM using Self-Debugging, when tasked with generating a Python script, would first generate the code, execute it, and then generate a feedback message based on the code and its execution result. If the code contains errors, the model would identify these mistakes and correct them, thereby improving the quality of the generated code."}, "category": "FINE-TUNING", "novelty_analysis": "The concept of teaching LLMs to debug their own code is a significant advancement in the field. While previous works have explored program repair approaches, the Self-Debugging method stands out for its ability to improve code generation performance without relying on external feedback or additional model training.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical as it delves into the details of the Self-Debugging approach and its implementation. However, it does not involve complex mathematical theories or algorithms, making it accessible to readers with a background in machine learning and programming.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting and novel approach to improving LLMs' code generation capabilities. The practical implications of the Self-Debugging method make the paper an engaging read for practitioners in the field.", "enjoyable_score": 3}