{"Published": "2023-05-17", "Title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "Authors": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, Karthik Narasimhan", "Summary": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/ysymyth/tree-of-thought-llm.", "main_contribution": {"headline": "Tree of Thoughts (ToT) framework enhances LLMs' problem-solving abilities", "description": "The paper introduces a novel framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models. ToT enables exploration over coherent units of text, referred to as 'thoughts', that serve as intermediate steps toward problem solving. It allows language models to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action. It also enables the models to look ahead or backtrack when necessary to make global choices. The authors demonstrate that ToT significantly enhances language models' problem-solving abilities on tasks requiring non-trivial planning or search."}, "takeaways": {"headline": "ToT framework offers a new approach to problem-solving with LLMs", "description": "The Tree of Thoughts (ToT) framework presents a new way to enhance the problem-solving abilities of Large Language Models (LLMs). By allowing LLMs to consider multiple reasoning paths and self-evaluate choices, ToT can be used to improve the performance of LLMs on tasks that require strategic planning or search. This approach could be particularly useful in applications that require complex decision-making or problem-solving abilities, such as game playing, creative writing, or solving crosswords.", "example": "For instance, in a game-playing application, an LLM using the ToT framework could consider multiple possible moves (thoughts), evaluate the potential outcomes of each move, and then choose the move that is most likely to lead to a win. This process could be repeated at each step of the game, allowing the LLM to make strategic decisions and potentially improve its performance."}, "category": "PROMPTING", "novelty_analysis": "The Tree of Thoughts (ToT) framework represents a significant advancement in the field of language model inference. While it builds on the existing Chain of Thought approach, it introduces the novel concept of exploring multiple reasoning paths and self-evaluating choices, which is a significant departure from the traditional left-to-right decision-making process used by most language models.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, as it introduces a new framework for language model inference and discusses its implementation in detail. However, the concepts are explained clearly and should be accessible to readers with a basic understanding of language models and machine learning.", "technical_score": 2, "enjoyable_analysis": "The paper is well-written and presents an interesting and novel approach to enhancing the problem-solving abilities of language models. The use of clear examples and diagrams helps to make the concepts accessible and engaging for the reader.", "enjoyable_score": 3}