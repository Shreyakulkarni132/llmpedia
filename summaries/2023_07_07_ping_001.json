{"Published": "2023-07-07", "Title": "ALERT: Adapting Language Models to Reasoning Tasks", "Authors": "Ping Yu, Tianlu Wang, Olga Golovneva, Badr Alkhamissy, Gargi Ghosh, Mona Diab, Asli Celikyilmaz", "Summary": "Current large language models can perform reasonably well on complex tasks that require step-by-step reasoning with few-shot learning. Are these models applying reasoning skills they have learnt during pre-training and reason outside of their training context, or are they simply memorizing their training corpus at finer granularity and have learnt to better understand their context? To tease apart these possibilities, we introduce ALERT, a benchmark and suite of analyses for assessing language models' reasoning ability comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. ALERT provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. We leverage ALERT to further investigate the role of finetuning. With extensive empirical analysis we find that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during finetuning stage compared to pretraining state. We also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.", "main_contribution": {"headline": "ALERT: A Benchmark for Evaluating Reasoning Skills of Language Models", "description": "The paper introduces ALERT, a benchmark and suite of analyses designed to evaluate the reasoning skills of language models. ALERT provides a platform to compare pre-trained and fine-tuned models on complex tasks that require reasoning skills. It spans over 20 datasets and covers 10 different reasoning skills. The authors use ALERT to investigate the role of fine-tuning in language models, revealing that models acquire more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during the fine-tuning stage compared to the pre-training stage."}, "takeaways": {"headline": "ALERT Provides Insights into Reasoning Skills of Language Models", "description": "ALERT provides a valuable tool for AI researchers and practitioners working with language models. It allows for a comprehensive evaluation of a model's reasoning skills, offering insights into how these skills are developed during the pre-training and fine-tuning stages. This can guide the development and fine-tuning of language models, helping to improve their performance on complex tasks. However, the study also reveals a potential pitfall: fine-tuned models tend to overfit to the prompt template, which can hurt their robustness and lead to generalization problems.", "example": "For instance, using ALERT, a practitioner can evaluate a language model's ability to perform tasks requiring textual entailment, abductive reasoning, or analogical reasoning. If the model performs poorly, the practitioner might decide to fine-tune the model to improve these skills. However, they should be careful not to overfit the model to the prompt template, as this could hurt the model's robustness."}, "category": "BEHAVIOR", "novelty_analysis": "The introduction of ALERT represents a significant contribution to the field of language model research. While previous studies have examined the performance of language models on complex tasks, this is the first to provide a comprehensive benchmark for evaluating a model's reasoning skills and investigating the role of fine-tuning in the development of these skills.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, as it delves into the details of how ALERT was developed and used to evaluate the reasoning skills of language models. However, it does not require advanced mathematical knowledge and should be accessible to anyone with a background in AI and machine learning.", "technical_score": 2, "enjoyable_analysis": "The paper is well-written and organized, presenting a novel and intriguing contribution to the field of language model research. It provides practical insights that will be of interest to AI researchers and practitioners, making it an enjoyable read.", "enjoyable_score": 3}