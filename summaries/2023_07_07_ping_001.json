{"Published": "2023-07-07", "Title": "ALERT: Adapting Language Models to Reasoning Tasks", "Authors": "Ping Yu, Tianlu Wang, Olga Golovneva, Badr Alkhamissy, Gargi Ghosh, Mona Diab, Asli Celikyilmaz", "Summary": "Current large language models can perform reasonably well on complex tasks that require step-by-step reasoning with few-shot learning. Are these models applying reasoning skills they have learnt during pre-training and reason outside of their training context, or are they simply memorizing their training corpus at finer granularity and have learnt to better understand their context? To tease apart these possibilities, we introduce ALERT, a benchmark and suite of analyses for assessing language models' reasoning ability comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. ALERT provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. We leverage ALERT to further investigate the role of finetuning. With extensive empirical analysis we find that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during finetuning stage compared to pretraining state. We also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.", "main_contribution": {"headline": "ALERT: A Benchmark for Evaluating Reasoning Skills of Language Models", "description": "The paper introduces ALERT, a benchmark and suite of analyses designed to evaluate the reasoning skills of large language models (LLMs). ALERT provides a platform to compare pre-trained and fine-tuned models on complex tasks that require reasoning skills. It spans over 20 datasets and covers 10 different reasoning skills. The authors use ALERT to investigate the role of fine-tuning in LLMs, revealing that models acquire more reasoning skills during the fine-tuning stage compared to the pre-training stage. However, they also find that fine-tuned models tend to overfit to the prompt template, which can lead to generalization problems."}, "takeaways": {"headline": "ALERT Provides Insights into Reasoning Skills Acquisition and Overfitting in LLMs", "description": "ALERT offers a comprehensive benchmark for evaluating and comparing the reasoning skills of LLMs. It can be used to assess how well models perform on complex tasks that require reasoning, providing valuable insights for model development and fine-tuning. The findings of this study highlight the importance of the fine-tuning stage in acquiring reasoning skills and the potential issue of overfitting to the prompt template. These insights can guide practitioners in improving the robustness and generalization of their models.", "example": "For instance, using ALERT, a practitioner can evaluate an LLM's performance on tasks requiring textual entailment, abductive reasoning, and analogical reasoning. If the model performs poorly, the practitioner might consider fine-tuning the model with a diverse set of prompts to improve its reasoning skills while avoiding overfitting to a specific prompt template."}, "category": "BEHAVIOR", "novelty_analysis": "The introduction of ALERT as a benchmark for evaluating the reasoning skills of LLMs is a novel contribution. It provides a structured way to compare the performance of pre-trained and fine-tuned models on complex reasoning tasks, which was not previously available.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical as it introduces a new benchmark and discusses the role of fine-tuning in LLMs. However, it does not delve into complex mathematical theories or algorithms, making it accessible to readers with a basic understanding of LLMs and fine-tuning.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting investigation into the reasoning skills of LLMs. The introduction of ALERT and the insights gained from its application make the paper an engaging read for those interested in the development and evaluation of LLMs.", "enjoyable_score": 2}