{"Published": "2022-12-15", "Title": "Constitutional AI: Harmlessness from AI Feedback", "Authors": "Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, Jared Kaplan", "Summary": "As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.", "main_contribution": {"headline": "Introduction of Constitutional AI for training harmless AI assistants", "description": "The paper introduces a novel method called 'Constitutional AI' for training AI assistants that are harmless and non-evasive. The method involves a supervised learning phase and a reinforcement learning phase. In the supervised phase, the initial model is sampled, self-critiques and revisions are generated, and the original model is finetuned on revised responses. In the reinforcement learning phase, the finetuned model is sampled, a model is used to evaluate which of the two samples is better, and a preference model is trained from this dataset of AI preferences. The preference model is then used as the reward signal for reinforcement learning. This method allows for more precise control of AI behavior with fewer human labels."}, "takeaways": {"headline": "Constitutional AI provides a method for training harmless AI assistants", "description": "The Constitutional AI method provides a way for LLM practitioners to train AI assistants that are harmless and non-evasive. The method involves both supervised learning and reinforcement learning phases, allowing for more precise control of AI behavior with fewer human labels. This could be particularly useful in applications where it is important to ensure that the AI does not produce harmful outputs, such as in customer service or healthcare applications.", "example": "For instance, in a customer service application, an AI assistant trained using Constitutional AI could be used to handle customer queries. The assistant would be able to engage with potentially harmful queries by explaining its objections to them, rather than simply refusing to respond or providing a harmful response."}, "category": "TRAINING", "novelty_analysis": "The paper introduces a novel method for training AI assistants that are harmless and non-evasive. This method, called Constitutional AI, involves both supervised learning and reinforcement learning phases, and allows for more precise control of AI behavior with fewer human labels.", "novelty_score": 3, "technical_analysis": "The paper is highly technical, detailing the process of training an AI assistant using the Constitutional AI method. It involves complex concepts from both supervised learning and reinforcement learning, and requires a deep understanding of these areas to fully comprehend.", "technical_score": 3, "enjoyable_analysis": "The paper is well-written and presents a novel and intriguing contribution to the field of AI training. However, the high level of technical detail may make it a challenging read for those without a strong background in machine learning.", "enjoyable_score": 2}