{"Published": "2023-04-01", "Title": "Evaluating Large Language Models on a Highly-specialized Topic, Radiation Oncology Physics", "Authors": "Jason Holmes, Zhengliang Liu, Lian Zhang, Yuzhen Ding, Terence T. Sio, Lisa A. McGee, Jonathan B. Ashman, Xiang Li, Tianming Liu, Jiajian Shen, Wei Liu", "Summary": "We present the first study to investigate Large Language Models (LLMs) in answering radiation oncology physics questions. Because popular exams like AP Physics, LSAT, and GRE have large test-taker populations and ample test preparation resources in circulation, they may not allow for accurately assessing the true potential of LLMs. This paper proposes evaluating LLMs on a highly-specialized topic, radiation oncology physics, which may be more pertinent to scientific and medical communities in addition to being a valuable benchmark of LLMs. We developed an exam consisting of 100 radiation oncology physics questions based on our expertise at Mayo Clinic. Four LLMs, ChatGPT (GPT-3.5), ChatGPT (GPT-4), Bard (LaMDA), and BLOOMZ, were evaluated against medical physicists and non-experts. ChatGPT (GPT-4) outperformed all other LLMs as well as medical physicists, on average. The performance of ChatGPT (GPT-4) was further improved when prompted to explain first, then answer. ChatGPT (GPT-3.5 and GPT-4) showed a high level of consistency in its answer choices across a number of trials, whether correct or incorrect, a characteristic that was not observed in the human test groups. In evaluating ChatGPTs (GPT-4) deductive reasoning ability using a novel approach (substituting the correct answer with \"None of the above choices is the correct answer.\"), ChatGPT (GPT-4) demonstrated surprising accuracy, suggesting the potential presence of an emergent ability. Finally, although ChatGPT (GPT-4) performed well overall, its intrinsic properties did not allow for further improvement when scoring based on a majority vote across trials. In contrast, a team of medical physicists were able to greatly outperform ChatGPT (GPT-4) using a majority vote. This study suggests a great potential for LLMs to work alongside radiation oncology experts as highly knowledgeable assistants.", "main_contribution": {"headline": "Evaluating LLMs on highly-specialized topics: A case study in radiation oncology physics", "description": "This paper presents the first study to evaluate Large Language Models (LLMs) on a highly-specialized topic, radiation oncology physics. The authors developed an exam of 100 questions in this field and evaluated four LLMs, including ChatGPT (GPT-3.5 and GPT-4), Bard (LaMDA), and BLOOMZ. The study found that ChatGPT (GPT-4) outperformed all other LLMs and even medical physicists on average. The paper also introduces a novel approach to evaluate the deductive reasoning ability of LLMs, which showed promising results."}, "takeaways": {"headline": "LLMs show potential in specialized fields and can be improved with specific prompting strategies", "description": "The study demonstrates that LLMs, particularly ChatGPT (GPT-4), can perform well in highly specialized fields, suggesting potential applications in scientific and medical communities. The performance of LLMs can be further improved by prompting them to explain first, then answer. However, the study also found that LLMs' intrinsic properties do not allow for further improvement when scoring based on a majority vote across trials, unlike human teams.", "example": "For instance, in a medical consultation application, an LLM could be prompted to first explain a complex medical concept before providing a diagnosis or treatment recommendation. This could improve the accuracy of the LLM's responses and also help the user understand the reasoning behind the LLM's recommendations."}, "category": "BEHAVIOR", "novelty_analysis": "The paper presents a novel approach to evaluating LLMs by focusing on a highly-specialized topic, radiation oncology physics. This approach provides a new benchmark for LLMs and reveals their potential in scientific and medical fields. The introduction of a novel method to evaluate LLMs' deductive reasoning ability also adds to the novelty of the paper.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical as it involves a specialized field, radiation oncology physics. However, it does not delve into the technical details of the LLMs or the algorithms used, focusing instead on the evaluation and application of the models.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting and novel approach to evaluating LLMs. The findings are surprising and insightful, making the paper an enjoyable read for those interested in the application of LLMs in specialized fields.", "enjoyable_score": 3}