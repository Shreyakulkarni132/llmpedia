{"Published": "2023-07-05", "Title": "Building Cooperative Embodied Agents Modularly with Large Language Models", "Authors": "Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B. Tenenbaum, Tianmin Shu, Chuang Gan", "Summary": "Large Language Models (LLMs) have demonstrated impressive planning abilities in single-agent embodied tasks across various domains. However, their capacity for planning and communication in multi-agent cooperation remains unclear, even though these are crucial skills for intelligent embodied agents. In this paper, we present a novel framework that utilizes LLMs for multi-agent cooperation and tests it in various embodied environments. Our framework enables embodied agents to plan, communicate, and cooperate with other embodied agents or humans to accomplish long-horizon tasks efficiently. We demonstrate that recent LLMs, such as GPT-4, can surpass strong planning-based methods and exhibit emergent effective communication using our framework without requiring fine-tuning or few-shot prompting. We also discover that LLM-based agents that communicate in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for embodied AI and lays the foundation for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.", "main_contribution": {"headline": "A novel framework for multi-agent cooperation using Large Language Models", "description": "The paper introduces a new framework that leverages Large Language Models (LLMs) for multi-agent cooperation in embodied environments. The framework enables embodied agents to plan, communicate, and cooperate with other agents or humans to accomplish long-horizon tasks efficiently. It consists of five modules, each addressing a critical aspect of successful multi-agent cooperation, including a belief module, a communication module, and a reasoning module. The authors demonstrate that LLMs, such as GPT-4, can surpass strong planning-based methods and exhibit emergent effective communication using this framework without requiring fine-tuning or few-shot prompting."}, "takeaways": {"headline": "LLMs can be effectively used for multi-agent cooperation in embodied environments", "description": "The proposed framework opens up new possibilities for using LLMs in multi-agent cooperation scenarios. It shows that LLMs can be used to plan and communicate with other agents to cooperatively solve complex tasks without any fine-tuning or few-shot prompting. This can be particularly useful in scenarios where multiple agents, including humans, need to work together to accomplish a common goal. The framework's modular design also allows for easy customization and adaptation to different environments and tasks.", "example": "For instance, in a warehouse management scenario, multiple robotic agents powered by LLMs can cooperate to efficiently manage inventory. They can use the belief module to understand the current state of the warehouse, the communication module to share information with each other, and the reasoning module to make decisions about what actions to take."}, "category": "USE CASES", "novelty_analysis": "The paper presents a novel approach to using LLMs for multi-agent cooperation in embodied environments. While LLMs have been used for single-agent tasks before, their use in multi-agent cooperation is a new and significant contribution to the field.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical as it introduces a new framework and discusses its components in detail. However, it does not delve into complex mathematical theories or algorithms, making it accessible to readers with a basic understanding of LLMs and multi-agent systems.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting and novel use of LLMs. The use of a real-world example to illustrate the application of the proposed framework makes it engaging and easy to understand.", "enjoyable_score": 3}