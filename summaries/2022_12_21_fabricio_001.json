{"Published": "2022-12-21", "Title": "Crowd Score: A Method for the Evaluation of Jokes using Large Language Model AI Voters as Judges", "Authors": "Fabricio Goes, Zisen Zhou, Piotr Sawicki, Marek Grzes, Daniel G. Brown", "Summary": "This paper presents the Crowd Score, a novel method to assess the funniness of jokes using large language models (LLMs) as AI judges. Our method relies on inducing different personalities into the LLM and aggregating the votes of the AI judges into a single score to rate jokes. We validate the votes using an auditing technique that checks if the explanation for a particular vote is reasonable using the LLM. We tested our methodology on 52 jokes in a crowd of four AI voters with different humour types: affiliative, self-enhancing, aggressive and self-defeating. Our results show that few-shot prompting leads to better results than zero-shot for the voting question. Personality induction showed that aggressive and self-defeating voters are significantly more inclined to find more jokes funny of a set of aggressive/self-defeating jokes than the affiliative and self-enhancing voters. The Crowd Score follows the same trend as human judges by assigning higher scores to jokes that are also considered funnier by human judges. We believe that our methodology could be applied to other creative domains such as story, poetry, slogans, etc. It could both help the adoption of a flexible and accurate standard approach to compare different work in the CC community under a common metric and by minimizing human participation in assessing creative artefacts, it could accelerate the prototyping of creative artefacts and reduce the cost of hiring human participants to rate creative artefacts.", "main_contribution": "The paper introduces the Crowd Score, a novel method for evaluating the funniness of jokes using large language models (LLMs) as AI judges. The method involves inducing different personalities into the LLM and aggregating the votes of the AI judges into a single score. The authors validate the votes using an auditing technique that checks if the explanation for a particular vote is reasonable using the LLM. The methodology was tested on 52 jokes with four AI voters, each having a different humor type.", "takeaways": "The Crowd Score method offers a new way to evaluate creative artifacts, in this case, jokes, using AI. This could potentially reduce the need for human participation in assessing creative artifacts, thereby accelerating the prototyping of creative artifacts and reducing the cost of hiring human participants. The method also showed that personality induction in AI voters can influence their humor preferences, which could have implications for other areas of AI research.", "novelty_analysis": "The Crowd Score method is a novel approach to evaluating jokes using AI. While there has been previous work on using AI for humor detection, the use of AI judges with induced personalities to rate jokes and the use of an auditing technique to validate the votes are unique contributions of this paper.", "novelty_score": 3, "category": "USE CASES", "technical_analysis": "The paper presents a new methodology and discusses its implementation and testing, but does not delve deeply into the technical aspects of the LLMs used or the process of inducing personalities into the AI judges. Therefore, it is somewhat technical but should be reasonably understandable for someone with a computer science background.", "technical_score": 2, "enjoyable_analysis": "The paper presents an interesting and novel approach to evaluating jokes using AI, which makes it enjoyable to read. The results and implications of the study are also intriguing, adding to the enjoyment of the paper.", "enjoyable_score": 3}