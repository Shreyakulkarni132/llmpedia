{"Published": "2023-05-29", "Title": "Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model", "Authors": "Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, Soujanya Poria", "Summary": "The immense scale of the recent large language models (LLM) allows many interesting properties, such as, instruction- and chain-of-thought-based fine-tuning, that has significantly improved zero- and few-shot performance in many natural language processing (NLP) tasks. Inspired by such successes, we adopt such an instruction-tuned LLM Flan-T5 as the text encoder for text-to-audio (TTA) generation -- a task where the goal is to generate an audio from its textual description. The prior works on TTA either pre-trained a joint text-audio encoder or used a non-instruction-tuned model, such as, T5. Consequently, our latent diffusion model (LDM)-based approach TANGO outperforms the state-of-the-art AudioLDM on most metrics and stays comparable on the rest on AudioCaps test set, despite training the LDM on a 63 times smaller dataset and keeping the text encoder frozen. This improvement might also be attributed to the adoption of audio pressure level-based sound mixing for training set augmentation, whereas the prior methods take a random mix.", "main_contribution": {"headline": "Instruction-tuned LLM and Latent Diffusion Model for Text-to-Audio Generation", "description": "The paper presents a novel approach to text-to-audio (TTA) generation using an instruction-tuned Large Language Model (LLM) and a Latent Diffusion Model (LDM). The authors use an instruction-tuned LLM, FLAN-T5, as the text encoder, which improves text understanding and overall audio generation without any fine-tuning. The LDM-based approach, named TANGO, outperforms the state-of-the-art AudioLDM on most metrics, despite training on a significantly smaller dataset. The authors also introduce a pressure level-based mixing method for training set augmentation, which is a departure from the random mix used in prior methods."}, "takeaways": {"headline": "Instruction-tuned LLMs and LDMs can significantly improve text-to-audio generation", "description": "The paper demonstrates that instruction-tuned LLMs, combined with LDMs, can significantly improve the performance of text-to-audio generation tasks. This approach can be particularly useful in media production, where creators are often looking for novel sounds that fit their creations. The pressure level-based mixing method for training set augmentation can also be a useful technique for practitioners working with audio data.", "example": "For instance, an LLM practitioner could use the FLAN-T5 model to encode the textual description of the desired audio, and then use an LDM to generate the audio prior. The generated audio prior could then be decoded by a pre-trained VAE, followed by a vocoder, to produce the final audio output."}, "category": "USE CASES", "novelty_analysis": "The paper introduces a novel approach to text-to-audio generation using an instruction-tuned LLM and an LDM. The use of a pressure level-based mixing method for training set augmentation is also a unique contribution. However, the underlying techniques and models used are not new, and the paper builds upon existing work in the field.", "novelty_score": 2, "technical_analysis": "The paper is somewhat technical, as it involves the use of advanced models like LLMs and LDMs, and requires a good understanding of natural language processing and audio processing techniques. However, the concepts are explained clearly, and the paper should be accessible to readers with a background in these areas.", "technical_score": 2, "enjoyable_analysis": "The paper is well-written and presents an interesting application of LLMs in the field of audio generation. The results are impressive, and the paper provides a clear roadmap for other researchers and practitioners interested in this area.", "enjoyable_score": 3}