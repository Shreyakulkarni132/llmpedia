{"Published": "2023-06-01", "Title": "Teaching Small Language Models to Reason", "Authors": "Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, Aliaksei Severyn", "Summary": "Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with a size of over 100 billion parameters. In this paper, we explore the transfer of such reasoning capabilities to models with less than 100 billion parameters via knowledge distillation. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets. For example, the accuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned on PaLM-540B generated chains of thought.", "main_contribution": {"headline": "Transfer of reasoning capabilities from large to small language models via knowledge distillation", "description": "The paper explores the possibility of transferring the reasoning capabilities of large language models (LLMs) to smaller language models (SLMs) through knowledge distillation. The authors propose a method where a smaller 'student' model is finetuned on the 'chain of thought' outputs generated by a larger 'teacher' model. This approach is shown to improve the task performance of the smaller models across arithmetic, commonsense, and symbolic reasoning datasets. The authors also recommend generating the chain of thought from an LLM and providing the solution to the task in the few-shot prompt."}, "takeaways": {"headline": "Knowledge distillation can enhance the reasoning capabilities of smaller language models", "description": "The paper's findings suggest that the reasoning capabilities of LLMs can be transferred to SLMs through knowledge distillation, thereby improving the performance of SLMs on various tasks. This approach could be particularly useful for applications where computational resources are limited, as it allows for the use of smaller, more efficient models without sacrificing performance. The method involves finetuning the smaller model on the chain of thought outputs generated by the larger model, and providing the solution to the task in the few-shot prompt.", "example": "For instance, if you have a smaller model and want to improve its performance on a reasoning task, you could finetune it on the chain of thought outputs generated by a larger model that performs well on the task. This could involve feeding the smaller model a series of intermediate steps leading to the solution of the task, which are generated by the larger model."}, "category": "FINE-TUNING", "novelty_analysis": "The paper presents a novel approach to improving the performance of smaller language models by transferring the reasoning capabilities of larger models through knowledge distillation. While the concept of knowledge distillation is not new, its application in this context is unique and represents a significant contribution to the field.", "novelty_score": 2, "technical_analysis": "The paper is somewhat technical, as it delves into the specifics of the proposed method and presents detailed experimental results. However, the concepts are explained clearly and should be accessible to readers with a basic understanding of language models and knowledge distillation.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting approach to improving the performance of smaller language models. The clear explanation of the method and the detailed presentation of the experimental results make it an engaging read for those interested in the field.", "enjoyable_score": 2}