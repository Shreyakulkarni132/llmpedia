{"Published": "2023-06-06", "Title": "Large Language Models Can Be Easily Distracted by Irrelevant Context", "Authors": "Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Sch\u00e4rli, Denny Zhou", "Summary": "Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.", "main_contribution": {"headline": "Investigation of Large Language Models' Distractibility and Mitigation Strategies", "description": "This paper investigates the distractibility of large language models (LLMs), specifically how their problem-solving accuracy can be influenced by irrelevant context. The authors introduce a new benchmark, Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. The study reveals that the performance of LLMs significantly decreases when irrelevant information is included in the input. However, the authors propose several mitigation strategies, such as decoding with self-consistency and adding an instruction to the prompt that tells the LLM to ignore the irrelevant information."}, "takeaways": {"headline": "Awareness of LLMs' Distractibility and Strategies for Improvement", "description": "LLM practitioners should be aware of the distractibility of these models when irrelevant context is included in the input. This can significantly impact the accuracy of problem-solving tasks. However, the paper provides practical strategies to mitigate this issue, such as decoding with self-consistency and adding an instruction to the prompt that tells the LLM to ignore the irrelevant information. These strategies can be incorporated into the design and implementation of LLMs to improve their performance in real-world applications where irrelevant context is often present.", "example": "For instance, when using an LLM for a task like question answering, you could add an instruction to the prompt like 'Ignore any irrelevant information and answer the following question: ...'. This could help the model to focus on the relevant information and improve its accuracy."}, "category": "BEHAVIOR", "novelty_analysis": "The paper presents a novel investigation into the distractibility of LLMs, a topic that has not been extensively studied before. The introduction of the GSM-IC benchmark and the proposed mitigation strategies represent significant contributions to the field.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical as it delves into the workings of LLMs and the impact of irrelevant context on their performance. However, it does not involve complex mathematical theories or algorithms, making it accessible to those with a basic understanding of LLMs.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting investigation into a less-studied aspect of LLMs. The practical implications of the findings and the proposed mitigation strategies make it an engaging read for those interested in the behavior of LLMs.", "enjoyable_score": 2}