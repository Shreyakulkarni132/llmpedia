{"Published": "2023-08-10", "Title": "Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment", "Authors": "Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, Hang Li", "Summary": "Ensuring alignment, which refers to making models behave in accordance with human intentions [1,2], has become a critical task before deploying large language models (LLMs) in real-world applications. For instance, OpenAI devoted six months to iteratively aligning GPT-4 before its release [3]. However, a major challenge faced by practitioners is the lack of clear guidance on evaluating whether LLM outputs align with social norms, values, and regulations. This obstacle hinders systematic iteration and deployment of LLMs. To address this issue, this paper presents a comprehensive survey of key dimensions that are crucial to consider when assessing LLM trustworthiness. The survey covers seven major categories of LLM trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. Each major category is further divided into several sub-categories, resulting in a total of 29 sub-categories. Additionally, a subset of 8 sub-categories is selected for further investigation, where corresponding measurement studies are designed and conducted on several widely-used LLMs. The measurement results indicate that, in general, more aligned models tend to perform better in terms of overall trustworthiness. However, the effectiveness of alignment varies across the different trustworthiness categories considered. This highlights the importance of conducting more fine-grained analyses, testing, and making continuous improvements on LLM alignment. By shedding light on these key dimensions of LLM trustworthiness, this paper aims to provide valuable insights and guidance to practitioners in the field. Understanding and addressing these concerns will be crucial in achieving reliable and ethically sound deployment of LLMs in various applications.", "main_contribution": {"headline": "Comprehensive Survey and Guideline for Evaluating Trustworthiness of LLMs", "description": "The paper presents a comprehensive survey and guideline for evaluating the trustworthiness of Large Language Models (LLMs). It identifies seven major categories of LLM trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. Each category is further divided into several sub-categories, resulting in a total of 29 sub-categories. The paper also conducts measurement studies on a subset of 8 sub-categories across several widely-used LLMs. The results indicate that more aligned models tend to perform better in terms of overall trustworthiness, but the effectiveness of alignment varies across different trustworthiness categories."}, "takeaways": {"headline": "Guidelines for Evaluating LLM Trustworthiness Can Improve Model Deployment", "description": "The guidelines provided in this paper can be used to evaluate and improve the trustworthiness of LLMs before deployment. By considering the seven major categories and 29 sub-categories, practitioners can conduct more fine-grained analyses and make continuous improvements on LLM alignment. For example, to improve the safety of an LLM, one could use the sub-categories provided (violence, unlawful conduct, harms to minor, etc.) as a checklist for potential issues to address during model training and fine-tuning.", "example": "For instance, if an LLM is being developed for a customer service chatbot, the 'adherence to social norms' category could be particularly important. The sub-categories under this could be used to evaluate whether the model's responses are polite, respectful, and appropriate for the context."}, "category": "BEHAVIOR", "novelty_analysis": "The paper provides a comprehensive and structured approach to evaluating the trustworthiness of LLMs, which is a significant contribution to the field. However, the concept of evaluating model trustworthiness is not new, and the paper builds upon existing ideas and methodologies.", "novelty_score": 2, "technical_analysis": "The paper is somewhat technical as it delves into the specifics of evaluating LLM trustworthiness across multiple categories and sub-categories. However, it does not introduce new algorithms or mathematical theories, making it accessible to readers with a basic understanding of LLMs.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and provides valuable insights into the important aspect of LLM trustworthiness. It could be a useful read for practitioners in the field, but the focus on evaluation metrics and guidelines might not appeal to all readers.", "enjoyable_score": 2}