{"Published": "2023-07-25", "Title": "Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction", "Authors": "Sungmin Kang, Juyeon Yoon, Shin Yoo", "Summary": "Many automated test generation techniques have been developed to aid developers with writing tests. To facilitate full automation, most existing techniques aim to either increase coverage, or generate exploratory inputs. However, existing test generation techniques largely fall short of achieving more semantic objectives, such as generating tests to reproduce a given bug report. Reproducing bugs is nonetheless important, as our empirical study shows that the number of tests added in open source repositories due to issues was about 28% of the corresponding project test suite size. Meanwhile, due to the difficulties of transforming the expected program semantics in bug reports into test oracles, existing failure reproduction techniques tend to deal exclusively with program crashes, a small subset of all bug reports. To automate test generation from general bug reports, we propose LIBRO, a framework that uses Large Language Models (LLMs), which have been shown to be capable of performing code-related tasks. Since LLMs themselves cannot execute the target buggy code, we focus on post-processing steps that help us discern when LLMs are effective, and rank the produced tests according to their validity. Our evaluation of LIBRO shows that, on the widely studied Defects4J benchmark, LIBRO can generate failure reproducing test cases for 33% of all studied cases (251 out of 750), while suggesting a bug reproducing test in first place for 149 bugs. To mitigate data contamination, we also evaluate LIBRO against 31 bug reports submitted after the collection of the LLM training data terminated: LIBRO produces bug reproducing tests for 32% of the studied bug reports. Overall, our results show LIBRO has the potential to significantly enhance developer efficiency by automatically generating tests from bug reports.", "main_contribution": {"headline": "LIBRO: A Framework for Automating Test Generation from Bug Reports Using LLMs", "description": "The paper introduces LIBRO, a novel framework that leverages Large Language Models (LLMs) to automate the generation of tests from general bug reports. This is a significant departure from existing techniques that focus on increasing coverage or generating exploratory inputs. LIBRO uses LLMs to perform code-related tasks, and then applies post-processing steps to discern the effectiveness of the LLMs and rank the produced tests according to their validity. The framework was evaluated using the Defects4J benchmark and showed promising results, generating failure reproducing test cases for 33% of all studied cases."}, "takeaways": {"headline": "LIBRO Framework Enhances Developer Efficiency by Automating Test Generation", "description": "The LIBRO framework presents a new approach to automating test generation from bug reports, which can significantly enhance developer efficiency. By using LLMs to perform code-related tasks and applying post-processing steps to rank the produced tests, LIBRO can generate failure reproducing test cases for a significant portion of studied cases. This approach could be integrated into existing software development workflows to automate the tedious and time-consuming task of writing tests, freeing up developers to focus on other aspects of the development process.", "example": "For instance, when a bug report is submitted, LIBRO could be used to automatically generate a test case that reproduces the bug. The LLM would perform code-related tasks based on the bug report, and the post-processing steps would then rank the produced tests according to their validity. This could significantly reduce the time and effort required to write tests, enhancing developer efficiency."}, "category": "USE CASES", "novelty_analysis": "The paper presents a novel approach to automating test generation from bug reports using LLMs. While automated test generation techniques have been studied for almost half a century, the use of LLMs to perform code-related tasks and the application of post-processing steps to rank the produced tests is a unique contribution to the field.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, as it delves into the details of the LIBRO framework and how it uses LLMs to perform code-related tasks. However, it does not require advanced mathematical knowledge and can be understood by someone with a background in computer science or software engineering.", "technical_score": 2, "enjoyable_analysis": "The paper is well-organized and presents a novel and intriguing contribution to the field of automated test generation. The use of LLMs to automate the generation of tests from bug reports is a fascinating concept, and the paper does a good job of explaining the approach and its potential benefits.", "enjoyable_score": 3}