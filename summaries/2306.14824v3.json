{"Published": "2023-07-13", "Title": "Kosmos-2: Grounding Multimodal Large Language Models to the World", "Authors": "Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei", "Summary": "We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.", "main_contribution": {"headline": "KOSMOS-2: A Multimodal Large Language Model with Grounding Capabilities", "description": "The paper introduces KOSMOS-2, a Multimodal Large Language Model (MLLM) that can perceive object descriptions and ground text to the visual world. The authors represent refer expressions as links in Markdown, where object descriptions are sequences of location tokens. This approach, combined with multimodal corpora, allows the construction of large-scale data of grounded image-text pairs (called GRIT) to train the model. KOSMOS-2 extends the capabilities of MLLMs by integrating grounding capability into downstream applications, which is a significant step towards the development of Embodiment AI."}, "takeaways": {"headline": "KOSMOS-2 offers a new approach to grounding in MLLMs", "description": "KOSMOS-2 presents a novel approach to grounding in MLLMs, which can be beneficial for AI practitioners working on vision-language tasks. The model's ability to perceive object descriptions and ground text to the visual world can enhance human-AI interaction. The use of Markdown to represent refer expressions and the creation of GRIT for training the model can be adopted by practitioners to improve the performance of their MLLMs in similar tasks.", "example": "For instance, an AI practitioner can use KOSMOS-2's approach to train an MLLM for a task like image captioning. The model can be trained with GRIT, and the refer expressions can be represented as links in Markdown. This would allow the model to perceive object descriptions in the image and generate a caption that is grounded to the visual world."}, "category": "ARCHITECTURES", "novelty_analysis": "The introduction of KOSMOS-2 and its grounding capabilities represents a significant advancement in the field of MLLMs. The use of Markdown to represent refer expressions and the creation of GRIT for training the model are novel approaches that have not been explored in previous works.", "novelty_score": 3, "technical_analysis": "The paper is highly technical, detailing the architecture of KOSMOS-2 and the methods used for training the model. It requires a deep understanding of MLLMs and grounding techniques to fully comprehend the content.", "technical_score": 3, "enjoyable_analysis": "The paper is well-structured and presents a novel approach to grounding in MLLMs. However, the high level of technical detail might make it challenging for readers without a strong background in the field.", "enjoyable_score": 2}