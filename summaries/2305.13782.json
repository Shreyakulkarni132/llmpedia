{"Published": "2023-05-23", "Title": "Images in Language Space: Exploring the Suitability of Large Language Models for Vision & Language Tasks", "Authors": "Sherzod Hakimov, David Schlangen", "Summary": "Large language models have demonstrated robust performance on various language tasks using zero-shot or few-shot learning paradigms. While being actively researched, multimodal models that can additionally handle images as input have yet to catch up in size and generality with language-only models. In this work, we ask whether language-only models can be utilised for tasks that require visual input -- but also, as we argue, often require a strong reasoning component. Similar to some recent related work, we make visual information accessible to the language model using separate verbalisation models. Specifically, we investigate the performance of open-source, open-access language models against GPT-3 on five vision-language tasks when given textually-encoded visual information. Our results suggest that language models are effective for solving vision-language tasks even with limited samples. This approach also enhances the interpretability of a model's output by providing a means of tracing the output back through the verbalised image content.", "main_contribution": {"headline": "Language-only models can effectively handle vision-language tasks", "description": "The paper investigates the potential of large language models (LLMs) to handle tasks that require visual input. The authors argue that these tasks often require a strong reasoning component, which LLMs are well-equipped to handle. To make visual information accessible to the language model, the authors use separate verbalisation models. The performance of open-source, open-access language models is compared against GPT-3 on five vision-language tasks when given textually-encoded visual information. The results suggest that language models can effectively solve vision-language tasks even with limited samples."}, "takeaways": {"headline": "LLMs can be leveraged for vision-language tasks with textually-encoded visual information", "description": "The findings of this paper suggest that LLMs can be effectively used for tasks that require visual input, by encoding the visual information in language. This approach not only allows LLMs to handle vision-language tasks, but also enhances the interpretability of a model's output by providing a means of tracing the output back through the verbalised image content. This could be particularly useful in applications where both visual and textual data are important, such as image captioning, visual question answering, or visual storytelling.", "example": "For instance, an image could be encoded into textual form using a verbalisation model, and then fed into an LLM for a task like image captioning. The LLM would generate a caption based on the textually-encoded visual information, and the output could be traced back through the verbalised image content for interpretability."}, "category": "USE CASES", "novelty_analysis": "The paper presents a novel approach to using LLMs for vision-language tasks by encoding visual information in language. While the idea of using LLMs for tasks beyond pure language tasks is not new, the specific approach of using verbalisation models to encode visual information is a unique contribution.", "novelty_score": 2, "technical_analysis": "The paper is somewhat technical as it discusses the use of verbalisation models to encode visual information and the application of this approach to LLMs. However, it does not delve into complex mathematical theories or algorithms, making it accessible to readers with a basic understanding of LLMs and machine learning.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting approach to using LLMs for vision-language tasks. The results are clearly presented and the implications of the findings are discussed, making it an enjoyable read for those interested in the application of LLMs in new areas.", "enjoyable_score": 2}