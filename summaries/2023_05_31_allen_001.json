{"Published": "2023-05-31", "Title": "Multimodal Speech Recognition for Language-Guided Embodied Agents", "Authors": "Allen Chang, Xiaoyuan Zhu, Aarav Monga, Seoho Ahn, Tejas Srinivasan, Jesse Thomason", "Summary": "Benchmarks for language-guided embodied agents typically assume text-based instructions, but deployed agents will encounter spoken instructions. While Automatic Speech Recognition (ASR) models can bridge the input gap, erroneous ASR transcripts can hurt the agents' ability to complete tasks. In this work, we propose training a multimodal ASR model to reduce errors in transcribing spoken instructions by considering the accompanying visual context. We train our model on a dataset of spoken instructions, synthesized from the ALFRED task completion dataset, where we simulate acoustic noise by systematically masking spoken words. We find that utilizing visual observations facilitates masked word recovery, with multimodal ASR models recovering up to 30% more masked words than unimodal baselines. We also find that a text-trained embodied agent successfully completes tasks more often by following transcribed instructions from multimodal ASR models. github.com/Cylumn/embodied-multimodal-asr", "main_contribution": "The paper introduces a novel approach to Automatic Speech Recognition (ASR) for language-guided embodied agents, proposing a multimodal ASR model that leverages visual context to reduce errors in transcribing spoken instructions. The authors create a synthetic dataset of spoken instructions, derived from the ALFRED task completion dataset, and simulate acoustic noise by systematically masking spoken words. The multimodal ASR model is trained on this dataset and evaluated on its ability to recover masked words.", "takeaways": "The multimodal ASR model presents a significant improvement over unimodal ASR models, recovering up to 30% more masked words. This suggests that incorporating visual context can significantly enhance the performance of ASR models, particularly in the context of language-guided embodied agents. The practical implications of this work are significant, as it can improve the effectiveness of human-robot interaction in real-world scenarios where spoken instructions are often used.", "novelty_analysis": "The paper presents a novel approach to ASR by incorporating visual context, which is a significant departure from traditional unimodal ASR models. The creation of a synthetic dataset for training and evaluation also adds to the novelty of the work.", "novelty_score": 3, "category": "ARCHITECTURES", "technical_analysis": "The paper is somewhat technical, discussing the creation of a synthetic dataset, the training of the multimodal ASR model, and the evaluation of its performance. However, the concepts are explained clearly and should be understandable to someone with a background in computer science.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents a novel and interesting contribution to the field of ASR. The clear explanation of the concepts and the practical implications of the work make it an enjoyable read.", "enjoyable_score": 3}