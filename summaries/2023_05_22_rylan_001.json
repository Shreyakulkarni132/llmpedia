{"Published": "2023-05-22", "Title": "Are Emergent Abilities of Large Language Models a Mirage?", "Authors": "Rylan Schaeffer, Brando Miranda, Sanmi Koyejo", "Summary": "Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due to the researcher's choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous predictable changes in model performance. We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities; (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.", "main_contribution": {"headline": "Emergent abilities in LLMs may be a result of metric choice, not model scaling", "description": "The paper challenges the notion that emergent abilities in Large Language Models (LLMs) are a result of model scaling. The authors propose that these abilities may instead be a consequence of the researcher's choice of metric. They argue that nonlinear or discontinuous metrics can produce apparent emergent abilities, while linear or continuous metrics result in smooth, predictable changes in model performance. This hypothesis is tested and confirmed through three different analyses, suggesting that emergent abilities may not be a fundamental property of scaling AI models."}, "takeaways": {"headline": "Choice of metric can influence perceived emergent abilities in LLMs", "description": "LLM practitioners should be aware that the choice of metric can significantly influence the perceived emergent abilities of a model. Nonlinear or discontinuous metrics can create the illusion of emergent abilities, while linear or continuous metrics may reveal a smoother, more predictable performance progression. This insight can help practitioners better interpret model performance and avoid misattributions of emergent abilities to model scaling.", "example": "For instance, if an LLM shows a sudden improvement in performance on a specific task when scaled up, practitioners should consider whether this is due to a fundamental change in the model's behavior or simply a result of the chosen metric. By testing the model with both nonlinear and linear metrics, they can gain a more accurate understanding of the model's capabilities."}, "category": "BEHAVIOR", "novelty_analysis": "The paper presents a novel perspective on the emergent abilities of LLMs, challenging the prevailing view that these abilities are a result of model scaling. The authors' proposition that these abilities may instead be a consequence of metric choice represents a significant departure from existing theories.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, presenting a mathematical model to support its hypothesis and conducting three different analyses to test this model. However, the concepts and methods used are well-explained and should be accessible to readers with a background in AI and machine learning.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents a compelling argument that challenges existing theories on the emergent abilities of LLMs. The authors' thorough testing of their hypothesis and clear presentation of their findings make for an engaging and insightful read.", "enjoyable_score": 3}