{"Published": "2023-05-13", "Title": "Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge", "Authors": "Jiangjie Chen, Wei Shi, Ziquan Fu, Sijie Cheng, Lei Li, Yanghua Xiao", "Summary": "Large language models (LLMs) have been widely studied for their ability to store and utilize positive knowledge. However, negative knowledge, such as \"lions don't live in the ocean\", is also ubiquitous in the world but rarely mentioned explicitly in the text. What do LLMs know about negative knowledge? This work examines the ability of LLMs to negative commonsense knowledge. We design a constrained keywords-to-sentence generation task (CG) and a Boolean question-answering task (QA) to probe LLMs. Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions. We term this phenomenon the belief conflict of LLMs. Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.", "main_contribution": {"headline": "LLMs exhibit belief conflict in handling negative commonsense knowledge", "description": "This paper investigates the ability of Large Language Models (LLMs) to handle negative commonsense knowledge. The authors designed a constrained keywords-to-sentence generation task and a Boolean question-answering task to probe LLMs. The study reveals that while LLMs can correctly answer yes-or-no questions based on negative commonsense knowledge, they often fail to generate valid sentences grounded in such knowledge. This inconsistency, termed as the 'belief conflict' of LLMs, is attributed to statistical shortcuts and negation reporting bias during language modeling pre-training."}, "takeaways": {"headline": "Understanding LLMs' belief conflict can guide better model training and usage", "description": "The discovery of 'belief conflict' in LLMs provides valuable insights for practitioners. It highlights the need for careful consideration when using LLMs for tasks involving negative commonsense knowledge. The findings suggest that while LLMs can be reliable for Boolean question-answering tasks, they may not generate accurate sentences when given negative knowledge. This understanding can guide practitioners in choosing appropriate tasks for LLMs and in designing training strategies to mitigate the belief conflict.", "example": "For instance, if an LLM is tasked to generate a sentence with the keywords 'lion, located at, ocean', it might incorrectly state 'Lions live in the ocean.' However, if asked 'Do lions live in the ocean?', the same LLM would correctly answer 'No.' This inconsistency should be considered when designing tasks for LLMs."}, "category": "BEHAVIOR", "novelty_analysis": "The paper presents a novel investigation into the handling of negative commonsense knowledge by LLMs. The discovery of 'belief conflict' and its causes is a unique contribution, shedding light on an underexplored aspect of LLM behavior.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, discussing the design of probing tasks and the analysis of LLM behavior. However, it does not delve into complex mathematical theories or algorithms, making it accessible to readers with a basic understanding of LLMs.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an intriguing investigation into LLM behavior. The discovery of 'belief conflict' and its implications make for an interesting read for those interested in understanding the nuances of LLMs.", "enjoyable_score": 2}