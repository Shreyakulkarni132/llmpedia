{"Published": "2023-06-09", "Title": "PoET: A generative model of protein families as sequences-of-sequences", "Authors": "Timothy F. Truong Jr, Tristan Bepler", "Summary": "Generative protein language models are a natural way to design new proteins with desired functions. However, current models are either difficult to direct to produce a protein from a specific family of interest, or must be trained on a large multiple sequence alignment (MSA) from the specific family of interest, making them unable to benefit from transfer learning across families. To address this, we propose $\\textbf{P}$r$\\textbf{o}$tein $\\textbf{E}$volutionary $\\textbf{T}$ransformer (PoET), an autoregressive generative model of whole protein families that learns to generate sets of related proteins as sequences-of-sequences across tens of millions of natural protein sequence clusters. PoET can be used as a retrieval-augmented language model to generate and score arbitrary modifications conditioned on any protein family of interest, and can extrapolate from short context lengths to generalize well even for small families. This is enabled by a unique Transformer layer; we model tokens sequentially within sequences while attending between sequences order invariantly, allowing PoET to scale to context lengths beyond those used during training. PoET outperforms existing protein language models and evolutionary sequence models for variant function prediction in extensive experiments on deep mutational scanning datasets, improving variant effect prediction across proteins of all MSA depths.", "main_contribution": {"headline": "PoET: A novel generative model for protein families as sequences-of-sequences", "description": "The paper introduces the Protein Evolutionary Transformer (PoET), a generative model that learns to generate sets of related proteins as sequences-of-sequences across tens of millions of natural protein sequence clusters. Unlike existing models, PoET can be directed to produce a protein from a specific family of interest and benefits from transfer learning across families. It uses a unique Transformer layer to model tokens sequentially within sequences while attending between sequences order invariantly. This allows PoET to scale to context lengths beyond those used during training and generalize well even for small families. The model outperforms existing protein language models and evolutionary sequence models in variant function prediction."}, "takeaways": {"headline": "PoET offers a new approach to protein design and function prediction", "description": "PoET's ability to generate and score arbitrary modifications conditioned on any protein family of interest makes it a powerful tool for protein design and function prediction. It can be used to design new proteins with desired functions, reducing the need for costly and difficult experimental methods. Furthermore, its ability to extrapolate from short context lengths and generalize well for small families makes it applicable to a wide range of protein families. The model's superior performance in variant function prediction also suggests its potential in pharmaceuticals and biotechnology.", "example": "For instance, an LLM practitioner working in biotechnology could use PoET to generate a set of related proteins for a specific family of interest. The model could then score the generated proteins based on their likelihood of having the desired function, helping to narrow down the protein sequence search space and reduce the need for expensive experiments."}, "category": "ARCHITECTURES", "novelty_analysis": "The introduction of PoET represents a significant advancement in the field of generative protein language models. Its unique Transformer layer and ability to benefit from transfer learning across families are novel features that set it apart from existing models.", "novelty_score": 3, "technical_analysis": "The paper is highly technical, delving into the intricate details of the PoET model and its unique Transformer layer. It requires a solid understanding of protein language models, Transformer models, and protein sequence alignments.", "technical_score": 3, "enjoyable_analysis": "The paper presents a novel and intriguing contribution to the field of generative protein language models. However, its highly technical content and focus on a specialized application may make it a challenging read for those without a background in the field.", "enjoyable_score": 2}