{"Published": "2022-05-04", "Title": "A Systematic Evaluation of Large Language Models of Code", "Authors": "Frank F. Xu, Uri Alon, Graham Neubig, Vincent J. Hellendoorn", "Summary": "Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex (Chen et al., 2021)) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing open-source models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, which was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at https://github.com/VHellendoorn/Code-LMs, which enables future research and application in this area.", "main_contribution": {"headline": "Introduction of PolyCoder, an open-source large language model trained on multi-lingual code corpus", "description": "The paper introduces PolyCoder, a new open-source large language model (LLM) of code, trained on a multi-lingual corpus of code. PolyCoder, with 2.7 billion parameters based on the GPT-2 architecture, was trained on 249GB of code across 12 programming languages. The authors claim that in the C programming language, PolyCoder outperforms all existing models, including the state-of-the-art Codex. This model is publicly available, enabling future research and application in the area of code LLMs."}, "takeaways": {"headline": "PolyCoder offers a promising open-source alternative for code completion and synthesis", "description": "For practitioners working with LLMs for code completion or synthesis, PolyCoder presents a promising open-source alternative to proprietary models like Codex. Given its training on a multi-lingual code corpus, it could potentially offer better performance across a variety of programming languages. The public availability of the model also allows for further fine-tuning and adaptation to specific domains or tasks, which is not possible with black-box models like Codex.", "example": "For instance, an LLM practitioner could use PolyCoder for a code completion task in a specific programming language. They could further fine-tune the model on a domain-specific code corpus to improve its performance for that particular domain."}, "category": "TRAINING", "novelty_analysis": "While the concept of training LLMs on code is not new, the introduction of PolyCoder, an open-source model trained on a multi-lingual code corpus, is a significant contribution. It provides an alternative to proprietary models and opens up opportunities for further research and application in this area.", "novelty_score": 2, "technical_analysis": "The paper is somewhat technical, discussing the training of the PolyCoder model and its comparison with other models. However, it does not delve into complex mathematical theories or algorithms, making it accessible to readers with a basic understanding of LLMs and code completion tasks.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting contribution to the field of LLMs for code. The introduction of an open-source model and its comparison with existing models makes for an engaging read.", "enjoyable_score": 2}