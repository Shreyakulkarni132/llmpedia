{"Published": "2023-02-01", "Title": "Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models", "Authors": "Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu Chen", "Summary": "Large language models can perform various reasoning tasks by using chain-of-thought prompting, which guides them to find answers through step-by-step demonstrations. However, the quality of the prompts depends on the demonstrations given to the models, and creating many of them by hand is costly. We introduce Synthetic prompting, a method that leverages a few handcrafted examples to prompt the model to generate more examples by itself, and selects effective demonstrations to elicit better reasoning. Our method alternates between a backward and forward process to generate new examples. The backward process generates a question that match a sampled reasoning chain, so that the question is solvable and clear. The forward process produces a more detailed reasoning chain for the question, improving the quality of the example. We evaluate our method on numerical, symbolic, and algorithmic reasoning tasks, and show that it outperforms existing prompting techniques.", "main_contribution": {"headline": "Synthetic Prompting: A novel method for generating chain-of-thought demonstrations", "description": "The paper introduces Synthetic Prompting, a new method that enables Large Language Models (LLMs) to generate more examples for chain-of-thought demonstrations. This method alternates between a backward and forward process to generate new examples. The backward process generates a question that matches a sampled reasoning chain, ensuring the question is solvable and clear. The forward process produces a more detailed reasoning chain for the question, improving the quality of the example. The method also includes a selection scheme based on in-cluster complexity to choose the most effective demonstrations from the augmented set."}, "takeaways": {"headline": "Synthetic Prompting improves reasoning capabilities of LLMs", "description": "Synthetic Prompting can be used to enhance the reasoning capabilities of LLMs. By generating more examples for chain-of-thought demonstrations, it can improve the quality of prompts and thus the reasoning performance of LLMs. This method can be particularly useful in tasks that require complex and diverse reasoning patterns. It can also reduce the cost and effort of manually creating a large and diverse set of examples for demonstration selection.", "example": "For instance, given a few seed examples, an LLM can be prompted to generate more examples using Synthetic Prompting. The backward process generates a question based on a self-generated reasoning chain, and the forward process refines the reasoning chain to be more precise and consistent with the question. This process is repeated until enough synthetic examples are obtained."}, "category": "PROMPTING", "novelty_analysis": "The introduction of Synthetic Prompting represents a significant advancement in the field of prompting techniques for LLMs. It not only automates the generation of chain-of-thought demonstrations but also improves their quality, which can significantly enhance the reasoning capabilities of LLMs.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical as it introduces a new method and explains its workings in detail. However, it does not delve into complex mathematical theories or algorithms, making it accessible to readers with a basic understanding of LLMs and prompting techniques.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents a novel and intriguing contribution to the field of LLMs. It provides practical insights and demonstrates the efficacy of the proposed method through various reasoning tasks, making it an enjoyable read.", "enjoyable_score": 3}