{"Published": "2023-05-23", "Title": "Self-Critique Prompting with Large Language Models for Inductive Instructions", "Authors": "Rui Wang, Hongru Wang, Fei Mi, Yi Chen, Ruifeng Xu, Kam-Fai Wong", "Summary": "Numerous works are proposed to improve or evaluate the capabilities of Large language models (LLMs) to fulfill user instructions. However, they neglect the possibility that user inputs may inherently contain incorrect information due to users' false beliefs or malicious intents. In this way, blindly adhering to users' false content will cause deception and harm. To address this problem, we propose a challenging benchmark consisting of Inductive Instructions (INDust) to evaluate whether LLMs could resist these instructions. The INDust includes 15K instructions across three categories: Fact-Checking Instructions, Questions based on False Premises, and Creative Instructions based on False Premises. Our experiments on several strong LLMs reveal that current LLMs can be easily deceived by INDust into generating misleading and malicious statements. Hence we employ Self-Critique prompting to encourage LLMs to not only critique themselves like in previous works but also the users, which show remarkable improvement in handling inductive instructions under both zero-shot and few-shot settings.", "main_contribution": {"headline": "Self-Critique Prompting for LLMs to Handle Inductive Instructions", "description": "The paper introduces a novel benchmark, Inductive Instructions (INDust), to evaluate the ability of Large Language Models (LLMs) to resist instructions based on false premises. The authors identify a gap in existing LLM research, which often overlooks the possibility of user inputs containing incorrect information due to false beliefs or malicious intents. The INDust benchmark includes 15K instructions across three categories: Fact-Checking Instructions, Questions based on False Premises, and Creative Instructions based on False Premises. The paper also proposes the use of Self-Critique prompting to improve LLMs' ability to handle inductive instructions, showing significant improvement in both zero-shot and few-shot settings."}, "takeaways": {"headline": "Self-Critique Prompting Enhances LLMs' Resistance to False Instructions", "description": "The paper's findings highlight the importance of considering the possibility of false premises in user inputs when working with LLMs. The proposed Self-Critique prompting technique can be used to improve LLMs' ability to resist such instructions, thereby reducing the risk of generating misleading or harmful content. The INDust benchmark provides a valuable resource for evaluating and improving LLMs in this regard. LLM practitioners can use these insights to enhance the reliability and safety of their models.", "example": "For instance, when a user asks 'Why does eating watermelon cause cancer?', the LLM, using Self-Critique prompting, would first critique the false premise in the question before generating a response, thereby avoiding the propagation of misinformation."}, "category": "BEHAVIOR", "novelty_analysis": "The paper presents a novel benchmark for evaluating LLMs' resistance to false instructions, addressing a previously overlooked aspect of LLM research. The use of Self-Critique prompting to improve LLMs' ability to handle such instructions is also a unique contribution.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, discussing the design of the INDust benchmark and the implementation of Self-Critique prompting. However, it does not delve into complex mathematical theories or algorithms, making it accessible to readers with a basic understanding of LLMs.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting and important problem in the field of LLMs. The proposed solutions and their implications are clearly explained, making it an engaging read for those interested in the behavior and safety of LLMs.", "enjoyable_score": 3}