{"Published": "2023-07-31", "Title": "L-Eval: Instituting Standardized Evaluation for Long Context Language Models", "Authors": "Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, Xipeng Qiu", "Summary": "Recently, there has been growing interest in extending the context length of instruction-following models in order to effectively process single-turn long input (e.g. summarizing a paper) and conversations with more extensive histories. While proprietary models such as GPT-4 and Claude have shown significant strides in handling extremely lengthy input, open-sourced models are still in the early stages of experimentation. It also remains unclear whether extending the context can offer substantial gains over traditional methods such as retrieval, and to what extent it improves upon their regular counterparts in practical downstream tasks. To address this challenge, we propose instituting standardized evaluation for long context language models. Concretely, we develop L-Eval which contains 411 long documents and over 2,000 human-labeled query-response pairs encompassing areas such as law, finance, school lectures, lengthy conversations, news, long-form novels, and meetings. L-Eval also adopts diverse evaluation methods and instruction styles, enabling a more reliable assessment of Long Context Language Models (LCLMs). Our findings indicate that while open-source models typically lag behind commercial models, they still exhibit impressive performance compared with their regular versions. LLaMA2-13B achieves the best results on both open-ended tasks (win \\textbf{42}\\% vs turbo-16k-0613) and closed-ended tasks with only 4k context length. We release our new evaluation suite, code, and all generation results including predictions from all open-sourced LCLMs, GPT4-32k, Cluade-100k at {\\url{https://github.com/OpenLMLab/LEval}}.", "main_contribution": {"headline": "L-Eval: A Standardized Evaluation Suite for Long Context Language Models", "description": "The paper introduces L-Eval, a standardized evaluation suite for Long Context Language Models (LCLMs). L-Eval is designed to assess the performance of LCLMs on tasks involving long documents and extensive conversation histories. The suite contains 411 long documents and over 2,000 human-labeled query-response pairs covering a wide range of areas such as law, finance, school lectures, lengthy conversations, news, long-form novels, and meetings. The authors also propose diverse evaluation methods and instruction styles to ensure a more reliable assessment of LCLMs. The findings from the evaluation indicate that while open-source models typically lag behind commercial models, they still exhibit impressive performance compared with their regular versions."}, "takeaways": {"headline": "L-Eval Provides Comprehensive Evaluation of LCLMs", "description": "L-Eval offers a standardized evaluation suite for LCLMs, providing a benchmark for comparing the performance of different models on long context tasks. The suite's diverse evaluation methods and instruction styles allow for a more comprehensive and reliable assessment of LCLMs. The findings suggest that while open-source models typically lag behind commercial models, they still exhibit impressive performance compared with their regular versions. This could guide practitioners in selecting the most suitable LCLM for their specific use cases.", "example": "For instance, if a practitioner is working on a task that involves processing long documents or extensive conversation histories, they could use L-Eval to evaluate the performance of different LCLMs on these tasks. This could help them identify the most suitable model for their task, potentially improving the efficiency and effectiveness of their system."}, "category": "TRAINING", "novelty_analysis": "The introduction of L-Eval represents a significant contribution to the field of LCLMs. While there have been previous efforts to evaluate the performance of LCLMs, L-Eval is unique in its comprehensive approach, covering a wide range of areas and adopting diverse evaluation methods and instruction styles. This makes it a valuable tool for researchers and practitioners working with LCLMs.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, discussing the design and development of the L-Eval evaluation suite and presenting the results of the evaluation of various LCLMs. However, the concepts and methodologies are explained clearly and comprehensively, making it accessible to readers with a basic understanding of LCLMs.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents its findings in a clear and concise manner. The introduction of L-Eval and the discussion of its potential applications in the field of LCLMs make for an engaging read. The inclusion of detailed results from the evaluation of various LCLMs also adds to the paper's interest.", "enjoyable_score": 2}