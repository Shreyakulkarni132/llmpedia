{"Published": "2023-06-09", "Title": "PoET: A generative model of protein families as sequences-of-sequences", "Authors": "Timothy F. Truong Jr, Tristan Bepler", "Summary": "Generative protein language models are a natural way to design new proteins with desired functions. However, current models are either difficult to direct to produce a protein from a specific family of interest, or must be trained on a large multiple sequence alignment (MSA) from the specific family of interest, making them unable to benefit from transfer learning across families. To address this, we propose $\\textbf{P}$r$\\textbf{o}$tein $\\textbf{E}$volutionary $\\textbf{T}$ransformer (PoET), an autoregressive generative model of whole protein families that learns to generate sets of related proteins as sequences-of-sequences across tens of millions of natural protein sequence clusters. PoET can be used as a retrieval-augmented language model to generate and score arbitrary modifications conditioned on any protein family of interest, and can extrapolate from short context lengths to generalize well even for small families. This is enabled by a unique Transformer layer; we model tokens sequentially within sequences while attending between sequences order invariantly, allowing PoET to scale to context lengths beyond those used during training. PoET outperforms existing protein language models and evolutionary sequence models for variant function prediction in extensive experiments on deep mutational scanning datasets, improving variant effect prediction across proteins of all MSA depths.", "main_contribution": "The paper introduces the Protein Evolutionary Transformer (PoET), a generative model for protein families. Unlike previous models, PoET can generate sets of related proteins as sequences-of-sequences across a vast number of natural protein sequence clusters. It uses a unique Transformer layer to model tokens sequentially within sequences while attending between sequences order invariantly, enabling it to scale to context lengths beyond those used during training.", "takeaways": "PoET offers a significant advancement in the field of protein language models. It can be used as a retrieval-augmented language model to generate and score arbitrary modifications conditioned on any protein family of interest. It also generalizes well even for small families and outperforms existing protein language models and evolutionary sequence models for variant function prediction. This could have significant implications in pharmaceuticals and biotechnology, enabling the design of more novel proteins and functions.", "novelty_analysis": "The introduction of PoET represents a significant advancement in the field of protein language models. It addresses the limitations of previous models, which were either difficult to direct to produce a protein from a specific family of interest or had to be trained on a large multiple sequence alignment (MSA) from the specific family of interest, making them unable to benefit from transfer learning across families.", "novelty_score": 3, "category": "ARCHITECTURES", "technical_analysis": "The paper is highly technical, introducing a new model and discussing its implementation in detail. It requires a deep understanding of protein language models and the Transformer architecture.", "technical_score": 3, "enjoyable_analysis": "The paper is well-structured and presents a novel and interesting contribution to the field of protein language models. However, its highly technical content may be challenging for non-experts.", "enjoyable_score": 2}