{"Published": "2023-06-05", "Title": "Repository-Level Prompt Generation for Large Language Models of Code", "Authors": "Disha Shrivastava, Hugo Larochelle, Daniel Tarlow", "Summary": "With the success of large language models (LLMs) of code and their use as code assistants (e.g. Codex used in GitHub Copilot), techniques for introducing domain-specific knowledge in the prompt design process become important. In this work, we propose a framework called Repo-Level Prompt Generator that learns to generate example-specific prompts using prompt proposals. The prompt proposals take context from the entire repository, thereby incorporating both the structure of the repository and the context from other relevant files (e.g. imports, parent class files). Our technique doesn't require any access to the weights of the LLM, making it applicable in cases where we only have black-box access to the LLM. We conduct experiments on the task of single-line code-autocompletion using code repositories taken from Google Code archives. We demonstrate that an oracle constructed from our prompt proposals gives a remarkably high relative improvement of 36% over Codex, showing the quality of these proposals. Further, we show that when we train a model to predict a prompt proposal, we can achieve significant performance gains over Codex and other baselines. We release our code, data, and trained checkpoints at: \\url{https://github.com/shrivastavadisha/repo_level_prompt_generation}.", "main_contribution": {"headline": "Repo-Level Prompt Generator enhances LLMs for code auto-completion", "description": "The paper introduces a novel framework, Repo-Level Prompt Generator, that generates example-specific prompts for Large Language Models (LLMs) of code. The framework uses prompt proposals that take context from the entire code repository, including the structure of the repository and context from other relevant files. This approach does not require access to the weights of the LLM, making it applicable even when only black-box access to the LLM is available. The authors demonstrate that this technique significantly improves the performance of LLMs in the task of single-line code auto-completion."}, "takeaways": {"headline": "Repo-Level Prompt Generator improves code auto-completion with LLMs", "description": "The Repo-Level Prompt Generator framework can be used to enhance the performance of LLMs in code auto-completion tasks. By taking context from the entire code repository, it generates more relevant and effective prompts. This approach can be particularly useful in scenarios where only black-box access to the LLM is available. The authors have released their code, data, and trained checkpoints, which can be used by practitioners to apply this technique in their own projects.", "example": "For instance, when using an LLM for code auto-completion, the Repo-Level Prompt Generator can be used to generate prompts. These prompts take into account the context from the entire code repository, including the structure of the repository and other relevant files, leading to more accurate and relevant auto-completion suggestions."}, "category": "PROMPTING", "novelty_analysis": "The introduction of the Repo-Level Prompt Generator represents a significant advancement in the field of LLMs of code. The technique of generating prompts based on the context from the entire code repository is a novel approach that has not been explored in previous work.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, as it introduces a new framework and discusses its implementation in detail. However, it does not delve into complex mathematical theories or algorithms, making it accessible to readers with a background in machine learning and coding.", "technical_score": 2, "enjoyable_analysis": "The paper is well-written and presents a novel and intriguing contribution to the field of LLMs of code. The practical implications of the work and the release of the code, data, and trained checkpoints make it an interesting and useful read for practitioners in the field.", "enjoyable_score": 3}