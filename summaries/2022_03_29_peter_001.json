{"Published": "2022-03-29", "Title": "Probing Factually Grounded Content Transfer with Factual Ablation", "Authors": "Peter West, Chris Quirk, Michel Galley, Yejin Choi", "Summary": "Despite recent success, large neural models often generate factually incorrect text. Compounding this is the lack of a standard automatic evaluation for factuality--it cannot be meaningfully improved if it cannot be measured. Grounded generation promises a path to solving both of these problems: models draw on a reliable external document (grounding) for factual information, simplifying the challenge of factuality. Measuring factuality is also simplified--to factual consistency, testing whether the generation agrees with the grounding, rather than all facts. Yet, without a standard automatic metric for factual consistency, factually grounded generation remains an open problem.   We study this problem for content transfer, in which generations extend a prompt, using information from factual grounding. Particularly, this domain allows us to introduce the notion of factual ablation for automatically measuring factual consistency: this captures the intuition that the model should be less likely to produce an output given a less relevant grounding document. In practice, we measure this by presenting a model with two grounding documents, and the model should prefer to use the more factually relevant one. We contribute two evaluation sets to measure this. Applying our new evaluation, we propose multiple novel methods improving over strong baselines.", "main_contribution": "The paper introduces the concept of 'factual ablation' as a method for automatically measuring factual consistency in large language models (LLMs). This method is based on the idea that a model should be less likely to produce an output given a less relevant grounding document. The authors present a model with two grounding documents, and the model should prefer to use the more factually relevant one. They also contribute two evaluation sets to measure this.", "takeaways": "The introduction of 'factual ablation' provides a new way to measure the factual consistency of LLMs, which is a critical aspect of their performance. This method could be particularly useful for practitioners who are working on content transfer tasks, where the model needs to extend a prompt using information from a factual grounding. The evaluation sets provided by the authors could also be valuable resources for researchers and practitioners in the field.", "novelty_analysis": "The concept of 'factual ablation' is a novel contribution to the field of LLMs. While the idea of using grounding documents to measure factual consistency is not new, the specific method of presenting the model with two documents and expecting it to prefer the more factually relevant one is a unique approach.", "novelty_score": 3, "category": "BEHAVIOR", "technical_analysis": "The paper is somewhat technical, as it introduces a new method for measuring factual consistency and discusses its implementation in detail. However, the concepts are explained clearly and should be understandable to someone with a background in computer science or AI research.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents a novel and interesting contribution to the field. However, the technical nature of the content and the specific focus on factual consistency might limit its appeal to a broader audience.", "enjoyable_score": 2}