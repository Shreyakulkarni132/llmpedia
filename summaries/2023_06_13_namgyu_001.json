{"Published": "2023-06-13", "Title": "Large Language Models Are Reasoning Teachers", "Authors": "Namgyu Ho, Laura Schmid, Se-Young Yun", "Summary": "Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper, we use these large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude. We propose Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tune smaller models. We evaluate our method on a wide range of public models and complex tasks. We find that Fine-tune-CoT enables substantial reasoning capability in small models, far outperforming prompt-based baselines and even the teacher model in many tasks. Additionally, we extend our method by leveraging the teacher model's ability to generate multiple distinct rationales for each original sample. Enriching the fine-tuning data with such diverse reasoning results in a substantial performance boost across datasets, even for very small models. We conduct ablations and sample studies to understand the emergence of reasoning capabilities of student models. Our code implementation and data are available at https://github.com/itsnamgyu/reasoning-teacher.", "main_contribution": "The paper introduces Fine-tune-CoT, a novel method that leverages large language models (LLMs) as 'reasoning teachers' to enable complex reasoning in smaller models. The method works by generating reasoning samples from the large teacher models, which are then used to fine-tune the smaller student models. The authors also extend their method by leveraging the teacher model's ability to generate multiple distinct rationales for each original sample, which when used to enrich the fine-tuning data, results in a substantial performance boost across datasets, even for very small models.", "takeaways": "Fine-tune-CoT offers a practical solution to the challenge of deploying large models at scale, by enabling complex reasoning in smaller models. This method significantly reduces the model size requirements, making it more feasible to deploy these models in real-world applications. The ability of Fine-tune-CoT to outperform even the teacher model in many tasks demonstrates its effectiveness and potential for wide-ranging applications.", "novelty_analysis": "The concept of using large models as 'reasoning teachers' to enable complex reasoning in smaller models is a unique approach to the challenge of deploying large models at scale. The extension of this method to leverage multiple distinct rationales for each original sample further enhances its novelty.", "novelty_score": 3, "category": "FINE-TUNING", "technical_analysis": "The paper is somewhat technical, introducing a new method and discussing its implementation and evaluation. However, the authors provide clear explanations and visual aids to help readers understand the method and its benefits.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents a novel and interesting contribution to the field. The clear explanations and visual aids make it relatively easy to read and understand, making it an enjoyable read for those interested in the field.", "enjoyable_score": 3}