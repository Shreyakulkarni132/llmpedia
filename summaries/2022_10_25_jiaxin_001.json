{"Published": "2022-10-25", "Title": "Large Language Models Can Self-Improve", "Authors": "Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han", "Summary": "Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate \"high-confidence\" rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.", "main_contribution": {"headline": "LLMs can self-improve reasoning abilities using unlabeled datasets", "description": "The paper presents a novel method for self-improvement of Large Language Models (LLMs) using unlabeled datasets. The authors use a pre-trained LLM to generate 'high-confidence' rationale-augmented answers for unlabeled questions using a technique called Chain-of-Thought prompting and self-consistency. The LLM is then fine-tuned using these self-generated solutions as target outputs. This approach significantly improves the general reasoning ability of the LLM and achieves state-of-the-art-level performance without any ground truth label."}, "takeaways": {"headline": "Self-improvement of LLMs can enhance performance without labeled data", "description": "The paper's approach demonstrates that LLMs can improve their reasoning abilities without the need for labeled data, which is typically resource-intensive to obtain. By generating 'high-confidence' rationale-augmented answers for unlabeled questions and using these for fine-tuning, LLMs can achieve improved performance. This method can be particularly useful in scenarios where labeled data is scarce or expensive to obtain.", "example": "For instance, an LLM can be used to generate answers for a set of unlabeled questions. These answers, along with their rationale, can then be used to fine-tune the LLM, improving its ability to answer similar questions in the future."}, "category": "FINE-TUNING", "novelty_analysis": "The paper presents a novel approach to improving the reasoning abilities of LLMs using unlabeled data. This is a significant departure from traditional methods that rely on extensive supervision and labeled data for fine-tuning LLMs.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, discussing the details of the Chain-of-Thought prompting technique and the process of fine-tuning the LLM using self-generated solutions. However, it does not delve into complex mathematical theories or algorithms.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting approach to improving the reasoning abilities of LLMs. The results are impressive and the implications for the field are significant, making it an enjoyable read.", "enjoyable_score": 3}