{"Published": "2023-07-17", "Title": "AlpaGasus: Training A Better Alpaca with Fewer Data", "Authors": "Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, Hongxia Jin", "Summary": "Large language models~(LLMs) obtain instruction-following capability through instruction-finetuning (IFT) on supervised instruction/response data. However, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly contain many low-quality instances with incorrect or irrelevant responses, which are misleading and detrimental to IFT. In this paper, we propose a simple and effective data selection strategy that automatically identifies and removes low-quality data using a strong LLM (e.g., ChatGPT). To this end, we introduce AlpaGasus, which is finetuned on only 9k high-quality data filtered from the 52k Alpaca data. AlpaGasus significantly outperforms the original Alpaca as evaluated by GPT-4 on multiple test sets and its 13B variant matches $>90\\%$ performance of its teacher LLM (i.e., Text-Davinci-003) on test tasks. It also provides 5.7x faster training, reducing the training time for a 7B variant from 80 minutes (for Alpaca) to 14 minutes \\footnote{We apply IFT for the same number of epochs as Alpaca(7B) but on fewer data, using 4$\\times$NVIDIA A100 (80GB) GPUs and following the original Alpaca setting and hyperparameters.}. Overall, AlpaGasus demonstrates a novel data-centric IFT paradigm that can be generally applied to instruction-tuning data, leading to faster training and better instruction-following models. Our project page is available at: \\url{https://lichang-chen.github.io/AlpaGasus/}.", "main_contribution": {"headline": "AlpaGasus: A novel data-centric IFT paradigm for faster and better instruction-following models", "description": "The paper introduces AlpaGasus, a novel approach to instruction fine-tuning (IFT) of Large Language Models (LLMs) that focuses on data quality. AlpaGasus uses a data selection strategy to automatically identify and remove low-quality data from IFT datasets, using a strong LLM like ChatGPT. This approach significantly improves the performance of the model, as demonstrated by evaluations on multiple test sets. AlpaGasus also reduces training time, demonstrating the benefits of a data-centric IFT paradigm."}, "takeaways": {"headline": "AlpaGasus offers a data-centric approach to improve LLM performance and efficiency", "description": "AlpaGasus presents a new way for LLM practitioners to improve the performance of their models and reduce training time. By focusing on data quality and automatically filtering out low-quality data from IFT datasets, AlpaGasus can lead to better instruction-following models. This approach can be generally applied to instruction-tuning data, making it a versatile tool for LLM practitioners.", "example": "For instance, an LLM practitioner could use AlpaGasus to filter out low-quality data from their IFT dataset. The model would then be fine-tuned on this high-quality data, leading to improved performance and reduced training time."}, "category": "FINE-TUNING", "novelty_analysis": "The introduction of AlpaGasus represents a significant advancement in the field of LLMs. The focus on data quality and the automatic filtering of low-quality data from IFT datasets is a novel approach that has not been previously explored in depth.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, as it delves into the details of the AlpaGasus approach and how it improves the performance of LLMs. However, it does not require advanced mathematical knowledge and can be understood by someone with a background in computer science.", "technical_score": 2, "enjoyable_analysis": "The paper is well-written and presents a novel and intriguing contribution to the field of LLMs. The focus on data quality and the introduction of AlpaGasus make it an interesting read.", "enjoyable_score": 3}