{"Published": "2023-07-17", "Title": "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs", "Authors": "Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, Bingyi Kang", "Summary": "LLMs have demonstrated remarkable abilities at interacting with humans through language, especially with the usage of instruction-following data. Recent advancements in LLMs, such as MiniGPT-4, LLaVA, and X-LLM, further enlarge their abilities by incorporating multi-modal inputs, including image, video, and speech. Despite their effectiveness at generating precise and detailed language understanding of the given modality signal, these LLMs give up the ability to ground specific parts of inputs, thus only constructing a coarse-grained mapping. However, explicit and informative correspondence between text and other modalities will not only improve the user experience but also help to expand the application scenario of multi-modal LLMs. Therefore, we propose BuboGPT, a multi-modal LLM with visual grounding that can perform cross-modal interaction between vision, audio and language, providing fine-grained understanding of visual objects and other given modalities. As a result, BuboGPT is able to point out the specific location of an object in the image, when it is generating response or description for that object. Our contributions are two-fold: 1) An off-the-shelf visual grounding module based on SAM that extracts entities in a sentence and find corresponding masks in the image. 2) A two-stage training scheme and instruction dataset to endow joint text-image-audio understanding. Our experiments show that BuboGPT achieves impressive multi-modality understanding and visual grounding abilities during the interaction with human. It performs consistently well when provided by arbitrary modality combinations (either aligned or unaligned). Our code, model and dataset are available at https://bubo-gpt.github.io .", "main_contribution": {"headline": "BuboGPT: A Multi-Modal LLM with Visual Grounding for Fine-Grained Understanding", "description": "The paper introduces BuboGPT, a multi-modal Large Language Model (LLM) that incorporates visual grounding, enabling it to perform cross-modal interaction between vision, audio, and language. Unlike previous LLMs that only construct a coarse-grained mapping, BuboGPT can point out the specific location of an object in an image while generating a response or description for that object. The authors achieve this by introducing an off-the-shelf visual grounding module based on SAM that extracts entities in a sentence and finds corresponding masks in the image. They also propose a two-stage training scheme and instruction dataset to endow joint text-image-audio understanding."}, "takeaways": {"headline": "BuboGPT Enhances Multi-Modal Understanding and Visual Grounding in LLMs", "description": "BuboGPT's ability to perform cross-modal interaction and visual grounding can significantly improve the user experience and expand the application scenarios of multi-modal LLMs. For instance, in a chatbot application, BuboGPT can provide a more detailed and precise response by pointing out specific parts of an image or video while interacting with the user. The proposed visual grounding module and two-stage training scheme can be used as a reference for researchers and practitioners aiming to enhance the multi-modal understanding capabilities of their LLMs.", "example": "For example, when BuboGPT is given an image of a park and a sentence 'The dog is playing with the ball', it can identify the dog and the ball in the image and generate a response like 'Yes, I see the dog playing with the ball near the tree on the right side of the image.'"}, "category": "ARCHITECTURES", "novelty_analysis": "The introduction of BuboGPT marks a significant advancement in the field of multi-modal LLMs. The incorporation of visual grounding into LLMs and the ability to perform cross-modal interaction between vision, audio, and language are novel contributions that extend the capabilities of existing LLMs.", "novelty_score": 3, "technical_analysis": "The paper is highly technical, detailing the architecture of BuboGPT, the workings of the visual grounding module, and the two-stage training scheme. It requires a solid understanding of LLMs, multi-modal understanding, and visual grounding.", "technical_score": 3, "enjoyable_analysis": "The paper is well-structured and presents an interesting and novel approach to enhancing the capabilities of LLMs. However, the high level of technical detail might make it a challenging read for those without a strong background in the field.", "enjoyable_score": 2}