{"Published": "2023-05-23", "Title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks", "Authors": "Tiedong Liu, Bryan Kian Hsiang Low", "Summary": "We introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-of-the-art performance on BIG-bench arithmetic sub-task. In particular, the zero-shot Goat-7B matches or even surpasses the accuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achieve near-perfect accuracy on large-number addition and subtraction through supervised fine-tuning only, which is almost impossible with previous pretrained language models, such as Bloom, OPT, GPT-NeoX, etc. We attribute Goat's exceptional performance to LLaMA's consistent tokenization of numbers. To tackle more challenging tasks like large-number multiplication and division, we propose an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks, such as multi-digit multiplication and division, into a series of learnable tasks by leveraging basic arithmetic principles. We thoroughly examine the performance of our model, offering a comprehensive evaluation of the effectiveness of our proposed decomposition steps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAM GPU, facilitating reproducibility for other researchers. We release our model, dataset, and the Python script for dataset generation.", "main_contribution": {"headline": "Goat: A fine-tuned LLaMA model excelling in arithmetic tasks", "description": "The paper introduces Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Goat is trained on a synthetically generated dataset and achieves state-of-the-art performance on BIG-bench arithmetic sub-task. The model can achieve near-perfect accuracy on large-number addition and subtraction through supervised fine-tuning only, a feat almost impossible with previous pretrained language models. For more challenging tasks like large-number multiplication and division, Goat employs a novel approach that classifies tasks based on their learnability and decomposes unlearnable tasks into a series of learnable tasks by leveraging basic arithmetic principles."}, "takeaways": {"headline": "Goat offers a new benchmark in arithmetic tasks for LLMs", "description": "Goat's exceptional performance in arithmetic tasks, particularly in large-number addition and subtraction, demonstrates the potential of fine-tuning LLMs for specific tasks. The approach of classifying tasks based on their learnability and decomposing unlearnable tasks into learnable ones can be applied to other complex tasks beyond arithmetic. The fact that Goat-7B can be easily trained using LoRA on a 24GB VRAM GPU also facilitates reproducibility for other researchers.", "example": "For instance, to perform a large-number multiplication task, Goat would decompose the task into a series of smaller, learnable tasks. For example, for the multiplication 397 x 4429, Goat would break it down into 4429 x 300 + 4429 x 90 + 4429 x 7, and then perform each multiplication separately."}, "category": "FINE-TUNING", "novelty_analysis": "The paper presents a novel approach to fine-tuning LLMs for arithmetic tasks, particularly in the way it decomposes complex tasks into a series of simpler tasks. This approach, combined with the use of a synthetically generated dataset for training, represents a significant advancement in the field.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, detailing the fine-tuning process and the approach to decomposing complex tasks. However, it does not delve into complex mathematical theories or algorithms, making it accessible to a wider audience.", "technical_score": 2, "enjoyable_analysis": "The paper is well-written and organized, presenting a novel and intriguing contribution to the field of LLMs. The clear explanation of the approach and the detailed evaluation of the model's performance make it an enjoyable read.", "enjoyable_score": 3}