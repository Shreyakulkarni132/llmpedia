{"Published": "2023-07-28", "Title": "Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding", "Authors": "Xuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang, Yu Wang", "Summary": "This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the thinking and writing process of humans, we propose \"Skeleton-of-Thought\" (SoT), which guides LLMs to first generate the skeleton of the answer, and then conducts parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. Not only does SoT provide considerable speed-up (up to 2.39x across 11 different LLMs), but it can also potentially improve the answer quality on several question categories in terms of diversity and relevance. SoT is an initial attempt at data-centric optimization for efficiency, and reveal the potential of pushing LLMs to think more like a human for answer quality.", "main_contribution": {"headline": "Skeleton-of-Thought (SoT) method accelerates LLMs by parallelizing decoding", "description": "The paper introduces the Skeleton-of-Thought (SoT) method, a novel approach to decrease the end-to-end generation latency of Large Language Models (LLMs). SoT is inspired by the human thinking and writing process and guides LLMs to first generate a skeleton of the answer, then conducts parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. This method provides a significant speed-up across different LLMs and can potentially improve the answer quality in terms of diversity and relevance."}, "takeaways": {"headline": "SoT method offers a new way to speed up LLMs and improve answer quality", "description": "The Skeleton-of-Thought (SoT) method can be a game-changer for LLM practitioners, as it offers a way to significantly reduce the generation latency of LLMs. By generating a skeleton of the answer first and then filling in the details in parallel, the SoT method not only speeds up the process but also potentially improves the quality of the answers. This method can be particularly useful in applications where speed and quality of responses are critical.", "example": "For instance, in a chatbot application, the SoT method can be used to generate responses faster and with better quality. The LLM first generates a skeleton of the response, such as '1. Explain concept, 2. Provide examples, 3. Summarize'. Then, each of these points is expanded in parallel, resulting in a faster and potentially better response."}, "category": "TRAINING", "novelty_analysis": "The Skeleton-of-Thought (SoT) method is a novel approach to reducing the generation latency of LLMs. It challenges the common assumption that LLMs have to generate tokens sequentially, and instead proposes a parallel approach inspired by the human thinking and writing process.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical as it introduces a new method for LLMs and discusses its implementation in detail. However, it does not delve into complex mathematical theories or algorithms, making it accessible to readers with a basic understanding of LLMs.", "technical_score": 2, "enjoyable_analysis": "The paper is well-written and presents an interesting and novel approach to improving the performance of LLMs. The comparison with the human thinking and writing process makes the concept easy to understand and engaging for the reader.", "enjoyable_score": 3}