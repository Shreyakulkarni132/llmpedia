{"Published": "2023-06-21", "Title": "Deep Language Networks: Joint Prompt Training of Stacked LLMs using Variational Inference", "Authors": "Alessandro Sordoni, Xingdi Yuan, Marc-Alexandre C\u00f4t\u00e9, Matheus Pereira, Adam Trischler, Ziang Xiao, Arian Hosseini, Friederike Niedtner, Nicolas Le Roux", "Summary": "We view large language models (LLMs) as stochastic \\emph{language layers} in a network, where the learnable parameters are the natural language \\emph{prompts} at each layer. We stack two such layers, feeding the output of one layer to the next. We call the stacked architecture a \\emph{Deep Language Network} (DLN). We first show how to effectively perform prompt optimization for a 1-Layer language network (DLN-1). We then show how to train 2-layer DLNs (DLN-2), where two prompts must be learnt. We consider the output of the first layer as a latent variable to marginalize, and devise a variational inference algorithm for joint prompt training. A DLN-2 reaches higher performance than a single layer, sometimes comparable to few-shot GPT-4 even when each LLM in the network is smaller and less powerful. The DLN code is open source: https://github.com/microsoft/deep-language-networks .", "main_contribution": {"headline": "Deep Language Networks: A novel approach to stack LLMs using variational inference", "description": "This paper introduces Deep Language Networks (DLNs), a novel approach to stacking Large Language Models (LLMs) as layers in a network. The learnable parameters in each layer are the natural language prompts. The authors demonstrate how to perform prompt optimization for a 1-Layer language network (DLN-1) and then extend this to a 2-layer DLN (DLN-2), where two prompts must be learnt. The output of the first layer is considered as a latent variable to marginalize, and a variational inference algorithm is devised for joint prompt training. The DLN-2 outperforms a single layer and sometimes matches the performance of few-shot GPT-4, even when each LLM in the network is smaller and less powerful."}, "takeaways": {"headline": "DLNs offer a new way to leverage smaller LLMs for high performance", "description": "The Deep Language Network approach provides a new way to leverage the power of smaller LLMs by stacking them in a network and optimizing the prompts at each layer. This could potentially improve the efficiency and performance of LLMs in various applications, even when the individual models are less powerful. The open-source code also provides a practical resource for LLM practitioners to experiment with and apply these techniques in their own projects.", "example": "For instance, an LLM practitioner could use the DLN approach to stack several smaller LLMs for a complex task. Each layer could be optimized to handle a specific sub-task, with the output of one layer serving as the input to the next. This could potentially improve the overall performance and efficiency of the system."}, "category": "ARCHITECTURES", "novelty_analysis": "The concept of stacking LLMs as layers in a network and optimizing the prompts at each layer is a novel approach. The use of variational inference for joint prompt training also adds a unique element to this work. However, the idea of stacking models and optimizing parameters at each layer is not entirely new in the field of machine learning.", "novelty_score": 2, "technical_analysis": "The paper is quite technical, discussing the details of the DLN architecture, the process of prompt optimization, and the use of variational inference for joint prompt training. It requires a good understanding of LLMs, variational inference, and machine learning concepts.", "technical_score": 3, "enjoyable_analysis": "The paper presents an interesting and novel approach to leveraging the power of LLMs. The technical details are well explained, and the potential implications of the work are exciting. However, the high level of technical detail might make it a challenging read for those without a strong background in the field.", "enjoyable_score": 2}