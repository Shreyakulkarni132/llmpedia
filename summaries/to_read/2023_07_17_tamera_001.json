{"Published": "2023-07-17", "Title": "Measuring Faithfulness in Chain-of-Thought Reasoning", "Authors": "Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamil\u0117 Luko\u0161i\u016bt\u0117, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam McCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared Kaplan, Jan Brauner, Samuel R. Bowman, Ethan Perez", "Summary": "Large language models (LLMs) perform better when they produce step-by-step, \"Chain-of-Thought\" (CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it. CoT's performance boost does not seem to come from CoT's added test-time compute alone or from information encoded via the particular phrasing of the CoT. As models become larger and more capable, they produce less faithful reasoning on most tasks we study. Overall, our results suggest that CoT can be faithful if the circumstances such as the model size and task are carefully chosen.", "main_contribution": {"headline": "Investigation of Faithfulness in Chain-of-Thought Reasoning in LLMs", "description": "The paper investigates the faithfulness of Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs). The authors propose tests to measure CoT faithfulness by intervening on the CoT, such as adding mistakes or paraphrasing it, and observing how the model predictions change. The study reveals that models show large variation across tasks in how strongly they condition on the CoT when predicting their answer. The paper also finds that as models become larger and more capable, they produce less faithful reasoning on most tasks studied."}, "takeaways": {"headline": "Understanding CoT Faithfulness Can Improve LLMs' Reasoning", "description": "The paper's findings suggest that understanding and measuring the faithfulness of CoT reasoning can help improve the reasoning capabilities of LLMs. By intervening on the CoT, such as adding mistakes or paraphrasing it, practitioners can observe how the model predictions change and adjust the model accordingly. However, as models become larger and more capable, they tend to produce less faithful reasoning, indicating that careful consideration of model size and task is necessary when applying CoT reasoning.", "example": "For instance, if an LLM is tasked with answering a complex question, a practitioner can use CoT reasoning and then intervene on the CoT by adding a mistake. If the model's prediction changes significantly, it indicates that the model heavily relies on the CoT and the practitioner may need to adjust the model to improve its reasoning capabilities."}, "category": "BEHAVIOR", "novelty_analysis": "The paper's approach to investigating the faithfulness of CoT reasoning in LLMs by intervening on the CoT is a novel contribution. However, the finding that larger and more capable models produce less faithful reasoning aligns with previous research, making the overall novelty of the paper incremental.", "novelty_score": 2, "technical_analysis": "The paper is somewhat technical as it delves into the details of how to measure CoT faithfulness by intervening on the CoT. However, it does not involve complex mathematical theories or algorithms, making it accessible to those with a basic understanding of LLMs.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting investigation into the faithfulness of CoT reasoning in LLMs. The proposed tests for measuring CoT faithfulness and the findings of the study make the paper an engaging read for those interested in the behavior of LLMs.", "enjoyable_score": 2}