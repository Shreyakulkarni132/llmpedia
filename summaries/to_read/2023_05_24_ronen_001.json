{"Published": "2023-05-24", "Title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?", "Authors": "Ronen Eldan, Yuanzhi Li", "Summary": "Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention).   In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities.   We also introduce a new paradigm for the evaluation of language models: We suggest a framework which uses GPT-4 to grade the content generated by these models as if those were stories written by students and graded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often requires the model's output to be very structures, and moreover provides a multidimensional score for the model, providing scores for different capabilities such as grammar, creativity and consistency.   We hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-resource or specialized domains, and shed light on the emergence of language capabilities in LMs.", "main_contribution": {"headline": "TinyStories: Training and Evaluating Small Language Models with a Synthetic Dataset", "description": "The paper introduces TinyStories, a synthetic dataset of short stories generated by GPT-3.5 and GPT-4, designed to train and evaluate smaller language models (LMs) with less than 10 million parameters. The authors demonstrate that these smaller LMs, when trained on TinyStories, can generate fluent and consistent English text, demonstrating reasoning capabilities. The paper also introduces a new evaluation paradigm that uses GPT-4 to grade the content generated by these models, providing a multidimensional score for different capabilities such as grammar, creativity, and instruction-following."}, "takeaways": {"headline": "TinyStories Facilitates Training and Evaluation of Smaller LMs", "description": "TinyStories provides a new avenue for training and evaluating smaller LMs, which can be beneficial for low-resource or specialized domains. The introduced evaluation paradigm can provide a more comprehensive understanding of a model's capabilities. For instance, an LLM practitioner can use TinyStories to train a small LM for a specific task, and then use the GPT-4 grading system to evaluate its performance in terms of grammar, creativity, and instruction-following.", "example": "For example, an LLM practitioner could train a small LM on the TinyStories dataset for a task like generating children's stories. The practitioner could then use the GPT-4 grading system to evaluate the model's performance, providing a comprehensive understanding of its capabilities."}, "category": "TRAINING", "novelty_analysis": "The introduction of TinyStories and the new evaluation paradigm represent a novel approach to training and evaluating smaller LMs. The use of GPT-4 for grading the generated content is a unique contribution, providing a more comprehensive evaluation of the model's capabilities.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, discussing the creation of the TinyStories dataset and the new evaluation paradigm. However, it does not delve into complex mathematical theories or algorithms, making it accessible to a wider audience.", "technical_score": 2, "enjoyable_analysis": "The paper is well-written and presents an interesting approach to training and evaluating smaller LMs. The introduction of the TinyStories dataset and the new evaluation paradigm provide fresh insights into the field of LLMs.", "enjoyable_score": 3}