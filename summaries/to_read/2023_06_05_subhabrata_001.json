{"Published": "2023-06-05", "Title": "Orca: Progressive Learning from Complex Explanation Traces of GPT-4", "Authors": "Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awadallah", "Summary": "Recent research has focused on enhancing the capability of smaller models through imitation learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from limited imitation signals from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in overestimating the small model's capability as they tend to learn to imitate the style, but not the reasoning process of LFMs. To address these challenges, we develop Orca (We are working with our legal team to publicly release a diff of the model weights in accordance with LLaMA's release policy to be published at https://aka.ms/orca-lm), a 13-billion parameter model that learns to imitate the reasoning process of LFMs. Orca learns from rich signals from GPT-4 including explanation traces; step-by-step thought processes; and other complex instructions, guided by teacher assistance from ChatGPT. To promote this progressive learning, we tap into large-scale and diverse imitation data with judicious sampling and selection. Orca surpasses conventional state-of-the-art instruction-tuned models such as Vicuna-13B by more than 100% in complex zero-shot reasoning benchmarks like Big-Bench Hard (BBH) and 42% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH benchmark and shows competitive performance (4 pts gap with optimized system message) in professional and academic examinations like the SAT, LSAT, GRE, and GMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our research indicates that learning from step-by-step explanations, whether these are generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills.", "main_contribution": {"headline": "Orca: A model that learns to imitate the reasoning process of LFMs", "description": "The paper introduces Orca, a 13-billion parameter model that learns to imitate the reasoning process of Large Foundation Models (LFMs) rather than just their style. Orca learns from rich signals from GPT-4, including explanation traces, step-by-step thought processes, and other complex instructions, guided by teacher assistance from ChatGPT. The model surpasses conventional state-of-the-art instruction-tuned models in complex zero-shot reasoning benchmarks and shows competitive performance in professional and academic examinations."}, "takeaways": {"headline": "Orca's imitation of LFM reasoning process improves model capabilities", "description": "Orca's approach of learning from step-by-step explanations, whether generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills. This approach can be used to enhance the performance of smaller models, making them more efficient and effective in tasks that require complex reasoning. The model's competitive performance in professional and academic examinations suggests potential applications in educational technology.", "example": "For instance, an LLM practitioner could use Orca's approach to train a model for a tutoring application. The model could provide step-by-step explanations for complex problems, imitating the reasoning process of a human tutor."}, "category": "FINE-TUNING", "novelty_analysis": "The paper presents a novel approach to imitation learning, focusing on the imitation of the reasoning process of LFMs rather than just their style. This approach, demonstrated through the development of Orca, represents a significant advancement in the field of imitation learning.", "novelty_score": 3, "technical_analysis": "The paper is highly technical, detailing the development and training of Orca, a complex model that learns to imitate the reasoning process of LFMs. It requires a deep understanding of machine learning concepts and techniques.", "technical_score": 3, "enjoyable_analysis": "The paper is well-structured and presents an interesting and novel approach to imitation learning. However, its highly technical nature may make it a challenging read for those without a strong background in machine learning.", "enjoyable_score": 2}