{"Published": "2023-06-01", "Title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only", "Authors": "Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, Julien Launay", "Summary": "Large language models are commonly trained on a mixture of filtered web data and curated high-quality corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.", "main_contribution": {"headline": "RefinedWeb: A Large-Scale Dataset for Training LLMs Using Filtered and Deduplicated Web Data", "description": "The paper introduces RefinedWeb, a large-scale dataset for training large language models (LLMs) using filtered and deduplicated web data. The authors challenge the common belief that a mixture of web data and curated high-quality corpora is necessary for training performant LLMs. They demonstrate that properly filtered and deduplicated web data alone can lead to powerful models, even outperforming models trained on curated corpora like The Pile. The RefinedWeb dataset, extracted from CommonCrawl, contains five trillion tokens, with a public release of 600 billion tokens and language models trained on it."}, "takeaways": {"headline": "RefinedWeb offers a scalable solution for training LLMs using web data", "description": "The RefinedWeb dataset provides a scalable solution for training LLMs as it leverages web data, which is abundant and constantly updated. This approach can be particularly useful when training larger models that require pretraining on trillions of tokens, where curated high-quality data might be limited. The authors' method of extensive filtering and deduplication ensures the quality of the data used for training. The demonstrated performance of models trained on RefinedWeb suggests that this approach can be a viable alternative to using curated corpora.", "example": "For instance, to train a new LLM, instead of curating a high-quality dataset, one could use the RefinedWeb dataset. The model can be pretrained on this dataset, which has been extensively filtered and deduplicated to ensure quality."}, "category": "TRAINING", "novelty_analysis": "The paper presents a novel approach to training LLMs using only web data, challenging the common belief in the necessity of curated high-quality corpora. The introduction of the RefinedWeb dataset and the demonstrated performance of models trained on it represent a significant contribution to the field.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, discussing the process of filtering and deduplicating web data to create the RefinedWeb dataset. However, it does not delve into complex mathematical theories or algorithms, making it accessible to readers with a basic understanding of LLMs and dataset creation.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting challenge to common beliefs about training LLMs. The introduction of the RefinedWeb dataset and the demonstration of its effectiveness make for an engaging read.", "enjoyable_score": 2}