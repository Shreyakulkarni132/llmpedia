{"Published": "2023-07-25", "Title": "Retentive Network: A Successor to Transformer for Large Language Models", "Authors": "Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei", "Summary": "In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.", "main_contribution": {"headline": "Retentive Network (RetNet): A New Architecture for Large Language Models", "description": "The paper introduces Retentive Network (RetNet), a new architecture for large language models that achieves training parallelism, low-cost inference, and good performance. RetNet is based on a retention mechanism for sequence modeling, which supports three computation paradigms: parallel, recurrent, and chunkwise recurrent. The parallel representation enables training parallelism, the recurrent representation allows for low-cost O(1) inference, and the chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity. RetNet is presented as a strong successor to the Transformer architecture for large language models."}, "takeaways": {"headline": "RetNet Offers Efficient Training, Deployment, and Inference for Large Language Models", "description": "RetNet's architecture offers a promising solution for practitioners working with large language models. Its ability to achieve training parallelism, low-cost inference, and good performance simultaneously can significantly improve the efficiency of model training and deployment. The retention mechanism, with its three computation paradigms, provides flexibility in handling different sequence modeling tasks. Practitioners can leverage RetNet to build more efficient and scalable language models.", "example": "For instance, when training a large language model, one can use RetNet's parallel representation to achieve training parallelism. During inference, the recurrent representation can be used to achieve low-cost O(1) inference. For long-sequence modeling tasks, the chunkwise recurrent representation can be used to efficiently handle long sequences with linear complexity."}, "category": "ARCHITECTURES", "novelty_analysis": "The introduction of RetNet as a new architecture for large language models that achieves training parallelism, low-cost inference, and good performance simultaneously is a significant contribution. The retention mechanism and its three computation paradigms offer a novel approach to sequence modeling in large language models.", "novelty_score": 3, "technical_analysis": "The paper is highly technical, delving into the details of the RetNet architecture, the retention mechanism, and the three computation paradigms. It requires a solid understanding of large language models, sequence modeling, and related concepts.", "technical_score": 3, "enjoyable_analysis": "The paper is well-structured and presents a novel and significant contribution to the field of large language models. However, its highly technical content may make it a challenging read for those without a strong background in the field.", "enjoyable_score": 2}