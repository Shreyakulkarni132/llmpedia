{"Published": "2023-05-23", "Title": "QLoRA: Efficient Finetuning of Quantized LLMs", "Authors": "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer", "Summary": "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.", "main_contribution": {"headline": "QLORA: An efficient finetuning approach for quantized LLMs", "description": "The paper introduces QLoRA, a novel finetuning approach that enables finetuning of a 65B parameter model on a single 48GB GPU while maintaining full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). The paper also introduces several innovations to save memory without sacrificing performance, including a new data type, 4-bit NormalFloat (NF4), double quantization to reduce the average memory footprint, and paged optimizers to manage memory spikes."}, "takeaways": {"headline": "QLORA enables efficient finetuning of large language models", "description": "QLORA provides a practical solution for finetuning large language models on a single GPU, making it more accessible for practitioners with limited resources. The introduced innovations, such as 4-bit NormalFloat (NF4), double quantization, and paged optimizers, offer practical ways to manage memory usage without sacrificing performance. The paper also provides a detailed analysis of chatbot performance, suggesting that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation.", "example": "For instance, using QLoRA, a practitioner could finetune a 65B parameter model on a single 48GB GPU. The model is first quantized to 4-bit, then a small set of learnable Low-rank Adapter weights are added and tuned by backpropagating gradients through the quantized weights."}, "category": "FINE-TUNING", "novelty_analysis": "The paper presents a novel approach to finetuning large language models, introducing several innovations to manage memory usage. The ability to finetune a 65B parameter model on a single 48GB GPU is a significant advancement in the field.", "novelty_score": 3, "technical_analysis": "The paper is highly technical, delving into the details of the QLoRA approach, the new data type, and the methods used to manage memory usage. It requires a strong understanding of machine learning and language models.", "technical_score": 3, "enjoyable_analysis": "The paper is well-structured and provides a detailed analysis of the proposed approach and its performance. However, the high level of technical detail may make it challenging for some readers.", "enjoyable_score": 2}