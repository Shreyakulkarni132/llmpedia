{"Published": "2023-07-12", "Title": "PolyLM: An Open Source Polyglot Large Language Model", "Authors": "Xiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang Ren, Mei Li, Yu Wan, Zhiwei Cao, Binbin Xie, Tianxiang Hu, Shangjie Li, Binyuan Hui, Bowen Yu, Dayiheng Liu, Baosong Yang, Fei Huang, Jun Xie", "Summary": "Large language models (LLMs) demonstrate remarkable ability to comprehend, reason, and generate following nature language instructions. However, the development of LLMs has been primarily focused on high-resource languages, such as English, thereby limiting their applicability and research in other languages. Consequently, we present PolyLM, a multilingual LLM trained on 640 billion (B) tokens, avaliable in two model sizes: 1.7B and 13B. To enhance its multilingual capabilities, we 1) integrate bilingual data into training data; and 2) adopt a curriculum learning strategy that increases the proportion of non-English data from 30% in the first stage to 60% in the final stage during pre-training. Further, we propose a multilingual self-instruct method which automatically generates 132.7K diverse multilingual instructions for model fine-tuning. To assess the model's performance, we collect several existing multilingual tasks, including multilingual understanding, question answering, generation, and translation. Extensive experiments show that PolyLM surpasses other open-source models such as LLaMA and BLOOM on multilingual tasks while maintaining comparable performance in English. Our models, alone with the instruction data and multilingual benchmark, are available at: \\url{https://modelscope.cn/models/damo/nlp_polylm_13b_text_generation}.", "main_contribution": {"headline": "PolyLM: A Multilingual Large Language Model with Enhanced Training Strategy", "description": "The paper introduces PolyLM, a multilingual Large Language Model (LLM) trained on a massive dataset of 640 billion tokens. PolyLM is available in two model sizes: 1.7B and 13B. To enhance its multilingual capabilities, the authors integrate bilingual data into the training data and adopt a curriculum learning strategy that gradually increases the proportion of non-English data during pre-training. Additionally, they propose a multilingual self-instruct method that automatically generates diverse multilingual instructions for model fine-tuning. PolyLM outperforms other open-source models on multilingual tasks while maintaining comparable performance in English."}, "takeaways": {"headline": "PolyLM Offers Enhanced Multilingual Capabilities for LLMs", "description": "PolyLM's multilingual capabilities, achieved through the integration of bilingual data and a curriculum learning strategy, can be leveraged to improve the performance of LLMs on non-English tasks. The multilingual self-instruct method for generating diverse instructions for model fine-tuning can also be adopted to enhance model performance. The techniques used in PolyLM can be applied to other LLMs to improve their multilingual capabilities and performance on multilingual tasks.", "example": "For instance, to train a multilingual LLM, one could follow PolyLM's approach: 1. Integrate bilingual data into the training data. 2. Gradually increase the proportion of non-English data during pre-training. 3. Use a multilingual self-instruct method to generate diverse instructions for model fine-tuning."}, "category": "TRAINING", "novelty_analysis": "PolyLM's approach to enhancing multilingual capabilities in LLMs, particularly the integration of bilingual data and the curriculum learning strategy, is a novel contribution to the field. The multilingual self-instruct method for generating diverse instructions for model fine-tuning is also a unique addition.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, detailing the training strategy and the multilingual self-instruct method used in PolyLM. However, it does not delve into complex mathematical theories or algorithms, making it accessible to readers with a basic understanding of LLMs.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting approach to enhancing multilingual capabilities in LLMs. The practical implications of the techniques used in PolyLM make the paper an engaging read for those interested in multilingual LLMs.", "enjoyable_score": 2}