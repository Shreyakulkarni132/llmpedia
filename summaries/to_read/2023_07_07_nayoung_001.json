{"Published": "2023-07-07", "Title": "Teaching Arithmetic to Small Transformers", "Authors": "Nayoung Lee, Kartik Sreenivasan, Jason D. Lee, Kangwook Lee, Dimitris Papailiopoulos", "Summary": "Large language models like GPT-4 exhibit emergent capabilities across general-purpose tasks, such as basic arithmetic, when trained on extensive text data, even though these tasks are not explicitly encoded by the unsupervised, next-token prediction objective. This study investigates how small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the next-token prediction objective. We first demonstrate that conventional training data is not the most effective for arithmetic learning, and simple formatting changes can significantly improve accuracy. This leads to sharp phase transitions as a function of training data scale, which, in some cases, can be explained through connections to low-rank matrix completion. Building on prior work, we then train on chain-of-thought style data that includes intermediate step results. Even in the complete absence of pretraining, this approach significantly and simultaneously improves accuracy, sample complexity, and convergence speed. We also study the interplay between arithmetic and text data during training and examine the effects of few-shot prompting, pretraining, and model scale. Additionally, we discuss length generalization challenges. Our work highlights the importance of high-quality, instructive data that considers the particular characteristics of the next-word prediction objective for rapidly eliciting arithmetic capabilities.", "main_contribution": {"headline": "Efficiently Teaching Arithmetic to Small Transformers", "description": "The paper explores how small transformers can be trained to efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the next-token prediction objective. The authors demonstrate that conventional training data is not the most effective for arithmetic learning and that simple formatting changes can significantly improve accuracy. They also introduce a novel training approach using chain-of-thought style data that includes intermediate step results, which significantly improves accuracy, sample complexity, and convergence speed, even without pretraining."}, "takeaways": {"headline": "High-quality, instructive data and chain-of-thought training can enhance arithmetic learning in LLMs", "description": "The findings of this paper can be used to improve the efficiency of arithmetic learning in LLMs. By using high-quality, instructive data and chain-of-thought style training, practitioners can significantly enhance the accuracy, sample complexity, and convergence speed of their models. This approach can be particularly useful in applications where LLMs are required to perform arithmetic operations, such as in financial analysis or scientific computation.", "example": "For instance, to train an LLM to perform addition, instead of using conventional training data, one could use chain-of-thought style data that includes intermediate step results. This could look like: '1 + 1 = 2 -> 2 + 1 = 3 -> 3 + 1 = 4', and so on. This approach could significantly improve the model's ability to learn and perform arithmetic operations."}, "category": "TRAINING", "novelty_analysis": "The paper presents a novel approach to teaching arithmetic to small transformers, demonstrating that simple formatting changes and chain-of-thought style training can significantly improve learning efficiency. This represents a significant advancement in the field of LLM training.", "novelty_score": 3, "technical_analysis": "The paper is highly technical, delving into the intricacies of training small transformers to perform arithmetic operations. It requires a deep understanding of LLMs, next-token prediction objectives, and the mathematics behind arithmetic operations.", "technical_score": 3, "enjoyable_analysis": "The paper is well-structured and presents an interesting and novel approach to a common problem in LLM training. However, the high level of technical detail may make it a challenging read for those without a strong background in the field.", "enjoyable_score": 2}