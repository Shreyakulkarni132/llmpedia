{"Published": "2023-07-13", "Title": "In-context Autoencoder for Context Compression in a Large Language Model", "Authors": "Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, Furu Wei", "Summary": "We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM). The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes. We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses. Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts. The promising results demonstrate significant implications of the ICAE for its novel approach to the long context problem and its potential to reduce computation and memory overheads for LLM inference in practice, suggesting further research effort in context management for an LLM. Our code and data will be released shortly.", "main_contribution": {"headline": "In-context Autoencoder (ICAE) for context compression in Large Language Models", "description": "The paper introduces the In-context Autoencoder (ICAE), a novel approach to context compression in Large Language Models (LLMs). The ICAE consists of two modules: a learnable encoder adapted from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes. The ICAE is pretrained using both autoencoding and language modeling objectives on massive text data, and then fine-tuned on a small amount of instruct data. The ICAE can effectively produce memory slots with 4\u00d7 context compression, which can be well conditioned on by the target LLM to respond to various prompts."}, "takeaways": {"headline": "ICAE offers a novel approach to context management in LLMs", "description": "The In-context Autoencoder (ICAE) provides a new way to handle long context problems in LLMs, offering potential to reduce computation and memory overheads during inference. By compressing the context into memory slots, the ICAE allows the LLM to condition on these slots to generate responses, thereby enhancing the LLM's interaction with various prompts. This approach could be particularly useful in applications where the context is long and complex, such as document summarization or question answering.", "example": "For instance, an LLM using ICAE could take a long document as input, compress it into memory slots using the learnable encoder, and then use the fixed decoder to generate a summary based on these memory slots."}, "category": "ARCHITECTURES", "novelty_analysis": "The paper presents a novel approach to the long context problem in LLMs, introducing the concept of context compression using an In-context Autoencoder (ICAE). This approach differs significantly from previous research which has primarily focused on architectural innovations of the LLM itself.", "novelty_score": 3, "technical_analysis": "The paper is highly technical, detailing the workings of the In-context Autoencoder (ICAE), its pretraining and fine-tuning processes, and how it interacts with the target LLM. It requires a solid understanding of LLMs and autoencoders to fully comprehend.", "technical_score": 3, "enjoyable_analysis": "The paper is well-structured and presents a novel and intriguing solution to the long context problem in LLMs. However, the high level of technical detail may make it a challenging read for those without a strong background in the field.", "enjoyable_score": 2}