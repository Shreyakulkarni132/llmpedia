{"Published": "2023-05-12", "Title": "GPT-NER: Named Entity Recognition via Large Language Models", "Authors": "Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei Wu, Tianwei Zhang, Jiwei Li, Guoyin Wang", "Summary": "Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model.   In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text \"Columbus is a city\" is transformed to generate the text sequence \"@@Columbus## is a city\", where special tokens @@## marks the entity to extract. To efficiently address the \"hallucination\" issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag.   We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.", "main_contribution": {"headline": "GPT-NER: A novel approach to Named Entity Recognition using Large Language Models", "description": "The paper introduces GPT-NER, a novel approach to Named Entity Recognition (NER) using Large Language Models (LLMs). The authors address the gap between NER, a sequence labeling task, and LLMs, which are text-generation models, by transforming the NER task into a text-generation task. This is achieved by marking entities in the input text with special tokens, transforming the task of finding entities into generating a text sequence with marked entities. To address the 'hallucination' issue of LLMs, where they over-confidently label NULL inputs as entities, a self-verification strategy is proposed, prompting the LLM to verify whether the extracted entities belong to a labeled entity tag."}, "takeaways": {"headline": "GPT-NER offers a new approach to NER tasks, especially in low-resource setups", "description": "GPT-NER provides a new way to approach NER tasks using LLMs, transforming the sequence labeling task into a text-generation task. This approach is particularly beneficial in low-resource and few-shot setups, where training data is scarce. The self-verification strategy proposed to address the 'hallucination' issue of LLMs can also be useful in other applications where over-confident labeling is a concern.", "example": "For instance, in a low-resource setup where training data for NER is limited, GPT-NER can be used to perform the task. The input text 'Columbus is a city' is transformed to '@@Columbus## is a city', marking the entity. The LLM is then prompted to verify whether 'Columbus' belongs to a labeled entity tag, addressing the 'hallucination' issue."}, "category": "FINE-TUNING", "novelty_analysis": "The paper presents a novel approach to NER using LLMs, addressing the gap between the two tasks and proposing a self-verification strategy to address the 'hallucination' issue of LLMs. This represents a significant advancement in the field of NER.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, discussing the transformation of the NER task into a text-generation task and the self-verification strategy to address the 'hallucination' issue of LLMs. However, the concepts are explained clearly and can be understood by someone with a background in AI and machine learning.", "technical_score": 2, "enjoyable_analysis": "The paper is well-written and presents a novel approach to a common problem in NER. The introduction of GPT-NER and the self-verification strategy provides interesting insights into the potential of LLMs in NER tasks.", "enjoyable_score": 2}