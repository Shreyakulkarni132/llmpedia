{"Published": "2023-03-12", "Title": "Learning ASR pathways: A sparse multilingual ASR model", "Authors": "Mu Yang, Andros Tjandra, Chunxi Liu, David Zhang, Duc Le, Ozlem Kalinli", "Summary": "Neural network pruning compresses automatic speech recognition (ASR) models effectively. However, in multilingual ASR, language-agnostic pruning may lead to severe performance drops on some languages because language-agnostic pruning masks may not fit all languages and discard important language-specific parameters. In this work, we present ASR pathways, a sparse multilingual ASR model that activates language-specific sub-networks (\"pathways\"), such that the parameters for each language are learned explicitly. With the overlapping sub-networks, the shared parameters can also enable knowledge transfer for lower-resource languages via joint multilingual training. We propose a novel algorithm to learn ASR pathways, and evaluate the proposed method on 4 languages with a streaming RNN-T model. Our proposed ASR pathways outperform both dense models and a language-agnostically pruned model, and provide better performance on low-resource languages compared to the monolingual sparse models.", "main_contribution": {"headline": "ASR Pathways: A Novel Sparse Multilingual ASR Model", "description": "The paper introduces ASR pathways, a novel sparse multilingual automatic speech recognition (ASR) model that activates language-specific sub-networks or 'pathways'. This approach allows the model to learn language-specific parameters explicitly, overcoming the limitations of language-agnostic pruning which can lead to performance drops for certain languages. The model also leverages shared parameters across overlapping sub-networks to enable knowledge transfer for lower-resource languages. The authors propose a two-step training scheme to learn these ASR pathways and demonstrate its effectiveness on four languages using a streaming RNN-T model."}, "takeaways": {"headline": "ASR Pathways Enhance Multilingual ASR Model Performance", "description": "The ASR pathways approach can significantly improve the performance of multilingual ASR models, particularly for low-resource languages. By activating language-specific sub-networks, the model can learn language-specific parameters, avoiding the pitfalls of language-agnostic pruning. This approach could be particularly useful for LLM practitioners working on multilingual applications, as it provides a more efficient and effective way to handle multiple languages within a single model.", "example": "For instance, an LLM practitioner could use the ASR pathways approach to develop a multilingual voice assistant. The assistant would activate the appropriate language-specific sub-network based on the user's language, ensuring optimal performance across all supported languages."}, "category": "ARCHITECTURES", "novelty_analysis": "The paper presents a novel approach to multilingual ASR model training, introducing the concept of ASR pathways. This represents a significant advancement in the field, addressing the limitations of previous language-agnostic pruning methods and improving performance for low-resource languages.", "novelty_score": 3, "technical_analysis": "The paper is highly technical, detailing a novel algorithm for learning ASR pathways and discussing the implementation and evaluation of this approach on a streaming RNN-T model. It requires a solid understanding of ASR models, neural network pruning, and multilingual training.", "technical_score": 3, "enjoyable_analysis": "The paper is well-structured and presents a novel and intriguing contribution to the field of multilingual ASR models. However, the high level of technical detail may make it challenging for readers without a strong background in the subject.", "enjoyable_score": 2}