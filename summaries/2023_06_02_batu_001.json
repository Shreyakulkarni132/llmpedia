{"Published": "2023-06-02", "Title": "ThinkSum: Probabilistic reasoning over sets using large language models", "Authors": "Batu Ozturkler, Nikolay Malkin, Zhen Wang, Nebojsa Jojic", "Summary": "Large language models (LLMs) have a substantial capacity for high-level analogical reasoning: reproducing patterns in linear text that occur in their training data (zero-shot evaluation) or in the provided context (few-shot in-context learning). However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions. We propose a two-stage probabilistic inference paradigm, ThinkSum, which reasons over sets of objects or facts in a structured manner. In the first stage (Think - retrieval of associations), a LLM is queried in parallel over a set of phrases extracted from the prompt or an auxiliary model call. In the second stage (Sum - probabilistic inference or reasoning), the results of these queries are aggregated to make the final prediction. We demonstrate the possibilities and advantages of ThinkSum on the BIG-bench suite of LLM evaluation tasks, achieving improvements over the state of the art using GPT-family models on thirteen difficult tasks, often with far smaller model variants. We also compare and contrast ThinkSum with other proposed modifications to direct prompting of LLMs, such as variants of chain-of-thought prompting. Our results suggest that because the probabilistic inference in ThinkSum is performed outside of calls to the LLM, ThinkSum is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs. Overall, our proposed paradigm represents a promising approach for enhancing the reasoning capabilities of LLMs.", "main_contribution": {"headline": "ThinkSum: A two-stage probabilistic inference paradigm for enhancing LLM reasoning capabilities", "description": "The paper introduces ThinkSum, a novel two-stage probabilistic inference paradigm designed to improve the reasoning capabilities of Large Language Models (LLMs). The first stage, 'Think', involves querying an LLM in parallel over a set of phrases extracted from the prompt or an auxiliary model call. The second stage, 'Sum', aggregates the results of these queries to make the final prediction. This structured approach to reasoning over sets of objects or facts addresses the limitations of LLMs in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions. The authors demonstrate the effectiveness of ThinkSum on the BIG-bench suite of LLM evaluation tasks, achieving improvements over the state of the art using GPT-family models on thirteen difficult tasks."}, "takeaways": {"headline": "ThinkSum offers a structured approach to enhance LLM reasoning capabilities", "description": "The ThinkSum paradigm provides a promising approach to enhance the reasoning capabilities of LLMs. By structuring the reasoning process into two stages - 'Think' and 'Sum' - it allows LLMs to handle complex reasoning tasks more effectively. This approach could be particularly useful in applications that require reasoning over multiple objects or facts and making sequences of logical deductions. Furthermore, ThinkSum is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs.", "example": "For instance, in a task requiring reasoning over multiple facts, ThinkSum would first query the LLM with each fact separately (Think stage), then aggregate the results of these queries to make the final prediction (Sum stage). This structured approach could lead to more accurate and interpretable predictions."}, "category": "PROMPTING", "novelty_analysis": "The introduction of ThinkSum represents a significant advancement in the field of LLM prompting. The two-stage probabilistic inference paradigm is a novel approach to enhancing the reasoning capabilities of LLMs, addressing a key limitation of existing LLMs.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, detailing the workings of the ThinkSum paradigm and its application to the BIG-bench suite of LLM evaluation tasks. However, the concepts are explained clearly and should be accessible to readers with a background in AI and machine learning.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents a novel and intriguing contribution to the field of LLM prompting. The clear explanation of the ThinkSum paradigm and its potential benefits make the paper an interesting read.", "enjoyable_score": 3}