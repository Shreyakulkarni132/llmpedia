{"Published": "2023-06-20", "Title": "A Simple and Effective Pruning Approach for Large Language Models", "Authors": "Mingjie Sun, Zhuang Liu, Anna Bair, J. Zico Kolter", "Summary": "As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prune weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method on LLaMA across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and competes favorably against recent methods involving intensive weight update. Code is available at https://github.com/locuslab/wanda.", "main_contribution": {"headline": "Wanda: A Simple and Effective Pruning Approach for Large Language Models", "description": "The paper introduces a novel pruning method for Large Language Models (LLMs) called Wanda (Pruning by Weights and activations). Unlike existing methods that require retraining or computationally expensive weight reconstruction, Wanda prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. This approach is inspired by the recent observation of emergent large magnitude features in LLMs. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. The authors conducted a thorough evaluation of Wanda on LLaMA across various language benchmarks, where it significantly outperformed the established baseline of magnitude pruning and competed favorably against recent methods involving intensive weight update."}, "takeaways": {"headline": "Wanda: A Computationally Efficient Pruning Method for LLMs", "description": "Wanda presents a computationally efficient way to prune LLMs without the need for retraining or weight updates. This makes it a practical choice for applications where computational resources are limited. The method can be executed in a single forward pass and requires minimal memory overhead. Furthermore, Wanda's performance is robust and can be easily estimated using a modest number of calibration samples. This makes it a promising approach for inducing sparsity in pretrained LLMs and could potentially lead to more efficient deployment of these models in real-world applications.", "example": "Consider a pretrained LLM that needs to be pruned for deployment on a resource-constrained device. Using Wanda, we can prune the model in a single forward pass without the need for retraining or weight updates. The pruned model can then be deployed as is, saving significant computational resources."}, "category": "TRAINING", "novelty_analysis": "The paper introduces a novel pruning method for LLMs that is both simple and effective. While pruning is not a new concept, the authors' approach of incorporating input activations into the pruning metric and comparing weights on a per-output basis is unique. This approach addresses the limitations of existing pruning methods and offers a more efficient way to induce sparsity in pretrained LLMs.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, delving into the specifics of the proposed pruning method, Wanda. It discusses the computation of the pruning metric and the importance of pruning granularity. However, the authors also provide clear explanations and visual illustrations to aid understanding. The paper is accessible to readers with a basic understanding of neural networks and LLMs.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents a clear narrative, making it an enjoyable read. The authors' approach to addressing the challenges of pruning LLMs is intriguing, and the results of their evaluation provide valuable insights into the effectiveness of their method. The inclusion of a comparison with existing methods also adds to the paper's appeal.", "enjoyable_score": 2}