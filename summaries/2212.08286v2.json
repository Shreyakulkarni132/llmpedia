{"Published": "2023-07-07", "Title": "ALERT: Adapting Language Models to Reasoning Tasks", "Authors": "Ping Yu, Tianlu Wang, Olga Golovneva, Badr Alkhamissy, Gargi Ghosh, Mona Diab, Asli Celikyilmaz", "Summary": "Current large language models can perform reasonably well on complex tasks that require step-by-step reasoning with few-shot learning. Are these models applying reasoning skills they have learnt during pre-training and reason outside of their training context, or are they simply memorizing their training corpus at finer granularity and have learnt to better understand their context? To tease apart these possibilities, we introduce ALERT, a benchmark and suite of analyses for assessing language models' reasoning ability comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. ALERT provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. We leverage ALERT to further investigate the role of finetuning. With extensive empirical analysis we find that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during finetuning stage compared to pretraining state. We also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.", "main_contribution": {"headline": "ALERT: A Benchmark for Evaluating Reasoning Skills of Language Models", "description": "The paper introduces ALERT, a benchmark and suite of analyses designed to evaluate the reasoning skills of language models. ALERT allows for the comparison of pre-trained and fine-tuned models on complex tasks that require reasoning skills. It provides a test bed for assessing any language model on fine-grained reasoning skills, spanning over 20 datasets and covering 10 different reasoning skills. The authors use ALERT to investigate the role of fine-tuning in language models, revealing that models acquire more reasoning skills during the fine-tuning stage compared to the pre-training stage."}, "takeaways": {"headline": "ALERT Provides Insight into Reasoning Skills Acquired During Fine-tuning", "description": "ALERT is a valuable tool for LLM practitioners as it provides a comprehensive benchmark for evaluating the reasoning skills of language models. It can be used to compare the performance of pre-trained and fine-tuned models on complex tasks, providing insights into the reasoning skills acquired during the fine-tuning process. However, the authors also found that fine-tuned models tend to overfit to the prompt template, which can negatively impact the model's robustness and generalization capabilities.", "example": "For instance, an LLM practitioner could use ALERT to evaluate a model's reasoning skills before and after fine-tuning. This could help identify the specific reasoning skills that are improved during fine-tuning and highlight potential overfitting issues."}, "category": "FINE-TUNING", "novelty_analysis": "The introduction of ALERT as a benchmark for evaluating the reasoning skills of language models is a novel contribution. It provides a comprehensive test bed for assessing fine-grained reasoning skills, which is a significant advancement in the field.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical as it introduces a new benchmark and discusses the role of fine-tuning in language models. However, it does not delve into complex mathematical theories or algorithms, making it accessible to readers with a basic understanding of language models and fine-tuning.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents a novel contribution in the form of ALERT. It provides valuable insights into the role of fine-tuning in language models, making it an interesting read for those interested in the development and evaluation of language models.", "enjoyable_score": 2}