{"Published": "2023-04-26", "Title": "GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models", "Authors": "Archiki Prasad, Peter Hase, Xiang Zhou, Mohit Bansal", "Summary": "Providing natural language instructions in prompts is a useful new paradigm for improving task performance of large language models in a zero-shot setting. Recent work has aimed to improve such prompts via manual rewriting or gradient-based tuning. However, manual rewriting is time-consuming and requires subjective interpretation, while gradient-based tuning can be extremely computationally demanding for large models and may not be feasible for API-based models. In this work, we introduce Gradient-free Instructional Prompt Search (GrIPS), a gradient-free, edit-based search approach for improving task instructions for large language models. GrIPS takes in instructions designed for humans and automatically returns an improved, edited prompt, while allowing for API-based tuning. With InstructGPT models, GrIPS improves the average task performance by up to 4.30 percentage points on eight classification tasks from the Natural Instructions dataset (with similar improvements for OPT, BLOOM, and FLAN-T5). We see improvements for both instruction-only prompts and instruction + k-shot examples prompts. Notably, GrIPS outperforms manual rewriting and purely example-based prompts while controlling for the available compute and data budget. Further, performance of GrIPS is comparable to select gradient-based tuning approaches. Qualitatively, we show our edits can simplify instructions and at times make them incoherent but nonetheless improve accuracy. Our code is available at: https://github.com/archiki/GrIPS", "main_contribution": "The paper introduces a new method called Gradient-free Instructional Prompt Search (GrIPS) for improving task instructions for large language models. GrIPS is a gradient-free, edit-based search approach that takes in instructions designed for humans and automatically returns an improved, edited prompt. This method allows for API-based tuning, which is beneficial for models available only via APIs where model gradients and weights are not standardly accessible. The authors demonstrate that GrIPS can improve the average task performance by up to 4.30 percentage points on eight classification tasks from the Natural Instructions dataset.", "takeaways": "GrIPS offers a more efficient and less computationally demanding alternative to gradient-based tuning methods for improving task instructions for large language models. It outperforms manual rewriting and purely example-based prompts while controlling for the available compute and data budget. The method can be applied to a variety of models, including InstructGPT, OPT, BLOOM, and FLAN-T5, and can improve both instruction-only prompts and instruction + k-shot examples prompts.", "novelty_analysis": "The introduction of GrIPS represents a significant advancement in the field of prompting for large language models. Unlike previous methods, GrIPS is gradient-free and allows for API-based tuning, making it more efficient and less computationally demanding. The method's ability to improve task performance by editing human-designed instructions is a novel approach in the field.", "novelty_score": 3, "category": "PROMPTING", "technical_analysis": "The paper is somewhat technical, introducing a new method for improving task instructions for large language models and discussing its implementation and performance in detail. However, the authors provide clear explanations and examples, making the content accessible to readers with a background in computer science.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents a novel and interesting contribution to the field of prompting for large language models. The authors' clear explanations and examples make the content engaging and easy to follow.", "enjoyable_score": 3}