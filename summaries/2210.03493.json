{"Published": "2022-10-07", "Title": "Automatic Chain of Thought Prompting in Large Language Models", "Authors": "Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola", "Summary": "Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like \"Let's think step by step\" to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the \"Let's think step by step\" prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot", "main_contribution": {"headline": "Auto-CoT: An automatic chain-of-thought prompting method for LLMs", "description": "The paper introduces Auto-CoT, an automatic chain-of-thought (CoT) prompting method for large language models (LLMs). Auto-CoT leverages the 'Let's think step by step' prompt to generate reasoning chains for demonstrations one by one, thereby eliminating the need for manual crafting of task-specific demonstrations. The authors also highlight the importance of diversity in question sampling for constructing demonstrations. Auto-CoT has been shown to match or exceed the performance of manual CoT prompting on ten public benchmark reasoning tasks with GPT-3."}, "takeaways": {"headline": "Auto-CoT brings automation and diversity to CoT prompting, improving LLMs reasoning", "description": "Auto-CoT represents a significant step towards automating the process of CoT prompting, thereby streamlining the process of reasoning for LLMs. By leveraging diversity in question sampling, this method provides effective reasoning chains for consistent performance. Its demonstrated superiority on complex tasks brings efficiency and potential performance improvements to AI and machine learning practitioners.", "example": "For instance, an LLM utilizing Auto-CoT, when fed a complex reasoning task, would generate a series of thought-sequence instead of a direct prediction. This sequence presents a more comprehensive reasoning chain, aiding in more accurate task completion."}, "category": "PROMPTING", "novelty_analysis": "The introduction of Auto-CoT, an automatic method for generating CoT prompts, advances the field by reducing the reliance on human-crafted prompts, marking a significant stride in the prompting techniques for LLMs.", "novelty_score": 3, "technical_analysis": "The paper delves into the intricate details of the Auto-CoT method, its implementation, and how it leverages diversity in question sampling, rendering it a highly technical read that might appeal to experts in the field.", "technical_score": 3, "enjoyable_analysis": "The discovery of Auto-CoT and its potential are interesting, but the complexity of the technical content can pose a challenge for readers without a strong background in machine learning and statistical algorithms.", "enjoyable_score": 2}