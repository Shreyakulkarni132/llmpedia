{"Published": "2022-03-20", "Title": "How does the pre-training objective affect what large language models learn about linguistic properties?", "Authors": "Ahmed Alajrami, Nikolaos Aletras", "Summary": "Several pre-training objectives, such as masked language modeling (MLM), have been proposed to pre-train language models (e.g. BERT) with the aim of learning better language representations. However, to the best of our knowledge, no previous work so far has investigated how different pre-training objectives affect what BERT learns about linguistics properties. We hypothesize that linguistically motivated objectives such as MLM should help BERT to acquire better linguistic knowledge compared to other non-linguistically motivated objectives that are not intuitive or hard for humans to guess the association between the input and the label to be predicted. To this end, we pre-train BERT with two linguistically motivated objectives and three non-linguistically motivated ones. We then probe for linguistic characteristics encoded in the representation of the resulting models. We find strong evidence that there are only small differences in probing performance between the representations learned by the two different types of objectives. These surprising results question the dominant narrative of linguistically informed pre-training.", "main_contribution": {"headline": "Investigation of the Impact of Pre-training Objectives on Linguistic Knowledge Acquisition in BERT", "description": "The paper investigates how different pre-training objectives affect the linguistic knowledge acquired by BERT. The authors hypothesize that linguistically motivated objectives, such as Masked Language Modeling (MLM), should help BERT acquire better linguistic knowledge compared to non-linguistically motivated objectives. To test this, they pre-train BERT with two linguistically motivated objectives and three non-linguistically motivated ones, and then probe for linguistic characteristics encoded in the resulting models. Contrary to their hypothesis, they find only small differences in probing performance between the representations learned by the two different types of objectives."}, "takeaways": {"headline": "Choice of Pre-training Objective May Not Significantly Impact Linguistic Knowledge Acquisition", "description": "The paper's findings suggest that the choice of pre-training objective may not significantly impact the linguistic knowledge that BERT acquires. This could potentially simplify the process of pre-training BERT, as it indicates that even non-linguistically motivated objectives can result in similar linguistic knowledge acquisition. However, it's important to note that this conclusion is based on the specific objectives and probing tasks used in this study, and may not generalize to all possible objectives and tasks.", "example": "For instance, if you're pre-training BERT for a specific task, you might choose to use a non-linguistically motivated objective that's easier to implement or more closely aligned with your task, rather than a linguistically motivated objective like MLM. According to this paper's findings, this choice should not significantly impact the linguistic knowledge that BERT acquires."}, "category": "TRAINING", "novelty_analysis": "The paper presents a novel investigation into the impact of different pre-training objectives on the linguistic knowledge acquired by BERT. While previous work has explored how well and to what extent BERT learns linguistic information, this is the first study to specifically examine the role of the pre-training objective in this process.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, as it involves pre-training BERT with different objectives and probing the resulting models for linguistic characteristics. However, it does not delve into complex mathematical theories or algorithms, and should be reasonably accessible to someone with a background in machine learning.", "technical_score": 2, "enjoyable_analysis": "The paper is well-written and presents an interesting investigation into a previously unexplored aspect of BERT pre-training. However, the surprising results and their implications for the field make it a particularly engaging read.", "enjoyable_score": 3}