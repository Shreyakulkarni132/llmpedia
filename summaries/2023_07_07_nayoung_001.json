{"Published": "2023-07-07", "Title": "Teaching Arithmetic to Small Transformers", "Authors": "Nayoung Lee, Kartik Sreenivasan, Jason D. Lee, Kangwook Lee, Dimitris Papailiopoulos", "Summary": "Large language models like GPT-4 exhibit emergent capabilities across general-purpose tasks, such as basic arithmetic, when trained on extensive text data, even though these tasks are not explicitly encoded by the unsupervised, next-token prediction objective. This study investigates how small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the next-token prediction objective. We first demonstrate that conventional training data is not the most effective for arithmetic learning, and simple formatting changes can significantly improve accuracy. This leads to sharp phase transitions as a function of training data scale, which, in some cases, can be explained through connections to low-rank matrix completion. Building on prior work, we then train on chain-of-thought style data that includes intermediate step results. Even in the complete absence of pretraining, this approach significantly and simultaneously improves accuracy, sample complexity, and convergence speed. We also study the interplay between arithmetic and text data during training and examine the effects of few-shot prompting, pretraining, and model scale. Additionally, we discuss length generalization challenges. Our work highlights the importance of high-quality, instructive data that considers the particular characteristics of the next-word prediction objective for rapidly eliciting arithmetic capabilities.", "main_contribution": {"headline": "Teaching Arithmetic to Small Transformers with Chain-of-Thought Data", "description": "The paper investigates how small transformers, trained from random initialization, can efficiently learn arithmetic operations using the next-token prediction objective. The authors demonstrate that conventional training data is not the most effective for arithmetic learning, and simple formatting changes can significantly improve accuracy. They also introduce the concept of chain-of-thought (CoT) data during training, which includes step-by-step operations and intermediate results, leading to significant improvements in accuracy, sample complexity, and convergence speed. The study also explores the interplay between arithmetic and text data during training and examines the effects of few-shot prompting, pretraining, and model scale."}, "takeaways": {"headline": "High-Quality, Instructive Data Elicits Arithmetic Capabilities in LLMs", "description": "The findings of this paper can be applied to improve the efficiency of LLMs in learning arithmetic operations. By using chain-of-thought data during training, the model can learn the individual components of complex tasks, significantly improving learning in terms of both sample complexity and accuracy. This approach can be beneficial in applications where LLMs are required to perform arithmetic operations. The study also highlights the importance of high-quality, instructive data that considers the particular characteristics of the next-word prediction objective for rapidly eliciting arithmetic capabilities.", "example": "For example, if you're training an LLM to perform arithmetic operations, you could use chain-of-thought data during training. This could look like: 'Input: 128+367, Target: A->5 , C->1 A->9 , C->0 A->4 , C->0. 495'. This approach breaks down the required compositional function to be learned into individual components, allowing the model to learn a higher-dimensional but easier-to-learn function map."}, "category": "TRAINING", "novelty_analysis": "The paper presents a novel approach to teaching arithmetic to small transformers using chain-of-thought data during training. This approach significantly improves learning in terms of both sample complexity and accuracy, even in the absence of language pretraining. The study also provides theoretical justifications for some of the phenomena observed, which is a unique contribution to the field.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, discussing the specifics of training small transformers to perform arithmetic operations. It delves into the details of the chain-of-thought data format and the effects of few-shot prompting, pretraining, and model scale. However, the concepts are explained clearly, making it accessible to readers with a basic understanding of machine learning and language models.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and provides a clear narrative of the research process and findings. The use of visual aids and examples makes it easier to understand the concepts and results. The discussion on the interplay between arithmetic and text data during training and the effects of few-shot prompting, pretraining, and model scale is particularly engaging.", "enjoyable_score": 2}