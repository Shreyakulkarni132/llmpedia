{"Published": "2023-08-14", "Title": "The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation", "Authors": "Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, Andr\u00e9 F. T. Martins, Graham Neubig, Ankush Garg, Jonathan H. Clark, Markus Freitag, Orhan Firat", "Summary": "Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.", "main_contribution": {"headline": "AutoMQM: Leveraging LLMs for Fine-grained Machine Translation Evaluation", "description": "The paper introduces AutoMQM, a prompting technique that utilizes large language models (LLMs) to identify and categorize errors in machine translations. This technique is based on the Multidimensional Quality Metrics (MQM) framework, which provides detailed feedback by identifying error spans, categorizing them, and evaluating their severity. The authors evaluate the performance of recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting and study the impact of labeled data through in-context learning and finetuning. The results show that AutoMQM improves performance compared to just prompting for scores, particularly for larger models, while providing interpretability through error spans that align with human annotations."}, "takeaways": {"headline": "Fine-grained Evaluation of Machine Translation with LLMs", "description": "The AutoMQM technique offers a new approach to machine translation evaluation, providing detailed feedback on translation errors. This can be particularly useful for developers and researchers working on improving machine translation systems, as it allows them to identify specific areas of weakness in their models. The technique also demonstrates the potential of LLMs in providing AI feedback, which could be applied to other areas of natural language processing. For example, developers could use a similar approach to evaluate and improve the performance of text generation or summarization models.", "example": "Consider a machine translation system that translates text from English to Spanish. Using AutoMQM, developers could prompt an LLM to identify and categorize errors in the translated text. For instance, the LLM might identify a word that has been mistranslated, categorize the error as a 'mistranslation', and assign it a severity level. This detailed feedback can then be used to improve the translation system."}, "category": "USE CASES", "novelty_analysis": "The introduction of AutoMQM represents a novel approach to machine translation evaluation. While previous work has used LLMs for score prediction in machine translation evaluation, this is the first study to leverage LLMs for fine-grained evaluation based on the MQM framework. The technique provides detailed feedback on translation errors, offering a level of interpretability not available with traditional score prediction methods.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, as it involves the use of LLMs for machine translation evaluation and introduces a new prompting technique based on the MQM framework. However, the authors provide a clear explanation of the AutoMQM technique and its implementation, making the paper accessible to readers with a basic understanding of LLMs and machine translation.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents a novel approach to machine translation evaluation, making it an interesting read for researchers and practitioners in the field. The results are clearly presented and the authors provide a thorough discussion of the implications of their findings, enhancing the paper's readability and appeal.", "enjoyable_score": 2}