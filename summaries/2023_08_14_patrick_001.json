{"Published": "2023-08-14", "Title": "The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation", "Authors": "Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, Andr\u00e9 F. T. Martins, Graham Neubig, Ankush Garg, Jonathan H. Clark, Markus Freitag, Orhan Firat", "Summary": "Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.", "main_contribution": {"headline": "AutoMQM: Leveraging LLMs for Fine-grained Machine Translation Evaluation", "description": "The paper introduces AutoMQM, a novel technique for fine-grained evaluation of machine translation (MT) systems. AutoMQM leverages the reasoning and in-context learning capabilities of large language models (LLMs) to identify and categorize errors in translations. The technique is evaluated using recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting. The authors also study the impact of labeled data through in-context learning and fine-tuning. The results show that AutoMQM improves performance compared to just prompting for scores, especially for larger models, while providing interpretability through error spans that align with human annotations."}, "takeaways": {"headline": "AutoMQM Enhances Machine Translation Evaluation and Interpretability", "description": "AutoMQM offers a new approach to machine translation evaluation, providing more detailed and interpretable feedback than traditional scalar quality scores. By identifying and categorizing translation errors, AutoMQM can help developers better understand and improve their MT systems. The technique is particularly effective when used with larger LLMs, suggesting that it could be a valuable tool for developers working with these models. Furthermore, the use of in-context learning and fine-tuning with labeled data can further enhance the performance of AutoMQM.", "example": "For instance, when evaluating a translation from Portuguese to English, AutoMQM could be prompted to identify errors like this: 'Portuguese: {source}; English:{candidate} Score: 25 Errors: \u2018easy\u2019 - major/accuracy; \u2018are\u2019 - minor/\ufb02uency'. This detailed feedback can help developers pinpoint specific areas for improvement in their MT systems."}, "category": "USE CASES", "novelty_analysis": "The introduction of AutoMQM represents a significant advancement in the field of machine translation evaluation. While previous methods have relied on scalar quality scores, AutoMQM provides a more detailed and interpretable evaluation by identifying and categorizing translation errors. This approach leverages the capabilities of LLMs in a novel way and could have a significant impact on the development of MT systems.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, as it delves into the specifics of how AutoMQM leverages LLMs to identify and categorize translation errors. However, the authors do a good job of explaining these concepts in a clear and accessible way, making the paper understandable for readers with a basic knowledge of LLMs and machine translation.", "technical_score": 2, "enjoyable_analysis": "The paper is well-written and organized, presenting a novel and intriguing contribution to the field of machine translation evaluation. The authors' clear explanation of their methodology and their thorough evaluation of AutoMQM make the paper an engaging and informative read.", "enjoyable_score": 3}