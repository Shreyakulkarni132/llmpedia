{"Published": "2023-06-12", "Title": "Augmenting Language Models with Long-Term Memory", "Authors": "Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, Furu Wei", "Summary": "Existing large language models (LLMs) can only afford fix-sized inputs due to the input length limit, preventing them from utilizing rich long-context information from past inputs. To address this, we propose a framework, Language Models Augmented with Long-Term Memory (LongMem), which enables LLMs to memorize long history. We design a novel decoupled network architecture with the original backbone LLM frozen as a memory encoder and an adaptive residual side-network as a memory retriever and reader. Such a decoupled memory design can easily cache and update long-term past contexts for memory retrieval without suffering from memory staleness. Enhanced with memory-augmented adaptation training, LongMem can thus memorize long past context and use long-term memory for language modeling. The proposed memory retrieval module can handle unlimited-length context in its memory bank to benefit various downstream tasks. Typically, LongMem can enlarge the long-form memory to 65k tokens and thus cache many-shot extra demonstration examples as long-form memory for in-context learning. Experiments show that our method outperforms strong long-context models on ChapterBreak, a challenging long-context modeling benchmark, and achieves remarkable improvements on memory-augmented in-context learning over LLMs. The results demonstrate that the proposed method is effective in helping language models to memorize and utilize long-form contents. Our code is open-sourced at https://aka.ms/LongMem.", "main_contribution": {"headline": "LongMem: A novel framework for augmenting LLMs with long-term memory", "description": "The paper introduces a new framework, Language Models Augmented with Long-Term Memory (LongMem), designed to overcome the input length limit of existing Large Language Models (LLMs). LongMem employs a decoupled network architecture, with the original LLM serving as a memory encoder and a trainable residual side-network acting as a memory retriever and reader. This design allows for easy caching and updating of long-term past contexts, avoiding memory staleness. The framework can handle unlimited-length context in its memory bank, and can enlarge the long-form memory to 65k tokens, thus enabling the caching of many-shot extra demonstration examples for in-context learning."}, "takeaways": {"headline": "LongMem enhances LLMs' ability to utilize long-context information", "description": "The LongMem framework presents a significant advancement for practitioners working with LLMs, particularly in scenarios where processing long-form information is critical. By enabling LLMs to memorize and utilize long past context, LongMem can potentially improve the performance of LLMs on various downstream tasks. The framework's ability to handle unlimited-length context and cache many-shot extra demonstration examples also opens up new possibilities for in-context learning.", "example": "For instance, in a customer service chatbot application, LongMem could be used to enable the LLM to remember and utilize the entire conversation history, rather than just the most recent exchanges. This could lead to more accurate and contextually appropriate responses from the chatbot."}, "category": "ARCHITECTURES", "novelty_analysis": "The LongMem framework represents a significant advancement in the field of LLMs, addressing a key limitation of existing models - their inability to utilize long-context information. The novel decoupled network architecture and the concept of a memory bank capable of handling unlimited-length context are particularly innovative aspects of this work.", "novelty_score": 3, "technical_analysis": "The paper is highly technical, delving into the details of the LongMem framework, its decoupled network architecture, and the memory-augmented adaptation training process. It requires a solid understanding of LLMs and related concepts to fully comprehend.", "technical_score": 3, "enjoyable_analysis": "The paper is well-structured and presents a novel and intriguing contribution to the field of LLMs. However, the high level of technical detail may make it a challenging read for those without a strong background in the field.", "enjoyable_score": 2}