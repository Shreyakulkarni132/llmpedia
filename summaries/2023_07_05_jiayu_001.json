{"Published": "2023-07-05", "Title": "LongNet: Scaling Transformers to 1,000,000,000 Tokens", "Authors": "Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Furu Wei", "Summary": "Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. In this work, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between tokens; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. Experiments results demonstrate that LongNet yields strong performance on both long-sequence modeling and general language tasks. Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.", "main_contribution": "The paper introduces LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens. The key technique used is dilated attention, which expands the attentive field exponentially as the distance grows. This allows LongNet to handle extremely long sequences without sacrificing performance on shorter sequences. The dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with existing Transformer-based optimization.", "takeaways": "LongNet offers several advantages. It has a linear computation complexity and a logarithm dependency between tokens, making it computationally efficient. It can be used as a distributed trainer for extremely long sequences, making it suitable for large-scale tasks. The practical implications for LLM practitioners include the ability to model very long sequences, such as treating a whole corpus or even the entire Internet as a sequence.", "novelty_analysis": "The introduction of LongNet and the concept of dilated attention represent significant advancements in the field of LLMs. The ability to scale sequence length to more than 1 billion tokens without sacrificing performance on shorter sequences is a unique and meaningful contribution that extends the boundaries of prior work.", "novelty_score": 3, "category": "ARCHITECTURES", "technical_analysis": "The paper is highly technical, introducing a new Transformer variant and discussing its implementation using dilated attention. It requires a deep understanding of LLMs and Transformer-based optimization.", "technical_score": 3, "enjoyable_analysis": "The paper is well-written and presents a novel and interesting contribution. However, the highly technical content may be challenging for many readers, making it less enjoyable for those without a strong background in the field.", "enjoyable_score": 2}