{"Published": "2023-04-24", "Title": "How Many Demonstrations Do You Need for In-context Learning?", "Authors": "Jiuhai Chen, Lichang Chen, Chen Zhu, Tianyi Zhou", "Summary": "Large language models (LLMs) are capable to perform complex reasoning by in-context learning (ICL) when provided with a few input-output demonstrations (demos) and more powerful when intermediate reasoning steps (\"chain of thoughts (CoT)\") of the demos are given. Is it necessary to use multi-demo in ICL? In this paper, we study ICL using fewer demos for each test query on the tasks in~\\cite{wei2022chain}. Surprisingly, we do not observe significant degradation when using only one randomly chosen demo. To study this phenomenon, for each test query, we categorize demos into \"correct demos\" leading to the correct answer, and \"wrong demos\" resulting in wrong answers. Our analysis reveals an inherent bias in those widely studied datasets: most demos are correct for a majority of test queries, which explains the good performance of using one random demo. Moreover, ICL (with and w/o CoT) using only one correct demo significantly outperforms all-demo ICL adopted by most previous works, indicating the weakness of LLMs in finding correct demo(s) for input queries, which is difficult to evaluate on the biased datasets. Furthermore, we observe a counterintuitive behavior of ICL using multi-demo, i.e., its accuracy degrades(improves) when given more correct(wrong) demos. This implies that ICL can be easily misguided by interference among demos and their spurious correlations. Our analyses highlight several fundamental challenges that need to be addressed in LLMs training, ICL, and benchmark design.", "main_contribution": {"headline": "In-context Learning (ICL) with fewer demonstrations performs comparably to multi-demo ICL", "description": "The paper investigates the effect of using fewer demonstrations (demos) in In-context Learning (ICL) for Large Language Models (LLMs). The authors categorize demos into 'positive demos' leading to the correct answer, and 'negative demos' resulting in wrong answers. They find that using only one randomly chosen demo does not significantly degrade the performance of ICL. Interestingly, ICL using only one positive demo outperforms multi-demo ICL, indicating the weakness of LLMs in finding positive demos for input queries. The paper also observes a counterintuitive behavior of ICL using multi-demo, where its accuracy degrades when given more positive demos and improves when given more negative demos."}, "takeaways": {"headline": "Fewer demonstrations can be as effective as multiple ones in In-context Learning", "description": "The findings suggest that using fewer demonstrations in In-context Learning (ICL) can be as effective as using multiple ones, which can potentially save computational resources and time. This can be particularly useful when dealing with large datasets or complex tasks. However, the paper also highlights the need for careful selection of demonstrations, as the performance of ICL can be easily misguided by interference among demos and their spurious correlations.", "example": "For instance, when using ICL for a task, instead of providing multiple demonstrations, one can randomly select a single 'positive demo' (one that leads to the correct answer). This approach can potentially yield comparable performance while saving computational resources."}, "category": "TRAINING", "novelty_analysis": "The paper presents a novel perspective on the use of demonstrations in In-context Learning (ICL) for Large Language Models (LLMs). The finding that fewer demonstrations can be as effective as multiple ones is a significant departure from the common practice in the field.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, as it delves into the details of In-context Learning (ICL) and the categorization of demonstrations. However, it does not involve complex mathematical theories or algorithms, making it accessible to readers with a basic understanding of machine learning.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting and counterintuitive finding about the use of demonstrations in In-context Learning (ICL). The clear explanations and logical flow of arguments make it an enjoyable read.", "enjoyable_score": 3}