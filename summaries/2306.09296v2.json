{"Published": "2023-07-06", "Title": "KoLA: Carefully Benchmarking World Knowledge of Large Language Models", "Authors": "Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-Li, Xin Lv, Hao Peng, Zijun Yao, Xiaohan Zhang, Hanming Li, Chunyang Li, Zheyuan Zhang, Yushi Bai, Yantao Liu, Amy Xin, Nianyi Lin, Kaifeng Yun, Linlu Gong, Jianhui Chen, Zhili Wu, Yunjia Qi, Weikai Li, Yong Guan, Kaisheng Zeng, Ji Qi, Hailong Jin, Jinxin Liu, Yu Gu, Yuan Yao, Ning Ding, Lei Hou, Zhiyuan Liu, Bin Xu, Jie Tang, Juanzi Li", "Summary": "The unprecedented performance of large language models (LLMs) necessitates improvements in evaluations. Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations. Given the importance of world knowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark (KoLA), in which we carefully design three crucial factors: (1) For ability modeling, we mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering $19$ tasks. (2) For data, to ensure fair comparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs, along with continuously collected emerging corpora, aiming to evaluate the capacity to handle unseen data and evolving knowledge. (3) For evaluation criteria, we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models and a unique self-contrast metric for automatically evaluating knowledge hallucination. We evaluate $21$ open-source and commercial LLMs and obtain some intriguing findings. The KoLA dataset and open-participation leaderboard are publicly released at https://kola.xlore.cn and will be continuously updated to provide references for developing LLMs and knowledge-related systems.", "main_contribution": {"headline": "KoLA: A meticulously designed benchmark for evaluating world knowledge of LLMs", "description": "The paper introduces KoLA (Knowledge-oriented LLM Assessment), a benchmark designed to evaluate the world knowledge of Large Language Models (LLMs). The authors have carefully considered three key factors in its design: (1) Ability Modeling, where they mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering 19 tasks. (2) Data, where they use both Wikipedia and continuously collected emerging corpora to ensure fair comparisons and evaluate the capacity to handle unseen data and evolving knowledge. (3) Evaluation Criteria, where they adopt a contrastive system, including overall standard scores and a unique self-contrast metric for automatically evaluating knowledge hallucination. The benchmark was used to evaluate 21 open-source and commercial LLMs, yielding intriguing findings."}, "takeaways": {"headline": "KoLA provides a comprehensive framework for evaluating and improving LLMs", "description": "KoLA offers a robust and comprehensive framework for evaluating the world knowledge of LLMs. It provides a taxonomy of knowledge-related abilities, a diverse dataset, and a contrastive evaluation system, which can be used to assess and improve the performance of LLMs. The benchmark can be used to identify strengths and weaknesses of different LLMs, and guide future research and development efforts. The open-participation leaderboard and dataset are publicly available, encouraging continuous improvement and comparison of LLMs.", "example": "For instance, an LLM practitioner can use KoLA to evaluate their model's performance across a range of tasks and against other models. They can also use the benchmark's findings to identify areas of improvement and guide their model's development. For example, if their model scores low in the 'Understanding' ability, they might focus on improving the model's ability to understand entities, relations, concepts, and events."}, "category": "BEHAVIOR", "novelty_analysis": "The introduction of KoLA represents a significant contribution to the field of LLM evaluation. While there are existing benchmarks, KoLA's meticulous design, focus on world knowledge, and use of a contrastive evaluation system make it a unique and valuable tool for assessing LLMs.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, as it delves into the design and implementation of the KoLA benchmark. However, it does not involve complex mathematical theories or algorithms, making it accessible to readers with a basic understanding of LLMs and evaluation metrics.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents a novel and intriguing contribution to the field of LLM evaluation. It provides practical insights and interesting findings, making it an enjoyable read for those interested in LLMs and their evaluation.", "enjoyable_score": 3}