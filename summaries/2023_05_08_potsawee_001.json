{"Published": "2023-05-08", "Title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models", "Authors": "Potsawee Manakul, Adian Liusie, Mark J. F. Gales", "Summary": "Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose \"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if a LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that in sentence hallucination detection, our approach has AUC-PR scores comparable to or better than grey-box methods, while SelfCheckGPT is best at passage factuality assessment.", "main_contribution": {"headline": "SelfCheckGPT: A zero-resource black-box hallucination detection for LLMs", "description": "The paper introduces SelfCheckGPT, a novel, simple, and sampling-based approach for fact-checking black-box models without the need for an external database. The method is based on the idea that if a Large Language Model (LLM) has knowledge of a concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. The authors demonstrate that SelfCheckGPT can detect non-factual and factual sentences and rank passages in terms of factuality, showing comparable or better performance than existing methods."}, "takeaways": {"headline": "SelfCheckGPT offers a simple and effective approach for fact-checking LLMs", "description": "SelfCheckGPT provides a practical solution for LLM practitioners to detect hallucinated facts and non-factual statements generated by LLMs. This approach does not require access to the output probability distribution or external databases, making it a versatile tool for fact-checking in various applications. It can be particularly useful in applications where trust in the output of the LLM is crucial, such as in news generation, content creation, or information retrieval systems.", "example": "For instance, when using an LLM to generate a news article, SelfCheckGPT can be used to fact-check the generated content. By sampling multiple responses to the same prompt and comparing them, SelfCheckGPT can identify and flag potentially hallucinated or non-factual statements."}, "category": "BEHAVIOR", "novelty_analysis": "The paper presents a novel approach to fact-checking in LLMs that does not rely on output probability distribution or external databases. The concept of using stochastic sampling to detect hallucinated facts is a unique contribution to the field.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical as it introduces a new method for fact-checking in LLMs and provides a detailed analysis of its performance. However, the concepts are explained clearly and should be accessible to readers with a basic understanding of LLMs.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting solution to a significant problem in the use of LLMs. The clear explanation of the method and the detailed analysis of its performance make it an enjoyable read for those interested in the field.", "enjoyable_score": 2}