{"id": "http://arxiv.org/abs/2307.07164v1", "guidislink": true, "link": "http://arxiv.org/abs/2307.07164v1", "updated": "2023-07-14T05:23:08Z", "updated_parsed": [2023, 7, 14, 5, 23, 8, 4, 195, 0], "published": "2023-07-14T05:23:08Z", "published_parsed": [2023, 7, 14, 5, 23, 8, 4, 195, 0], "title": "Learning to Retrieve In-Context Examples for Large Language Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=learning+to+retrieve+in+context+examples+for+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Learning to Retrieve In-Context Examples for Large Language Models"}, "summary": "Large language models (LLMs) have demonstrated their ability to learn\nin-context, allowing them to perform various tasks based on a few input-output\nexamples. However, the effectiveness of in-context learning is heavily reliant\non the quality of the selected examples. In this paper, we propose a novel\nframework to iteratively train dense retrievers that can identify high-quality\nin-context examples for LLMs. Our framework initially trains a reward model\nbased on LLM feedback to evaluate the quality of candidate examples, followed\nby knowledge distillation to train a bi-encoder based dense retriever. Our\nexperiments on a suite of 30 tasks demonstrate that our framework significantly\nenhances in-context learning performance. Furthermore, we show the\ngeneralization ability of our framework to unseen tasks during training. An\nin-depth analysis reveals that our model improves performance by retrieving\nexamples with similar patterns, and the gains are consistent across LLMs of\nvarying sizes.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=learning+to+retrieve+in+context+examples+for+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Large language models (LLMs) have demonstrated their ability to learn\nin-context, allowing them to perform various tasks based on a few input-output\nexamples. However, the effectiveness of in-context learning is heavily reliant\non the quality of the selected examples. In this paper, we propose a novel\nframework to iteratively train dense retrievers that can identify high-quality\nin-context examples for LLMs. Our framework initially trains a reward model\nbased on LLM feedback to evaluate the quality of candidate examples, followed\nby knowledge distillation to train a bi-encoder based dense retriever. Our\nexperiments on a suite of 30 tasks demonstrate that our framework significantly\nenhances in-context learning performance. Furthermore, we show the\ngeneralization ability of our framework to unseen tasks during training. An\nin-depth analysis reveals that our model improves performance by retrieving\nexamples with similar patterns, and the gains are consistent across LLMs of\nvarying sizes."}, "authors": [{"name": "Liang Wang"}, {"name": "Nan Yang"}, {"name": "Furu Wei"}], "author_detail": {"name": "Furu Wei"}, "author": "Furu Wei", "arxiv_comment": "16 pages", "links": [{"href": "http://arxiv.org/abs/2307.07164v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.07164v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}