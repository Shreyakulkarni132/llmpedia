{"id": "http://arxiv.org/abs/2306.09896v3", "guidislink": true, "link": "http://arxiv.org/abs/2306.09896v3", "updated": "2023-06-22T17:55:21Z", "updated_parsed": [2023, 6, 22, 17, 55, 21, 3, 173, 0], "published": "2023-06-16T15:13:17Z", "published_parsed": [2023, 6, 16, 15, 13, 17, 4, 167, 0], "title": "Demystifying GPT Self-Repair for Code Generation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=demystifying+gpt+self+repair+for+code+generation&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Demystifying GPT Self-Repair for Code Generation"}, "summary": "Large Language Models (LLMs) have shown remarkable aptitude in code\ngeneration but still struggle on challenging programming tasks. Self-repair --\nin which the model debugs and fixes mistakes in its own code -- has recently\nbecome a popular way to boost performance in these settings. However, only very\nlimited studies on how and when self-repair works effectively exist in the\nliterature, and one might wonder to what extent a model is really capable of\nproviding accurate feedback on why the code is wrong when that code was\ngenerated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's\nability to perform self-repair on APPS, a challenging dataset consisting of\ndiverse coding challenges. To do so, we first establish a new evaluation\nstrategy dubbed pass@t that measures the pass rate of the tasks against the\ntotal number of tokens sampled from the model, enabling a fair comparison to\npurely sampling-based approaches. With this evaluation strategy, we find that\nthe effectiveness of self-repair is only seen in GPT-4. We also observe that\nself-repair is bottlenecked by the feedback stage; using GPT-4 to give feedback\non the programs generated by GPT-3.5 and using expert human programmers to give\nfeedback on the programs generated by GPT-4, we unlock significant performance\ngains.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=demystifying+gpt+self+repair+for+code+generation&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Large Language Models (LLMs) have shown remarkable aptitude in code\ngeneration but still struggle on challenging programming tasks. Self-repair --\nin which the model debugs and fixes mistakes in its own code -- has recently\nbecome a popular way to boost performance in these settings. However, only very\nlimited studies on how and when self-repair works effectively exist in the\nliterature, and one might wonder to what extent a model is really capable of\nproviding accurate feedback on why the code is wrong when that code was\ngenerated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's\nability to perform self-repair on APPS, a challenging dataset consisting of\ndiverse coding challenges. To do so, we first establish a new evaluation\nstrategy dubbed pass@t that measures the pass rate of the tasks against the\ntotal number of tokens sampled from the model, enabling a fair comparison to\npurely sampling-based approaches. With this evaluation strategy, we find that\nthe effectiveness of self-repair is only seen in GPT-4. We also observe that\nself-repair is bottlenecked by the feedback stage; using GPT-4 to give feedback\non the programs generated by GPT-3.5 and using expert human programmers to give\nfeedback on the programs generated by GPT-4, we unlock significant performance\ngains."}, "authors": [{"name": "Theo X. Olausson"}, {"name": "Jeevana Priya Inala"}, {"name": "Chenglong Wang"}, {"name": "Jianfeng Gao"}, {"name": "Armando Solar-Lezama"}], "author_detail": {"name": "Armando Solar-Lezama"}, "author": "Armando Solar-Lezama", "links": [{"href": "http://arxiv.org/abs/2306.09896v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2306.09896v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.PL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.SE", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}