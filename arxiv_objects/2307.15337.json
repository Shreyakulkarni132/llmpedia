{"id": "http://arxiv.org/abs/2307.15337v1", "guidislink": true, "link": "http://arxiv.org/abs/2307.15337v1", "updated": "2023-07-28T06:31:34Z", "updated_parsed": [2023, 7, 28, 6, 31, 34, 4, 209, 0], "published": "2023-07-28T06:31:34Z", "published_parsed": [2023, 7, 28, 6, 31, 34, 4, 209, 0], "title": "Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=skeleton+of+thought++large+language+models+can+do+parallel+decoding&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding"}, "summary": "This work aims at decreasing the end-to-end generation latency of large\nlanguage models (LLMs). One of the major causes of the high generation latency\nis the sequential decoding approach adopted by almost all state-of-the-art\nLLMs. In this work, motivated by the thinking and writing process of humans, we\npropose \"Skeleton-of-Thought\" (SoT), which guides LLMs to first generate the\nskeleton of the answer, and then conducts parallel API calls or batched\ndecoding to complete the contents of each skeleton point in parallel. Not only\ndoes SoT provide considerable speed-up (up to 2.39x across 11 different LLMs),\nbut it can also potentially improve the answer quality on several question\ncategories in terms of diversity and relevance. SoT is an initial attempt at\ndata-centric optimization for efficiency, and reveal the potential of pushing\nLLMs to think more like a human for answer quality.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=skeleton+of+thought++large+language+models+can+do+parallel+decoding&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "This work aims at decreasing the end-to-end generation latency of large\nlanguage models (LLMs). One of the major causes of the high generation latency\nis the sequential decoding approach adopted by almost all state-of-the-art\nLLMs. In this work, motivated by the thinking and writing process of humans, we\npropose \"Skeleton-of-Thought\" (SoT), which guides LLMs to first generate the\nskeleton of the answer, and then conducts parallel API calls or batched\ndecoding to complete the contents of each skeleton point in parallel. Not only\ndoes SoT provide considerable speed-up (up to 2.39x across 11 different LLMs),\nbut it can also potentially improve the answer quality on several question\ncategories in terms of diversity and relevance. SoT is an initial attempt at\ndata-centric optimization for efficiency, and reveal the potential of pushing\nLLMs to think more like a human for answer quality."}, "authors": [{"name": "Xuefei Ning"}, {"name": "Zinan Lin"}, {"name": "Zixuan Zhou"}, {"name": "Huazhong Yang"}, {"name": "Yu Wang"}], "author_detail": {"name": "Yu Wang"}, "author": "Yu Wang", "arxiv_comment": "Technical report, work in progress", "links": [{"href": "http://arxiv.org/abs/2307.15337v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.15337v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}