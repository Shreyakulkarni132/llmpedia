{"id": "http://arxiv.org/abs/2305.14201v1", "guidislink": true, "link": "http://arxiv.org/abs/2305.14201v1", "updated": "2023-05-23T16:20:30Z", "updated_parsed": [2023, 5, 23, 16, 20, 30, 1, 143, 0], "published": "2023-05-23T16:20:30Z", "published_parsed": [2023, 5, 23, 16, 20, 30, 1, 143, 0], "title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=goat++fine+tuned+llama+outperforms+gpt+4+on+arithmetic+tasks&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks"}, "summary": "We introduce Goat, a fine-tuned LLaMA model that significantly outperforms\nGPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated\ndataset, Goat achieves state-of-the-art performance on BIG-bench arithmetic\nsub-task. In particular, the zero-shot Goat-7B matches or even surpasses the\naccuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achieve\nnear-perfect accuracy on large-number addition and subtraction through\nsupervised fine-tuning only, which is almost impossible with previous\npretrained language models, such as Bloom, OPT, GPT-NeoX, etc. We attribute\nGoat's exceptional performance to LLaMA's consistent tokenization of numbers.\nTo tackle more challenging tasks like large-number multiplication and division,\nwe propose an approach that classifies tasks based on their learnability, and\nsubsequently decomposes unlearnable tasks, such as multi-digit multiplication\nand division, into a series of learnable tasks by leveraging basic arithmetic\nprinciples. We thoroughly examine the performance of our model, offering a\ncomprehensive evaluation of the effectiveness of our proposed decomposition\nsteps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAM\nGPU, facilitating reproducibility for other researchers. We release our model,\ndataset, and the Python script for dataset generation.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=goat++fine+tuned+llama+outperforms+gpt+4+on+arithmetic+tasks&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "We introduce Goat, a fine-tuned LLaMA model that significantly outperforms\nGPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated\ndataset, Goat achieves state-of-the-art performance on BIG-bench arithmetic\nsub-task. In particular, the zero-shot Goat-7B matches or even surpasses the\naccuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achieve\nnear-perfect accuracy on large-number addition and subtraction through\nsupervised fine-tuning only, which is almost impossible with previous\npretrained language models, such as Bloom, OPT, GPT-NeoX, etc. We attribute\nGoat's exceptional performance to LLaMA's consistent tokenization of numbers.\nTo tackle more challenging tasks like large-number multiplication and division,\nwe propose an approach that classifies tasks based on their learnability, and\nsubsequently decomposes unlearnable tasks, such as multi-digit multiplication\nand division, into a series of learnable tasks by leveraging basic arithmetic\nprinciples. We thoroughly examine the performance of our model, offering a\ncomprehensive evaluation of the effectiveness of our proposed decomposition\nsteps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAM\nGPU, facilitating reproducibility for other researchers. We release our model,\ndataset, and the Python script for dataset generation."}, "authors": [{"name": "Tiedong Liu"}, {"name": "Bryan Kian Hsiang Low"}], "author_detail": {"name": "Bryan Kian Hsiang Low"}, "author": "Bryan Kian Hsiang Low", "links": [{"href": "http://arxiv.org/abs/2305.14201v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2305.14201v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}