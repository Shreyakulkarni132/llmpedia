{"id": "http://arxiv.org/abs/2307.15189v1", "guidislink": true, "link": "http://arxiv.org/abs/2307.15189v1", "updated": "2023-07-27T20:36:02Z", "updated_parsed": [2023, 7, 27, 20, 36, 2, 3, 208, 0], "published": "2023-07-27T20:36:02Z", "published_parsed": [2023, 7, 27, 20, 36, 2, 3, 208, 0], "title": "Med-Flamingo: a Multimodal Medical Few-shot Learner", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=med+flamingo++a+multimodal+medical+few+shot+learner&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Med-Flamingo: a Multimodal Medical Few-shot Learner"}, "summary": "Medicine, by its nature, is a multifaceted domain that requires the synthesis\nof information across various modalities. Medical generative vision-language\nmodels (VLMs) make a first step in this direction and promise many exciting\nclinical applications. However, existing models typically have to be fine-tuned\non sizeable down-stream datasets, which poses a significant limitation as in\nmany medical applications data is scarce, necessitating models that are capable\nof learning from few examples in real-time. Here we propose Med-Flamingo, a\nmultimodal few-shot learner adapted to the medical domain. Based on\nOpenFlamingo-9B, we continue pre-training on paired and interleaved medical\nimage-text data from publications and textbooks. Med-Flamingo unlocks few-shot\ngenerative medical visual question answering (VQA) abilities, which we evaluate\non several datasets including a novel challenging open-ended VQA dataset of\nvisual USMLE-style problems. Furthermore, we conduct the first human evaluation\nfor generative medical VQA where physicians review the problems and blinded\ngenerations in an interactive app. Med-Flamingo improves performance in\ngenerative medical VQA by up to 20\\% in clinician's rating and firstly enables\nmultimodal medical few-shot adaptations, such as rationale generation. We\nrelease our model, code, and evaluation app under\nhttps://github.com/snap-stanford/med-flamingo.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=med+flamingo++a+multimodal+medical+few+shot+learner&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Medicine, by its nature, is a multifaceted domain that requires the synthesis\nof information across various modalities. Medical generative vision-language\nmodels (VLMs) make a first step in this direction and promise many exciting\nclinical applications. However, existing models typically have to be fine-tuned\non sizeable down-stream datasets, which poses a significant limitation as in\nmany medical applications data is scarce, necessitating models that are capable\nof learning from few examples in real-time. Here we propose Med-Flamingo, a\nmultimodal few-shot learner adapted to the medical domain. Based on\nOpenFlamingo-9B, we continue pre-training on paired and interleaved medical\nimage-text data from publications and textbooks. Med-Flamingo unlocks few-shot\ngenerative medical visual question answering (VQA) abilities, which we evaluate\non several datasets including a novel challenging open-ended VQA dataset of\nvisual USMLE-style problems. Furthermore, we conduct the first human evaluation\nfor generative medical VQA where physicians review the problems and blinded\ngenerations in an interactive app. Med-Flamingo improves performance in\ngenerative medical VQA by up to 20\\% in clinician's rating and firstly enables\nmultimodal medical few-shot adaptations, such as rationale generation. We\nrelease our model, code, and evaluation app under\nhttps://github.com/snap-stanford/med-flamingo."}, "authors": [{"name": "Michael Moor"}, {"name": "Qian Huang"}, {"name": "Shirley Wu"}, {"name": "Michihiro Yasunaga"}, {"name": "Cyril Zakka"}, {"name": "Yash Dalmia"}, {"name": "Eduardo Pontes Reis"}, {"name": "Pranav Rajpurkar"}, {"name": "Jure Leskovec"}], "author_detail": {"name": "Jure Leskovec"}, "author": "Jure Leskovec", "arxiv_comment": "Preprint", "links": [{"href": "http://arxiv.org/abs/2307.15189v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.15189v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}