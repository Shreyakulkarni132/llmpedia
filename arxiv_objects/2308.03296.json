{"id": "http://arxiv.org/abs/2308.03296v1", "guidislink": true, "link": "http://arxiv.org/abs/2308.03296v1", "updated": "2023-08-07T04:47:42Z", "updated_parsed": [2023, 8, 7, 4, 47, 42, 0, 219, 0], "published": "2023-08-07T04:47:42Z", "published_parsed": [2023, 8, 7, 4, 47, 42, 0, 219, 0], "title": "Studying Large Language Model Generalization with Influence Functions", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=studying+large+language+model+generalization+with+influence+functions&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Studying Large Language Model Generalization with Influence Functions"}, "summary": "When trying to gain better visibility into a machine learning model in order\nto understand and mitigate the associated risks, a potentially valuable source\nof evidence is: which training examples most contribute to a given behavior?\nInfluence functions aim to answer a counterfactual: how would the model's\nparameters (and hence its outputs) change if a given sequence were added to the\ntraining set? While influence functions have produced insights for small\nmodels, they are difficult to scale to large language models (LLMs) due to the\ndifficulty of computing an inverse-Hessian-vector product (IHVP). We use the\nEigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC)\napproximation to scale influence functions up to LLMs with up to 52 billion\nparameters. In our experiments, EK-FAC achieves similar accuracy to traditional\ninfluence function estimators despite the IHVP computation being orders of\nmagnitude faster. We investigate two algorithmic techniques to reduce the cost\nof computing gradients of candidate training sequences: TF-IDF filtering and\nquery batching. We use influence functions to investigate the generalization\npatterns of LLMs, including the sparsity of the influence patterns, increasing\nabstraction with scale, math and programming abilities, cross-lingual\ngeneralization, and role-playing behavior. Despite many apparently\nsophisticated forms of generalization, we identify a surprising limitation:\ninfluences decay to near-zero when the order of key phrases is flipped.\nOverall, influence functions give us a powerful new tool for studying the\ngeneralization properties of LLMs.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=studying+large+language+model+generalization+with+influence+functions&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "When trying to gain better visibility into a machine learning model in order\nto understand and mitigate the associated risks, a potentially valuable source\nof evidence is: which training examples most contribute to a given behavior?\nInfluence functions aim to answer a counterfactual: how would the model's\nparameters (and hence its outputs) change if a given sequence were added to the\ntraining set? While influence functions have produced insights for small\nmodels, they are difficult to scale to large language models (LLMs) due to the\ndifficulty of computing an inverse-Hessian-vector product (IHVP). We use the\nEigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC)\napproximation to scale influence functions up to LLMs with up to 52 billion\nparameters. In our experiments, EK-FAC achieves similar accuracy to traditional\ninfluence function estimators despite the IHVP computation being orders of\nmagnitude faster. We investigate two algorithmic techniques to reduce the cost\nof computing gradients of candidate training sequences: TF-IDF filtering and\nquery batching. We use influence functions to investigate the generalization\npatterns of LLMs, including the sparsity of the influence patterns, increasing\nabstraction with scale, math and programming abilities, cross-lingual\ngeneralization, and role-playing behavior. Despite many apparently\nsophisticated forms of generalization, we identify a surprising limitation:\ninfluences decay to near-zero when the order of key phrases is flipped.\nOverall, influence functions give us a powerful new tool for studying the\ngeneralization properties of LLMs."}, "authors": [{"name": "Roger Grosse"}, {"name": "Juhan Bae"}, {"name": "Cem Anil"}, {"name": "Nelson Elhage"}, {"name": "Alex Tamkin"}, {"name": "Amirhossein Tajdini"}, {"name": "Benoit Steiner"}, {"name": "Dustin Li"}, {"name": "Esin Durmus"}, {"name": "Ethan Perez"}, {"name": "Evan Hubinger"}, {"name": "Kamil\u0117 Luko\u0161i\u016bt\u0117"}, {"name": "Karina Nguyen"}, {"name": "Nicholas Joseph"}, {"name": "Sam McCandlish"}, {"name": "Jared Kaplan"}, {"name": "Samuel R. Bowman"}], "author_detail": {"name": "Samuel R. Bowman"}, "author": "Samuel R. Bowman", "arxiv_comment": "119 pages, 47 figures, 22 tables", "links": [{"href": "http://arxiv.org/abs/2308.03296v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2308.03296v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}