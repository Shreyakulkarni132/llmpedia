{"id": "http://arxiv.org/abs/2306.17806v1", "guidislink": true, "link": "http://arxiv.org/abs/2306.17806v1", "updated": "2023-06-30T17:07:02Z", "updated_parsed": [2023, 6, 30, 17, 7, 2, 4, 181, 0], "published": "2023-06-30T17:07:02Z", "published_parsed": [2023, 6, 30, 17, 7, 2, 4, 181, 0], "title": "Stay on topic with Classifier-Free Guidance", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=stay+on+topic+with+classifier+free+guidance&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Stay on topic with Classifier-Free Guidance"}, "summary": "Classifier-Free Guidance (CFG) has recently emerged in text-to-image\ngeneration as a lightweight technique to encourage prompt-adherence in\ngenerations. In this work, we demonstrate that CFG can be used broadly as an\ninference-time technique in pure language modeling. We show that CFG (1)\nimproves the performance of Pythia, GPT-2 and LLaMA-family models across an\narray of tasks: Q\\&A, reasoning, code generation, and machine translation,\nachieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements\nequivalent to a model with twice the parameter-count; (3) can stack alongside\nother inference-time methods like Chain-of-Thought and Self-Consistency,\nyielding further improvements in difficult tasks; (4) can be used to increase\nthe faithfulness and coherence of assistants in challenging form-driven and\ncontent-driven prompts: in a human evaluation we show a 75\\% preference for\nGPT4All using CFG over baseline.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=stay+on+topic+with+classifier+free+guidance&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Classifier-Free Guidance (CFG) has recently emerged in text-to-image\ngeneration as a lightweight technique to encourage prompt-adherence in\ngenerations. In this work, we demonstrate that CFG can be used broadly as an\ninference-time technique in pure language modeling. We show that CFG (1)\nimproves the performance of Pythia, GPT-2 and LLaMA-family models across an\narray of tasks: Q\\&A, reasoning, code generation, and machine translation,\nachieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements\nequivalent to a model with twice the parameter-count; (3) can stack alongside\nother inference-time methods like Chain-of-Thought and Self-Consistency,\nyielding further improvements in difficult tasks; (4) can be used to increase\nthe faithfulness and coherence of assistants in challenging form-driven and\ncontent-driven prompts: in a human evaluation we show a 75\\% preference for\nGPT4All using CFG over baseline."}, "authors": [{"name": "Guillaume Sanchez"}, {"name": "Honglu Fan"}, {"name": "Alexander Spangher"}, {"name": "Elad Levi"}, {"name": "Pawan Sasanka Ammanamanchi"}, {"name": "Stella Biderman"}], "author_detail": {"name": "Stella Biderman"}, "author": "Stella Biderman", "links": [{"href": "http://arxiv.org/abs/2306.17806v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2306.17806v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}