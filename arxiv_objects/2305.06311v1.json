{"id": "http://arxiv.org/abs/2305.06311v1", "guidislink": true, "link": "http://arxiv.org/abs/2305.06311v1", "updated": "2023-05-10T16:58:33Z", "updated_parsed": [2023, 5, 10, 16, 58, 33, 2, 130, 0], "published": "2023-05-10T16:58:33Z", "published_parsed": [2023, 5, 10, 16, 58, 33, 2, 130, 0], "title": "Automatic Evaluation of Attribution by Large Language Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=automatic+evaluation+of+attribution+by+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Automatic Evaluation of Attribution by Large Language Models"}, "summary": "A recent focus of large language model (LLM) development, as exemplified by\ngenerative search engines, is to incorporate external references to generate\nand support their claims. However, evaluating the attribution, i.e., verifying\nwhether the generated statement is indeed fully supported by the cited\nreference, remains an open problem. Although human evaluation is common\npractice, it is costly and time-consuming. In this paper, we investigate the\nautomatic evaluation of attribution by LLMs. We begin by providing a definition\nof attribution and then explore two approaches for automatic evaluation:\nprompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed\nfrom related tasks, such as question answering, fact-checking, natural language\ninference, and summarization. To facilitate the evaluation, we manually curate\na set of test examples covering 12 domains from a generative search engine, New\nBing. Our results on the curated test set and simulated test examples from\nexisting benchmark questions highlight both promising signals as well as\nremaining challenges for the automatic evaluation of attribution. We hope our\ntestbed, modeling methodology, and insights will help lay the foundation for\nfuture studies on this important problem.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=automatic+evaluation+of+attribution+by+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "A recent focus of large language model (LLM) development, as exemplified by\ngenerative search engines, is to incorporate external references to generate\nand support their claims. However, evaluating the attribution, i.e., verifying\nwhether the generated statement is indeed fully supported by the cited\nreference, remains an open problem. Although human evaluation is common\npractice, it is costly and time-consuming. In this paper, we investigate the\nautomatic evaluation of attribution by LLMs. We begin by providing a definition\nof attribution and then explore two approaches for automatic evaluation:\nprompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed\nfrom related tasks, such as question answering, fact-checking, natural language\ninference, and summarization. To facilitate the evaluation, we manually curate\na set of test examples covering 12 domains from a generative search engine, New\nBing. Our results on the curated test set and simulated test examples from\nexisting benchmark questions highlight both promising signals as well as\nremaining challenges for the automatic evaluation of attribution. We hope our\ntestbed, modeling methodology, and insights will help lay the foundation for\nfuture studies on this important problem."}, "authors": [{"name": "Xiang Yue"}, {"name": "Boshi Wang"}, {"name": "Kai Zhang"}, {"name": "Ziru Chen"}, {"name": "Yu Su"}, {"name": "Huan Sun"}], "author_detail": {"name": "Huan Sun"}, "author": "Huan Sun", "links": [{"href": "http://arxiv.org/abs/2305.06311v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2305.06311v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}