{"id": "http://arxiv.org/abs/2302.08399v5", "guidislink": true, "link": "http://arxiv.org/abs/2302.08399v5", "updated": "2023-03-14T13:47:26Z", "updated_parsed": [2023, 3, 14, 13, 47, 26, 1, 73, 0], "published": "2023-02-16T16:18:03Z", "published_parsed": [2023, 2, 16, 16, 18, 3, 3, 47, 0], "title": "Large Language Models Fail on Trivial Alterations to Theory-of-Mind\n  Tasks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=large+language+models+fail+on+trivial+alterations+to+theory+of+mind+tasks&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Large Language Models Fail on Trivial Alterations to Theory-of-Mind\n  Tasks"}, "summary": "Intuitive psychology is a pillar of common-sense reasoning. The replication\nof this reasoning in machine intelligence is an important stepping-stone on the\nway to human-like artificial intelligence. Several recent tasks and benchmarks\nfor examining this reasoning in Large-Large Models have focused in particular\non belief attribution in Theory-of-Mind tasks. These tasks have shown both\nsuccesses and failures. We consider in particular a recent purported success\ncase, and show that small variations that maintain the principles of ToM turn\nthe results on their head. We argue that in general, the zero-hypothesis for\nmodel evaluation in intuitive psychology should be skeptical, and that outlying\nfailure cases should outweigh average success rates. We also consider what\npossible future successes on Theory-of-Mind tasks by more powerful LLMs would\nmean for ToM tasks with people.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=large+language+models+fail+on+trivial+alterations+to+theory+of+mind+tasks&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Intuitive psychology is a pillar of common-sense reasoning. The replication\nof this reasoning in machine intelligence is an important stepping-stone on the\nway to human-like artificial intelligence. Several recent tasks and benchmarks\nfor examining this reasoning in Large-Large Models have focused in particular\non belief attribution in Theory-of-Mind tasks. These tasks have shown both\nsuccesses and failures. We consider in particular a recent purported success\ncase, and show that small variations that maintain the principles of ToM turn\nthe results on their head. We argue that in general, the zero-hypothesis for\nmodel evaluation in intuitive psychology should be skeptical, and that outlying\nfailure cases should outweigh average success rates. We also consider what\npossible future successes on Theory-of-Mind tasks by more powerful LLMs would\nmean for ToM tasks with people."}, "authors": [{"name": "Tomer Ullman"}], "author_detail": {"name": "Tomer Ullman"}, "author": "Tomer Ullman", "arxiv_comment": "11 pages, 2 figures", "links": [{"href": "http://arxiv.org/abs/2302.08399v5", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2302.08399v5", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "I.2.7", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}