{"id": "http://arxiv.org/abs/2305.15507v1", "guidislink": true, "link": "http://arxiv.org/abs/2305.15507v1", "updated": "2023-05-24T18:54:39Z", "updated_parsed": [2023, 5, 24, 18, 54, 39, 2, 144, 0], "published": "2023-05-24T18:54:39Z", "published_parsed": [2023, 5, 24, 18, 54, 39, 2, 144, 0], "title": "The Larger They Are, the Harder They Fail: Language Models do not\n  Recognize Identifier Swaps in Python", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=the+larger+they+are++the+harder+they+fail++language+models+do+not+recognize+identifier+swaps+in+python&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "The Larger They Are, the Harder They Fail: Language Models do not\n  Recognize Identifier Swaps in Python"}, "summary": "Large Language Models (LLMs) have successfully been applied to code\ngeneration tasks, raising the question of how well these models understand\nprogramming. Typical programming languages have invariances and equivariances\nin their semantics that human programmers intuitively understand and exploit,\nsuch as the (near) invariance to the renaming of identifiers. We show that LLMs\nnot only fail to properly generate correct Python code when default function\nnames are swapped, but some of them even become more confident in their\nincorrect predictions as the model size increases, an instance of the recently\ndiscovered phenomenon of Inverse Scaling, which runs contrary to the commonly\nobserved trend of increasing prediction quality with increasing model size. Our\nfindings indicate that, despite their astonishing typical-case performance,\nLLMs still lack a deep, abstract understanding of the content they manipulate,\nmaking them unsuitable for tasks that statistically deviate from their training\ndata, and that mere scaling is not enough to achieve such capability.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=the+larger+they+are++the+harder+they+fail++language+models+do+not+recognize+identifier+swaps+in+python&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Large Language Models (LLMs) have successfully been applied to code\ngeneration tasks, raising the question of how well these models understand\nprogramming. Typical programming languages have invariances and equivariances\nin their semantics that human programmers intuitively understand and exploit,\nsuch as the (near) invariance to the renaming of identifiers. We show that LLMs\nnot only fail to properly generate correct Python code when default function\nnames are swapped, but some of them even become more confident in their\nincorrect predictions as the model size increases, an instance of the recently\ndiscovered phenomenon of Inverse Scaling, which runs contrary to the commonly\nobserved trend of increasing prediction quality with increasing model size. Our\nfindings indicate that, despite their astonishing typical-case performance,\nLLMs still lack a deep, abstract understanding of the content they manipulate,\nmaking them unsuitable for tasks that statistically deviate from their training\ndata, and that mere scaling is not enough to achieve such capability."}, "authors": [{"name": "Antonio Valerio Miceli-Barone"}, {"name": "Fazl Barez"}, {"name": "Ioannis Konstas"}, {"name": "Shay B. Cohen"}], "author_detail": {"name": "Shay B. Cohen"}, "author": "Shay B. Cohen", "arxiv_comment": "17 pages, 5 figure, ACL 2023", "links": [{"href": "http://arxiv.org/abs/2305.15507v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2305.15507v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}