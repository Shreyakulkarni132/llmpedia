{"id": "http://arxiv.org/abs/2304.00740v2", "guidislink": true, "link": "http://arxiv.org/abs/2304.00740v2", "updated": "2023-05-22T14:53:20Z", "updated_parsed": [2023, 5, 22, 14, 53, 20, 0, 142, 0], "published": "2023-04-03T06:24:10Z", "published_parsed": [2023, 4, 3, 6, 24, 10, 0, 93, 0], "title": "Inspecting and Editing Knowledge Representations in Language Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=inspecting+and+editing+knowledge+representations+in+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Inspecting and Editing Knowledge Representations in Language Models"}, "summary": "Neural language models (LMs) represent facts about the world described by\ntext. Sometimes these facts derive from training data (in most LMs, a\nrepresentation of the word \"banana\" encodes the fact that bananas are fruits).\nSometimes facts derive from input text itself (a representation of the sentence\n\"I poured out the bottle\" encodes the fact that the bottle became empty). We\ndescribe REMEDI, a method for learning to map statements in natural language to\nfact encodings in an LM's internal representation system. REMEDI encodings can\nbe used as knowledge editors: when added to LM hidden representations, they\nmodify downstream generation to be consistent with new facts. REMEDI encodings\nmay also be used as probes: when compared to LM representations, they reveal\nwhich properties LMs already attribute to mentioned entities, in some cases\nmaking it possible to predict when LMs will generate outputs that conflict with\nbackground knowledge or input text. REMEDI thus links work on probing,\nprompting, and LM editing, and offers steps toward general tools for\nfine-grained inspection and control of knowledge in LMs.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=inspecting+and+editing+knowledge+representations+in+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Neural language models (LMs) represent facts about the world described by\ntext. Sometimes these facts derive from training data (in most LMs, a\nrepresentation of the word \"banana\" encodes the fact that bananas are fruits).\nSometimes facts derive from input text itself (a representation of the sentence\n\"I poured out the bottle\" encodes the fact that the bottle became empty). We\ndescribe REMEDI, a method for learning to map statements in natural language to\nfact encodings in an LM's internal representation system. REMEDI encodings can\nbe used as knowledge editors: when added to LM hidden representations, they\nmodify downstream generation to be consistent with new facts. REMEDI encodings\nmay also be used as probes: when compared to LM representations, they reveal\nwhich properties LMs already attribute to mentioned entities, in some cases\nmaking it possible to predict when LMs will generate outputs that conflict with\nbackground knowledge or input text. REMEDI thus links work on probing,\nprompting, and LM editing, and offers steps toward general tools for\nfine-grained inspection and control of knowledge in LMs."}, "authors": [{"name": "Evan Hernandez"}, {"name": "Belinda Z. Li"}, {"name": "Jacob Andreas"}], "author_detail": {"name": "Jacob Andreas"}, "author": "Jacob Andreas", "links": [{"href": "http://arxiv.org/abs/2304.00740v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2304.00740v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}