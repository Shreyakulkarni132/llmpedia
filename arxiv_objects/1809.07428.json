{"id": "http://arxiv.org/abs/1809.07428v1", "guidislink": true, "link": "http://arxiv.org/abs/1809.07428v1", "updated": "2018-09-19T23:25:24Z", "updated_parsed": [2018, 9, 19, 23, 25, 24, 2, 262, 0], "published": "2018-09-19T23:25:24Z", "published_parsed": [2018, 9, 19, 23, 25, 24, 2, 262, 0], "title": "Ranking Distillation: Learning Compact Ranking Models With High\n  Performance for Recommender System", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=ranking+distillation++learning+compact+ranking+models+with+high+performance+for+recommender+system&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Ranking Distillation: Learning Compact Ranking Models With High\n  Performance for Recommender System"}, "summary": "We propose a novel way to train ranking models, such as recommender systems,\nthat are both effective and efficient. Knowledge distillation (KD) was shown to\nbe successful in image recognition to achieve both effectiveness and\nefficiency. We propose a KD technique for learning to rank problems, called\n\\emph{ranking distillation (RD)}. Specifically, we train a smaller student\nmodel to learn to rank documents/items from both the training data and the\nsupervision of a larger teacher model. The student model achieves a similar\nranking performance to that of the large teacher model, but its smaller model\nsize makes the online inference more efficient. RD is flexible because it is\northogonal to the choices of ranking models for the teacher and student. We\naddress the challenges of RD for ranking problems. The experiments on public\ndata sets and state-of-the-art recommendation models showed that RD achieves\nits design purposes: the student model learnt with RD has a model size less\nthan half of the teacher model while achieving a ranking performance similar to\nthe teacher model and much better than the student model learnt without RD.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=ranking+distillation++learning+compact+ranking+models+with+high+performance+for+recommender+system&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "We propose a novel way to train ranking models, such as recommender systems,\nthat are both effective and efficient. Knowledge distillation (KD) was shown to\nbe successful in image recognition to achieve both effectiveness and\nefficiency. We propose a KD technique for learning to rank problems, called\n\\emph{ranking distillation (RD)}. Specifically, we train a smaller student\nmodel to learn to rank documents/items from both the training data and the\nsupervision of a larger teacher model. The student model achieves a similar\nranking performance to that of the large teacher model, but its smaller model\nsize makes the online inference more efficient. RD is flexible because it is\northogonal to the choices of ranking models for the teacher and student. We\naddress the challenges of RD for ranking problems. The experiments on public\ndata sets and state-of-the-art recommendation models showed that RD achieves\nits design purposes: the student model learnt with RD has a model size less\nthan half of the teacher model while achieving a ranking performance similar to\nthe teacher model and much better than the student model learnt without RD."}, "authors": [{"name": "Jiaxi Tang"}, {"name": "Ke Wang"}], "author_detail": {"name": "Ke Wang"}, "author": "Ke Wang", "arxiv_comment": "Accepted at KDD 2018", "links": [{"href": "http://arxiv.org/abs/1809.07428v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1809.07428v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}