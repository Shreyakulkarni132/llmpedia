{"id": "http://arxiv.org/abs/2201.11990v3", "guidislink": true, "link": "http://arxiv.org/abs/2201.11990v3", "updated": "2022-02-04T18:02:23Z", "updated_parsed": [2022, 2, 4, 18, 2, 23, 4, 35, 0], "published": "2022-01-28T08:59:57Z", "published_parsed": [2022, 1, 28, 8, 59, 57, 4, 28, 0], "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A\n  Large-Scale Generative Language Model", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=using+deepspeed+and+megatron+to+train+megatron+turing+nlg+530b++a+large+scale+generative+language+model&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A\n  Large-Scale Generative Language Model"}, "summary": "Pretrained general-purpose language models can achieve state-of-the-art\naccuracies in various natural language processing domains by adapting to\ndownstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of\ntheir success, the size of these models has increased rapidly, requiring\nhigh-performance hardware, software, and algorithmic techniques to enable\ntraining such large models. As the result of a joint effort between Microsoft\nand NVIDIA, we present details on the training of the largest monolithic\ntransformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530\nbillion parameters. In this paper, we first focus on the infrastructure as well\nas the 3D parallelism methodology used to train this model using DeepSpeed and\nMegatron. Next, we detail the training process, the design of our training\ncorpus, and our data curation techniques, which we believe is a key ingredient\nto the success of the model. Finally, we discuss various evaluation results, as\nwell as other interesting observations and new properties exhibited by MT-NLG.\nWe demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning\naccuracies on several NLP benchmarks and establishes new state-of-the-art\nresults. We believe that our contributions will help further the development of\nlarge-scale training infrastructures, large-scale language models, and natural\nlanguage generations.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=using+deepspeed+and+megatron+to+train+megatron+turing+nlg+530b++a+large+scale+generative+language+model&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Pretrained general-purpose language models can achieve state-of-the-art\naccuracies in various natural language processing domains by adapting to\ndownstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of\ntheir success, the size of these models has increased rapidly, requiring\nhigh-performance hardware, software, and algorithmic techniques to enable\ntraining such large models. As the result of a joint effort between Microsoft\nand NVIDIA, we present details on the training of the largest monolithic\ntransformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530\nbillion parameters. In this paper, we first focus on the infrastructure as well\nas the 3D parallelism methodology used to train this model using DeepSpeed and\nMegatron. Next, we detail the training process, the design of our training\ncorpus, and our data curation techniques, which we believe is a key ingredient\nto the success of the model. Finally, we discuss various evaluation results, as\nwell as other interesting observations and new properties exhibited by MT-NLG.\nWe demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning\naccuracies on several NLP benchmarks and establishes new state-of-the-art\nresults. We believe that our contributions will help further the development of\nlarge-scale training infrastructures, large-scale language models, and natural\nlanguage generations."}, "authors": [{"name": "Shaden Smith"}, {"name": "Mostofa Patwary"}, {"name": "Brandon Norick"}, {"name": "Patrick LeGresley"}, {"name": "Samyam Rajbhandari"}, {"name": "Jared Casper"}, {"name": "Zhun Liu"}, {"name": "Shrimai Prabhumoye"}, {"name": "George Zerveas"}, {"name": "Vijay Korthikanti"}, {"name": "Elton Zhang"}, {"name": "Rewon Child"}, {"name": "Reza Yazdani Aminabadi"}, {"name": "Julie Bernauer"}, {"name": "Xia Song"}, {"name": "Mohammad Shoeybi"}, {"name": "Yuxiong He"}, {"name": "Michael Houston"}, {"name": "Saurabh Tiwary"}, {"name": "Bryan Catanzaro"}], "author_detail": {"name": "Bryan Catanzaro"}, "author": "Bryan Catanzaro", "arxiv_comment": "Shaden Smith and Mostofa Patwary contributed equally", "links": [{"href": "http://arxiv.org/abs/2201.11990v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2201.11990v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}