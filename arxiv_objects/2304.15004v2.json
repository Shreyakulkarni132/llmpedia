{"id": "http://arxiv.org/abs/2304.15004v2", "guidislink": true, "link": "http://arxiv.org/abs/2304.15004v2", "updated": "2023-05-22T15:56:25Z", "updated_parsed": [2023, 5, 22, 15, 56, 25, 0, 142, 0], "published": "2023-04-28T17:52:11Z", "published_parsed": [2023, 4, 28, 17, 52, 11, 4, 118, 0], "title": "Are Emergent Abilities of Large Language Models a Mirage?", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=are+emergent+abilities+of+large+language+models+a+mirage+&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Are Emergent Abilities of Large Language Models a Mirage?"}, "summary": "Recent work claims that large language models display emergent abilities,\nabilities not present in smaller-scale models that are present in larger-scale\nmodels. What makes emergent abilities intriguing is two-fold: their sharpness,\ntransitioning seemingly instantaneously from not present to present, and their\nunpredictability, appearing at seemingly unforeseeable model scales. Here, we\npresent an alternative explanation for emergent abilities: that for a\nparticular task and model family, when analyzing fixed model outputs, emergent\nabilities appear due to the researcher's choice of metric rather than due to\nfundamental changes in model behavior with scale. Specifically, nonlinear or\ndiscontinuous metrics produce apparent emergent abilities, whereas linear or\ncontinuous metrics produce smooth, continuous predictable changes in model\nperformance. We present our alternative explanation in a simple mathematical\nmodel, then test it in three complementary ways: we (1) make, test and confirm\nthree predictions on the effect of metric choice using the InstructGPT/GPT-3\nfamily on tasks with claimed emergent abilities; (2) make, test and confirm two\npredictions about metric choices in a meta-analysis of emergent abilities on\nBIG-Bench; and (3) show to choose metrics to produce never-before-seen\nseemingly emergent abilities in multiple vision tasks across diverse deep\nnetworks. Via all three analyses, we provide evidence that alleged emergent\nabilities evaporate with different metrics or with better statistics, and may\nnot be a fundamental property of scaling AI models.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=are+emergent+abilities+of+large+language+models+a+mirage+&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Recent work claims that large language models display emergent abilities,\nabilities not present in smaller-scale models that are present in larger-scale\nmodels. What makes emergent abilities intriguing is two-fold: their sharpness,\ntransitioning seemingly instantaneously from not present to present, and their\nunpredictability, appearing at seemingly unforeseeable model scales. Here, we\npresent an alternative explanation for emergent abilities: that for a\nparticular task and model family, when analyzing fixed model outputs, emergent\nabilities appear due to the researcher's choice of metric rather than due to\nfundamental changes in model behavior with scale. Specifically, nonlinear or\ndiscontinuous metrics produce apparent emergent abilities, whereas linear or\ncontinuous metrics produce smooth, continuous predictable changes in model\nperformance. We present our alternative explanation in a simple mathematical\nmodel, then test it in three complementary ways: we (1) make, test and confirm\nthree predictions on the effect of metric choice using the InstructGPT/GPT-3\nfamily on tasks with claimed emergent abilities; (2) make, test and confirm two\npredictions about metric choices in a meta-analysis of emergent abilities on\nBIG-Bench; and (3) show to choose metrics to produce never-before-seen\nseemingly emergent abilities in multiple vision tasks across diverse deep\nnetworks. Via all three analyses, we provide evidence that alleged emergent\nabilities evaporate with different metrics or with better statistics, and may\nnot be a fundamental property of scaling AI models."}, "authors": [{"name": "Rylan Schaeffer"}, {"name": "Brando Miranda"}, {"name": "Sanmi Koyejo"}], "author_detail": {"name": "Sanmi Koyejo"}, "author": "Sanmi Koyejo", "links": [{"href": "http://arxiv.org/abs/2304.15004v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2304.15004v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}