{"id": "http://arxiv.org/abs/2301.12597v3", "guidislink": true, "link": "http://arxiv.org/abs/2301.12597v3", "updated": "2023-06-15T07:57:29Z", "updated_parsed": [2023, 6, 15, 7, 57, 29, 3, 166, 0], "published": "2023-01-30T00:56:51Z", "published_parsed": [2023, 1, 30, 0, 56, 51, 0, 30, 0], "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image\n  Encoders and Large Language Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=blip+2++bootstrapping+language+image+pre+training+with+frozen+image+encoders+and+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image\n  Encoders and Large Language Models"}, "summary": "The cost of vision-and-language pre-training has become increasingly\nprohibitive due to end-to-end training of large-scale models. This paper\nproposes BLIP-2, a generic and efficient pre-training strategy that bootstraps\nvision-language pre-training from off-the-shelf frozen pre-trained image\nencoders and frozen large language models. BLIP-2 bridges the modality gap with\na lightweight Querying Transformer, which is pre-trained in two stages. The\nfirst stage bootstraps vision-language representation learning from a frozen\nimage encoder. The second stage bootstraps vision-to-language generative\nlearning from a frozen language model. BLIP-2 achieves state-of-the-art\nperformance on various vision-language tasks, despite having significantly\nfewer trainable parameters than existing methods. For example, our model\noutperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable\nparameters. We also demonstrate the model's emerging capabilities of zero-shot\nimage-to-text generation that can follow natural language instructions.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=blip+2++bootstrapping+language+image+pre+training+with+frozen+image+encoders+and+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "The cost of vision-and-language pre-training has become increasingly\nprohibitive due to end-to-end training of large-scale models. This paper\nproposes BLIP-2, a generic and efficient pre-training strategy that bootstraps\nvision-language pre-training from off-the-shelf frozen pre-trained image\nencoders and frozen large language models. BLIP-2 bridges the modality gap with\na lightweight Querying Transformer, which is pre-trained in two stages. The\nfirst stage bootstraps vision-language representation learning from a frozen\nimage encoder. The second stage bootstraps vision-to-language generative\nlearning from a frozen language model. BLIP-2 achieves state-of-the-art\nperformance on various vision-language tasks, despite having significantly\nfewer trainable parameters than existing methods. For example, our model\noutperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable\nparameters. We also demonstrate the model's emerging capabilities of zero-shot\nimage-to-text generation that can follow natural language instructions."}, "authors": [{"name": "Junnan Li"}, {"name": "Dongxu Li"}, {"name": "Silvio Savarese"}, {"name": "Steven Hoi"}], "author_detail": {"name": "Steven Hoi"}, "author": "Steven Hoi", "links": [{"href": "http://arxiv.org/abs/2301.12597v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2301.12597v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}