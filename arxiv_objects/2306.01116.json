{"id": "http://arxiv.org/abs/2306.01116v1", "guidislink": true, "link": "http://arxiv.org/abs/2306.01116v1", "updated": "2023-06-01T20:03:56Z", "updated_parsed": [2023, 6, 1, 20, 3, 56, 3, 152, 0], "published": "2023-06-01T20:03:56Z", "published_parsed": [2023, 6, 1, 20, 3, 56, 3, 152, 0], "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora\n  with Web Data, and Web Data Only", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=the+refinedweb+dataset+for+falcon+llm++outperforming+curated+corpora+with+web+data++and+web+data+only&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora\n  with Web Data, and Web Data Only"}, "summary": "Large language models are commonly trained on a mixture of filtered web data\nand curated high-quality corpora, such as social media conversations, books, or\ntechnical papers. This curation process is believed to be necessary to produce\nperformant models with broad zero-shot generalization abilities. However, as\nlarger models requiring pretraining on trillions of tokens are considered, it\nis unclear how scalable is curation and whether we will run out of unique\nhigh-quality data soon. At variance with previous beliefs, we show that\nproperly filtered and deduplicated web data alone can lead to powerful models;\neven significantly outperforming models from the state-of-the-art trained on\nThe Pile. Despite extensive filtering, the high-quality data we extract from\nthe web is still plentiful, and we are able to obtain five trillion tokens from\nCommonCrawl. We publicly release an extract of 600 billion tokens from our\nRefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=the+refinedweb+dataset+for+falcon+llm++outperforming+curated+corpora+with+web+data++and+web+data+only&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Large language models are commonly trained on a mixture of filtered web data\nand curated high-quality corpora, such as social media conversations, books, or\ntechnical papers. This curation process is believed to be necessary to produce\nperformant models with broad zero-shot generalization abilities. However, as\nlarger models requiring pretraining on trillions of tokens are considered, it\nis unclear how scalable is curation and whether we will run out of unique\nhigh-quality data soon. At variance with previous beliefs, we show that\nproperly filtered and deduplicated web data alone can lead to powerful models;\neven significantly outperforming models from the state-of-the-art trained on\nThe Pile. Despite extensive filtering, the high-quality data we extract from\nthe web is still plentiful, and we are able to obtain five trillion tokens from\nCommonCrawl. We publicly release an extract of 600 billion tokens from our\nRefinedWeb dataset, and 1.3/7.5B parameters language models trained on it."}, "authors": [{"name": "Guilherme Penedo"}, {"name": "Quentin Malartic"}, {"name": "Daniel Hesslow"}, {"name": "Ruxandra Cojocaru"}, {"name": "Alessandro Cappelli"}, {"name": "Hamza Alobeidli"}, {"name": "Baptiste Pannier"}, {"name": "Ebtesam Almazrouei"}, {"name": "Julien Launay"}], "author_detail": {"name": "Julien Launay"}, "author": "Julien Launay", "links": [{"href": "http://arxiv.org/abs/2306.01116v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2306.01116v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}