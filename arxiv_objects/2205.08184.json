{"id": "http://arxiv.org/abs/2205.08184v1", "guidislink": true, "link": "http://arxiv.org/abs/2205.08184v1", "updated": "2022-05-17T09:12:22Z", "updated_parsed": [2022, 5, 17, 9, 12, 22, 1, 137, 0], "published": "2022-05-17T09:12:22Z", "published_parsed": [2022, 5, 17, 9, 12, 22, 1, 137, 0], "title": "SKILL: Structured Knowledge Infusion for Large Language Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=skill++structured+knowledge+infusion+for+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "SKILL: Structured Knowledge Infusion for Large Language Models"}, "summary": "Large language models (LLMs) have demonstrated human-level performance on a\nvast spectrum of natural language tasks. However, it is largely unexplored\nwhether they can better internalize knowledge from a structured data, such as a\nknowledge graph, or from text. In this work, we propose a method to infuse\nstructured knowledge into LLMs, by directly training T5 models on factual\ntriples of knowledge graphs (KGs). We show that models pre-trained on Wikidata\nKG with our method outperform the T5 baselines on FreebaseQA and WikiHop, as\nwell as the Wikidata-answerable subset of TriviaQA and NaturalQuestions. The\nmodels pre-trained on factual triples compare competitively with the ones on\nnatural language sentences that contain the same knowledge. Trained on a\nsmaller size KG, WikiMovies, we saw 3x improvement of exact match score on\nMetaQA task compared to T5 baseline. The proposed method has an advantage that\nno alignment between the knowledge graph and text corpus is required in\ncurating training data. This makes our method particularly useful when working\nwith industry-scale knowledge graphs.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=skill++structured+knowledge+infusion+for+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Large language models (LLMs) have demonstrated human-level performance on a\nvast spectrum of natural language tasks. However, it is largely unexplored\nwhether they can better internalize knowledge from a structured data, such as a\nknowledge graph, or from text. In this work, we propose a method to infuse\nstructured knowledge into LLMs, by directly training T5 models on factual\ntriples of knowledge graphs (KGs). We show that models pre-trained on Wikidata\nKG with our method outperform the T5 baselines on FreebaseQA and WikiHop, as\nwell as the Wikidata-answerable subset of TriviaQA and NaturalQuestions. The\nmodels pre-trained on factual triples compare competitively with the ones on\nnatural language sentences that contain the same knowledge. Trained on a\nsmaller size KG, WikiMovies, we saw 3x improvement of exact match score on\nMetaQA task compared to T5 baseline. The proposed method has an advantage that\nno alignment between the knowledge graph and text corpus is required in\ncurating training data. This makes our method particularly useful when working\nwith industry-scale knowledge graphs."}, "authors": [{"name": "Fedor Moiseev"}, {"name": "Zhe Dong"}, {"name": "Enrique Alfonseca"}, {"name": "Martin Jaggi"}], "author_detail": {"name": "Martin Jaggi"}, "author": "Martin Jaggi", "arxiv_comment": "NAACL 2022", "links": [{"href": "http://arxiv.org/abs/2205.08184v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2205.08184v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}