{"id": "http://arxiv.org/abs/2307.11795v1", "guidislink": true, "link": "http://arxiv.org/abs/2307.11795v1", "updated": "2023-07-21T08:39:15Z", "updated_parsed": [2023, 7, 21, 8, 39, 15, 4, 202, 0], "published": "2023-07-21T08:39:15Z", "published_parsed": [2023, 7, 21, 8, 39, 15, 4, 202, 0], "title": "Prompting Large Language Models with Speech Recognition Abilities", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=prompting+large+language+models+with+speech+recognition+abilities&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Prompting Large Language Models with Speech Recognition Abilities"}, "summary": "Large language models have proven themselves highly flexible, able to solve a\nwide range of generative tasks, such as abstractive summarization and\nopen-ended question answering. In this paper we extend the capabilities of LLMs\nby directly attaching a small audio encoder allowing it to perform speech\nrecognition. By directly prepending a sequence of audial embeddings to the text\ntoken embeddings, the LLM can be converted to an automatic speech recognition\n(ASR) system, and be used in the exact same manner as its textual counterpart.\nExperiments on Multilingual LibriSpeech (MLS) show that incorporating a\nconformer encoder into the open sourced LLaMA-7B allows it to outperform\nmonolingual baselines by 18% and perform multilingual speech recognition\ndespite LLaMA being trained overwhelmingly on English text. Furthermore, we\nperform ablation studies to investigate whether the LLM can be completely\nfrozen during training to maintain its original capabilities, scaling up the\naudio encoder, and increasing the audio encoder striding to generate fewer\nembeddings. The results from these studies show that multilingual ASR is\npossible even when the LLM is frozen or when strides of almost 1 second are\nused in the audio encoder opening up the possibility for LLMs to operate on\nlong-form audio.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=prompting+large+language+models+with+speech+recognition+abilities&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Large language models have proven themselves highly flexible, able to solve a\nwide range of generative tasks, such as abstractive summarization and\nopen-ended question answering. In this paper we extend the capabilities of LLMs\nby directly attaching a small audio encoder allowing it to perform speech\nrecognition. By directly prepending a sequence of audial embeddings to the text\ntoken embeddings, the LLM can be converted to an automatic speech recognition\n(ASR) system, and be used in the exact same manner as its textual counterpart.\nExperiments on Multilingual LibriSpeech (MLS) show that incorporating a\nconformer encoder into the open sourced LLaMA-7B allows it to outperform\nmonolingual baselines by 18% and perform multilingual speech recognition\ndespite LLaMA being trained overwhelmingly on English text. Furthermore, we\nperform ablation studies to investigate whether the LLM can be completely\nfrozen during training to maintain its original capabilities, scaling up the\naudio encoder, and increasing the audio encoder striding to generate fewer\nembeddings. The results from these studies show that multilingual ASR is\npossible even when the LLM is frozen or when strides of almost 1 second are\nused in the audio encoder opening up the possibility for LLMs to operate on\nlong-form audio."}, "authors": [{"name": "Yassir Fathullah"}, {"name": "Chunyang Wu"}, {"name": "Egor Lakomkin"}, {"name": "Junteng Jia"}, {"name": "Yuan Shangguan"}, {"name": "Ke Li"}, {"name": "Jinxi Guo"}, {"name": "Wenhan Xiong"}, {"name": "Jay Mahadeokar"}, {"name": "Ozlem Kalinli"}, {"name": "Christian Fuegen"}, {"name": "Mike Seltzer"}], "author_detail": {"name": "Mike Seltzer"}, "author": "Mike Seltzer", "links": [{"href": "http://arxiv.org/abs/2307.11795v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.11795v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "eess.AS", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "eess.AS", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}