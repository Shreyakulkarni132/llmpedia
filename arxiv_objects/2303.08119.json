{"id": "http://arxiv.org/abs/2303.08119v3", "guidislink": true, "link": "http://arxiv.org/abs/2303.08119v3", "updated": "2023-04-24T22:28:54Z", "updated_parsed": [2023, 4, 24, 22, 28, 54, 0, 114, 0], "published": "2023-03-14T17:50:45Z", "published_parsed": [2023, 3, 14, 17, 50, 45, 1, 73, 0], "title": "How Many Demonstrations Do You Need for In-context Learning?", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=how+many+demonstrations+do+you+need+for+in+context+learning+&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "How Many Demonstrations Do You Need for In-context Learning?"}, "summary": "Large language models (LLMs) are capable to perform complex reasoning by\nin-context learning (ICL) when provided with a few input-output demonstrations\n(demos) and more powerful when intermediate reasoning steps (\"chain of thoughts\n(CoT)\") of the demos are given. Is it necessary to use multi-demo in ICL? In\nthis paper, we study ICL using fewer demos for each test query on the tasks\nin~\\cite{wei2022chain}. Surprisingly, we do not observe significant degradation\nwhen using only one randomly chosen demo. To study this phenomenon, for each\ntest query, we categorize demos into \"correct demos\" leading to the correct\nanswer, and \"wrong demos\" resulting in wrong answers. Our analysis reveals an\ninherent bias in those widely studied datasets: most demos are correct for a\nmajority of test queries, which explains the good performance of using one\nrandom demo. Moreover, ICL (with and w/o CoT) using only one correct demo\nsignificantly outperforms all-demo ICL adopted by most previous works,\nindicating the weakness of LLMs in finding correct demo(s) for input queries,\nwhich is difficult to evaluate on the biased datasets. Furthermore, we observe\na counterintuitive behavior of ICL using multi-demo, i.e., its accuracy\ndegrades(improves) when given more correct(wrong) demos. This implies that ICL\ncan be easily misguided by interference among demos and their spurious\ncorrelations. Our analyses highlight several fundamental challenges that need\nto be addressed in LLMs training, ICL, and benchmark design.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=how+many+demonstrations+do+you+need+for+in+context+learning+&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Large language models (LLMs) are capable to perform complex reasoning by\nin-context learning (ICL) when provided with a few input-output demonstrations\n(demos) and more powerful when intermediate reasoning steps (\"chain of thoughts\n(CoT)\") of the demos are given. Is it necessary to use multi-demo in ICL? In\nthis paper, we study ICL using fewer demos for each test query on the tasks\nin~\\cite{wei2022chain}. Surprisingly, we do not observe significant degradation\nwhen using only one randomly chosen demo. To study this phenomenon, for each\ntest query, we categorize demos into \"correct demos\" leading to the correct\nanswer, and \"wrong demos\" resulting in wrong answers. Our analysis reveals an\ninherent bias in those widely studied datasets: most demos are correct for a\nmajority of test queries, which explains the good performance of using one\nrandom demo. Moreover, ICL (with and w/o CoT) using only one correct demo\nsignificantly outperforms all-demo ICL adopted by most previous works,\nindicating the weakness of LLMs in finding correct demo(s) for input queries,\nwhich is difficult to evaluate on the biased datasets. Furthermore, we observe\na counterintuitive behavior of ICL using multi-demo, i.e., its accuracy\ndegrades(improves) when given more correct(wrong) demos. This implies that ICL\ncan be easily misguided by interference among demos and their spurious\ncorrelations. Our analyses highlight several fundamental challenges that need\nto be addressed in LLMs training, ICL, and benchmark design."}, "authors": [{"name": "Jiuhai Chen"}, {"name": "Lichang Chen"}, {"name": "Chen Zhu"}, {"name": "Tianyi Zhou"}], "author_detail": {"name": "Tianyi Zhou"}, "author": "Tianyi Zhou", "links": [{"href": "http://arxiv.org/abs/2303.08119v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2303.08119v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}