{"id": "http://arxiv.org/abs/2307.16789v1", "guidislink": true, "link": "http://arxiv.org/abs/2307.16789v1", "updated": "2023-07-31T15:56:53Z", "updated_parsed": [2023, 7, 31, 15, 56, 53, 0, 212, 0], "published": "2023-07-31T15:56:53Z", "published_parsed": [2023, 7, 31, 15, 56, 53, 0, 212, 0], "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world\n  APIs", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=toolllm++facilitating+large+language+models+to+master+16000++real+world+apis&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world\n  APIs"}, "summary": "Despite the advancements of open-source large language models (LLMs) and\ntheir variants, e.g., LLaMA and Vicuna, they remain significantly limited in\nperforming higher-level tasks, such as following human instructions to use\nexternal tools (APIs). This is because current instruction tuning largely\nfocuses on basic language tasks instead of the tool-use domain. This is in\ncontrast to state-of-the-art (SOTA) LLMs, e.g., ChatGPT, which have\ndemonstrated excellent tool-use capabilities but are unfortunately closed\nsource. To facilitate tool-use capabilities within open-source LLMs, we\nintroduce ToolLLM, a general tool-use framework of data construction, model\ntraining and evaluation. We first present ToolBench, an instruction-tuning\ndataset for tool use, which is created automatically using ChatGPT.\nSpecifically, we collect 16,464 real-world RESTful APIs spanning 49 categories\nfrom RapidAPI Hub, then prompt ChatGPT to generate diverse human instructions\ninvolving these APIs, covering both single-tool and multi-tool scenarios.\nFinally, we use ChatGPT to search for a valid solution path (chain of API\ncalls) for each instruction. To make the searching process more efficient, we\ndevelop a novel depth-first search-based decision tree (DFSDT), enabling LLMs\nto evaluate multiple reasoning traces and expand the search space. We show that\nDFSDT significantly enhances the planning and reasoning capabilities of LLMs.\nFor efficient tool-use assessment, we develop an automatic evaluator: ToolEval.\nWe fine-tune LLaMA on ToolBench and obtain ToolLLaMA. Our ToolEval reveals that\nToolLLaMA demonstrates a remarkable ability to execute complex instructions and\ngeneralize to unseen APIs, and exhibits comparable performance to ChatGPT. To\nmake the pipeline more practical, we devise a neural API retriever to recommend\nappropriate APIs for each instruction, negating the need for manual API\nselection.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=toolllm++facilitating+large+language+models+to+master+16000++real+world+apis&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Despite the advancements of open-source large language models (LLMs) and\ntheir variants, e.g., LLaMA and Vicuna, they remain significantly limited in\nperforming higher-level tasks, such as following human instructions to use\nexternal tools (APIs). This is because current instruction tuning largely\nfocuses on basic language tasks instead of the tool-use domain. This is in\ncontrast to state-of-the-art (SOTA) LLMs, e.g., ChatGPT, which have\ndemonstrated excellent tool-use capabilities but are unfortunately closed\nsource. To facilitate tool-use capabilities within open-source LLMs, we\nintroduce ToolLLM, a general tool-use framework of data construction, model\ntraining and evaluation. We first present ToolBench, an instruction-tuning\ndataset for tool use, which is created automatically using ChatGPT.\nSpecifically, we collect 16,464 real-world RESTful APIs spanning 49 categories\nfrom RapidAPI Hub, then prompt ChatGPT to generate diverse human instructions\ninvolving these APIs, covering both single-tool and multi-tool scenarios.\nFinally, we use ChatGPT to search for a valid solution path (chain of API\ncalls) for each instruction. To make the searching process more efficient, we\ndevelop a novel depth-first search-based decision tree (DFSDT), enabling LLMs\nto evaluate multiple reasoning traces and expand the search space. We show that\nDFSDT significantly enhances the planning and reasoning capabilities of LLMs.\nFor efficient tool-use assessment, we develop an automatic evaluator: ToolEval.\nWe fine-tune LLaMA on ToolBench and obtain ToolLLaMA. Our ToolEval reveals that\nToolLLaMA demonstrates a remarkable ability to execute complex instructions and\ngeneralize to unseen APIs, and exhibits comparable performance to ChatGPT. To\nmake the pipeline more practical, we devise a neural API retriever to recommend\nappropriate APIs for each instruction, negating the need for manual API\nselection."}, "authors": [{"name": "Yujia Qin"}, {"name": "Shihao Liang"}, {"name": "Yining Ye"}, {"name": "Kunlun Zhu"}, {"name": "Lan Yan"}, {"name": "Yaxi Lu"}, {"name": "Yankai Lin"}, {"name": "Xin Cong"}, {"name": "Xiangru Tang"}, {"name": "Bill Qian"}, {"name": "Sihan Zhao"}, {"name": "Runchu Tian"}, {"name": "Ruobing Xie"}, {"name": "Jie Zhou"}, {"name": "Mark Gerstein"}, {"name": "Dahai Li"}, {"name": "Zhiyuan Liu"}, {"name": "Maosong Sun"}], "author_detail": {"name": "Maosong Sun"}, "author": "Maosong Sun", "links": [{"href": "http://arxiv.org/abs/2307.16789v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.16789v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}