{"id": "http://arxiv.org/abs/2307.15771v1", "guidislink": true, "link": "http://arxiv.org/abs/2307.15771v1", "updated": "2023-07-28T19:13:26Z", "updated_parsed": [2023, 7, 28, 19, 13, 26, 4, 209, 0], "published": "2023-07-28T19:13:26Z", "published_parsed": [2023, 7, 28, 19, 13, 26, 4, 209, 0], "title": "The Hydra Effect: Emergent Self-repair in Language Model Computations", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=the+hydra+effect++emergent+self+repair+in+language+model+computations&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "The Hydra Effect: Emergent Self-repair in Language Model Computations"}, "summary": "We investigate the internal structure of language model computations using\ncausal analysis and demonstrate two motifs: (1) a form of adaptive computation\nwhere ablations of one attention layer of a language model cause another layer\nto compensate (which we term the Hydra effect) and (2) a counterbalancing\nfunction of late MLP layers that act to downregulate the maximum-likelihood\ntoken. Our ablation studies demonstrate that language model layers are\ntypically relatively loosely coupled (ablations to one layer only affect a\nsmall number of downstream layers). Surprisingly, these effects occur even in\nlanguage models trained without any form of dropout. We analyse these effects\nin the context of factual recall and consider their implications for\ncircuit-level attribution in language models.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=the+hydra+effect++emergent+self+repair+in+language+model+computations&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "We investigate the internal structure of language model computations using\ncausal analysis and demonstrate two motifs: (1) a form of adaptive computation\nwhere ablations of one attention layer of a language model cause another layer\nto compensate (which we term the Hydra effect) and (2) a counterbalancing\nfunction of late MLP layers that act to downregulate the maximum-likelihood\ntoken. Our ablation studies demonstrate that language model layers are\ntypically relatively loosely coupled (ablations to one layer only affect a\nsmall number of downstream layers). Surprisingly, these effects occur even in\nlanguage models trained without any form of dropout. We analyse these effects\nin the context of factual recall and consider their implications for\ncircuit-level attribution in language models."}, "authors": [{"name": "Thomas McGrath"}, {"name": "Matthew Rahtz"}, {"name": "Janos Kramar"}, {"name": "Vladimir Mikulik"}, {"name": "Shane Legg"}], "author_detail": {"name": "Shane Legg"}, "author": "Shane Legg", "links": [{"href": "http://arxiv.org/abs/2307.15771v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.15771v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}