{"id": "http://arxiv.org/abs/2211.09066v1", "guidislink": true, "link": "http://arxiv.org/abs/2211.09066v1", "updated": "2022-11-15T06:12:28Z", "updated_parsed": [2022, 11, 15, 6, 12, 28, 1, 319, 0], "published": "2022-11-15T06:12:28Z", "published_parsed": [2022, 11, 15, 6, 12, 28, 1, 319, 0], "title": "Teaching Algorithmic Reasoning via In-context Learning", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=teaching+algorithmic+reasoning+via+in+context+learning&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Teaching Algorithmic Reasoning via In-context Learning"}, "summary": "Large language models (LLMs) have shown increasing in-context learning\ncapabilities through scaling up model and data size. Despite this progress,\nLLMs are still unable to solve algorithmic reasoning problems. While providing\na rationale with the final answer has led to further improvements in multi-step\nreasoning problems, Anil et al. 2022 showed that even simple algorithmic\nreasoning tasks such as parity are far from solved. In this work, we identify\nand study four key stages for successfully teaching algorithmic reasoning to\nLLMs: (1) formulating algorithms as skills, (2) teaching multiple skills\nsimultaneously (skill accumulation), (3) teaching how to combine skills (skill\ncomposition) and (4) teaching how to use skills as tools. We show that it is\npossible to teach algorithmic reasoning to LLMs via in-context learning, which\nwe refer to as algorithmic prompting. We evaluate our approach on a variety of\narithmetic and quantitative reasoning tasks, and demonstrate significant boosts\nin performance over existing prompting techniques. In particular, for long\nparity, addition, multiplication and subtraction, we achieve an error reduction\nof approximately 10x, 9x, 5x and 2x respectively compared to the best available\nbaselines.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=teaching+algorithmic+reasoning+via+in+context+learning&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Large language models (LLMs) have shown increasing in-context learning\ncapabilities through scaling up model and data size. Despite this progress,\nLLMs are still unable to solve algorithmic reasoning problems. While providing\na rationale with the final answer has led to further improvements in multi-step\nreasoning problems, Anil et al. 2022 showed that even simple algorithmic\nreasoning tasks such as parity are far from solved. In this work, we identify\nand study four key stages for successfully teaching algorithmic reasoning to\nLLMs: (1) formulating algorithms as skills, (2) teaching multiple skills\nsimultaneously (skill accumulation), (3) teaching how to combine skills (skill\ncomposition) and (4) teaching how to use skills as tools. We show that it is\npossible to teach algorithmic reasoning to LLMs via in-context learning, which\nwe refer to as algorithmic prompting. We evaluate our approach on a variety of\narithmetic and quantitative reasoning tasks, and demonstrate significant boosts\nin performance over existing prompting techniques. In particular, for long\nparity, addition, multiplication and subtraction, we achieve an error reduction\nof approximately 10x, 9x, 5x and 2x respectively compared to the best available\nbaselines."}, "authors": [{"name": "Hattie Zhou"}, {"name": "Azade Nova"}, {"name": "Hugo Larochelle"}, {"name": "Aaron Courville"}, {"name": "Behnam Neyshabur"}, {"name": "Hanie Sedghi"}], "author_detail": {"name": "Hanie Sedghi"}, "author": "Hanie Sedghi", "links": [{"href": "http://arxiv.org/abs/2211.09066v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2211.09066v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}