{"id": "http://arxiv.org/abs/2305.14540v1", "guidislink": true, "link": "http://arxiv.org/abs/2305.14540v1", "updated": "2023-05-23T21:50:06Z", "updated_parsed": [2023, 5, 23, 21, 50, 6, 1, 143, 0], "published": "2023-05-23T21:50:06Z", "published_parsed": [2023, 5, 23, 21, 50, 6, 1, 143, 0], "title": "LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=llms+as+factual+reasoners++insights+from+existing+benchmarks+and+beyond&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond"}, "summary": "With the recent appearance of LLMs in practical settings, having methods that\ncan effectively detect factual inconsistencies is crucial to reduce the\npropagation of misinformation and improve trust in model outputs. When testing\non existing factual consistency benchmarks, we find that a few large language\nmodels (LLMs) perform competitively on classification benchmarks for factual\ninconsistency detection compared to traditional non-LLM methods. However, a\ncloser analysis reveals that most LLMs fail on more complex formulations of the\ntask and exposes issues with existing evaluation benchmarks, affecting\nevaluation precision. To address this, we propose a new protocol for\ninconsistency detection benchmark creation and implement it in a 10-domain\nbenchmark called SummEdits. This new benchmark is 20 times more cost-effective\nper sample than previous benchmarks and highly reproducible, as we estimate\ninter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with\nperformance close to random chance. The best-performing model, GPT-4, is still\n8\\% below estimated human performance, highlighting the gaps in LLMs' ability\nto reason about facts and detect inconsistencies when they occur.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=llms+as+factual+reasoners++insights+from+existing+benchmarks+and+beyond&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "With the recent appearance of LLMs in practical settings, having methods that\ncan effectively detect factual inconsistencies is crucial to reduce the\npropagation of misinformation and improve trust in model outputs. When testing\non existing factual consistency benchmarks, we find that a few large language\nmodels (LLMs) perform competitively on classification benchmarks for factual\ninconsistency detection compared to traditional non-LLM methods. However, a\ncloser analysis reveals that most LLMs fail on more complex formulations of the\ntask and exposes issues with existing evaluation benchmarks, affecting\nevaluation precision. To address this, we propose a new protocol for\ninconsistency detection benchmark creation and implement it in a 10-domain\nbenchmark called SummEdits. This new benchmark is 20 times more cost-effective\nper sample than previous benchmarks and highly reproducible, as we estimate\ninter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with\nperformance close to random chance. The best-performing model, GPT-4, is still\n8\\% below estimated human performance, highlighting the gaps in LLMs' ability\nto reason about facts and detect inconsistencies when they occur."}, "authors": [{"name": "Philippe Laban"}, {"name": "Wojciech Kry\u015bci\u0144ski"}, {"name": "Divyansh Agarwal"}, {"name": "Alexander R. Fabbri"}, {"name": "Caiming Xiong"}, {"name": "Shafiq Joty"}, {"name": "Chien-Sheng Wu"}], "author_detail": {"name": "Chien-Sheng Wu"}, "author": "Chien-Sheng Wu", "links": [{"href": "http://arxiv.org/abs/2305.14540v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2305.14540v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}