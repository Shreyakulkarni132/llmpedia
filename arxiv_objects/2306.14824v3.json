{"id": "http://arxiv.org/abs/2306.14824v3", "guidislink": true, "link": "http://arxiv.org/abs/2306.14824v3", "updated": "2023-07-13T05:41:34Z", "updated_parsed": [2023, 7, 13, 5, 41, 34, 3, 194, 0], "published": "2023-06-26T16:32:47Z", "published_parsed": [2023, 6, 26, 16, 32, 47, 0, 177, 0], "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=kosmos+2++grounding+multimodal+large+language+models+to+the+world&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Kosmos-2: Grounding Multimodal Large Language Models to the World"}, "summary": "We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new\ncapabilities of perceiving object descriptions (e.g., bounding boxes) and\ngrounding text to the visual world. Specifically, we represent refer\nexpressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where\nobject descriptions are sequences of location tokens. Together with multimodal\ncorpora, we construct large-scale data of grounded image-text pairs (called\nGrIT) to train the model. In addition to the existing capabilities of MLLMs\n(e.g., perceiving general modalities, following instructions, and performing\nin-context learning), Kosmos-2 integrates the grounding capability into\ndownstream applications. We evaluate Kosmos-2 on a wide range of tasks,\nincluding (i) multimodal grounding, such as referring expression comprehension,\nand phrase grounding, (ii) multimodal referring, such as referring expression\ngeneration, (iii) perception-language tasks, and (iv) language understanding\nand generation. This work lays out the foundation for the development of\nEmbodiment AI and sheds light on the big convergence of language, multimodal\nperception, action, and world modeling, which is a key step toward artificial\ngeneral intelligence. Code and pretrained models are available at\nhttps://aka.ms/kosmos-2.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=kosmos+2++grounding+multimodal+large+language+models+to+the+world&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new\ncapabilities of perceiving object descriptions (e.g., bounding boxes) and\ngrounding text to the visual world. Specifically, we represent refer\nexpressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where\nobject descriptions are sequences of location tokens. Together with multimodal\ncorpora, we construct large-scale data of grounded image-text pairs (called\nGrIT) to train the model. In addition to the existing capabilities of MLLMs\n(e.g., perceiving general modalities, following instructions, and performing\nin-context learning), Kosmos-2 integrates the grounding capability into\ndownstream applications. We evaluate Kosmos-2 on a wide range of tasks,\nincluding (i) multimodal grounding, such as referring expression comprehension,\nand phrase grounding, (ii) multimodal referring, such as referring expression\ngeneration, (iii) perception-language tasks, and (iv) language understanding\nand generation. This work lays out the foundation for the development of\nEmbodiment AI and sheds light on the big convergence of language, multimodal\nperception, action, and world modeling, which is a key step toward artificial\ngeneral intelligence. Code and pretrained models are available at\nhttps://aka.ms/kosmos-2."}, "authors": [{"name": "Zhiliang Peng"}, {"name": "Wenhui Wang"}, {"name": "Li Dong"}, {"name": "Yaru Hao"}, {"name": "Shaohan Huang"}, {"name": "Shuming Ma"}, {"name": "Furu Wei"}], "author_detail": {"name": "Furu Wei"}, "author": "Furu Wei", "arxiv_comment": "20 pages", "links": [{"href": "http://arxiv.org/abs/2306.14824v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2306.14824v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}