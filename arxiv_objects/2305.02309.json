{"id": "http://arxiv.org/abs/2305.02309v2", "guidislink": true, "link": "http://arxiv.org/abs/2305.02309v2", "updated": "2023-07-11T21:11:23Z", "updated_parsed": [2023, 7, 11, 21, 11, 23, 1, 192, 0], "published": "2023-05-03T17:55:25Z", "published_parsed": [2023, 5, 3, 17, 55, 25, 2, 123, 0], "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=codegen2++lessons+for+training+llms+on+programming+and+natural+languages&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages"}, "summary": "Large language models (LLMs) have demonstrated remarkable abilities in\nrepresentation learning for program synthesis and understanding tasks. The\nquality of the learned representations appears to be dictated by the neural\nscaling laws as a function of the number of model parameters and observations,\nwhile imposing upper bounds on the model performance by the amount of available\ndata and compute, which is costly.\n  In this study, we attempt to render the training of LLMs for program\nsynthesis more efficient by unifying four key components: (1) model\narchitectures, (2) learning methods, (3) infill sampling, and, (4) data\ndistributions. Specifically, for the model architecture, we attempt to unify\nencoder and decoder-based models into a single prefix-LM. For learning methods,\n(i) causal language modeling, (ii) span corruption, (iii) infilling are unified\ninto a simple learning algorithm. For infill sampling, we explore the claim of\na \"free lunch\" hypothesis. For data distributions, the effect of a mixture\ndistribution and multi-epoch training of programming and natural languages on\nmodel performance is explored.\n  We conduct a comprehensive series of empirical experiments on 1B LLMs, for\nwhich failures and successes of this exploration are distilled into five\nlessons. We will provide a final recipe for training and release CodeGen2\nmodels in size 1B, 3.7B, 7B, and, 16B parameters, along with the training\nframework as open-source: https://github.com/salesforce/CodeGen.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=codegen2++lessons+for+training+llms+on+programming+and+natural+languages&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Large language models (LLMs) have demonstrated remarkable abilities in\nrepresentation learning for program synthesis and understanding tasks. The\nquality of the learned representations appears to be dictated by the neural\nscaling laws as a function of the number of model parameters and observations,\nwhile imposing upper bounds on the model performance by the amount of available\ndata and compute, which is costly.\n  In this study, we attempt to render the training of LLMs for program\nsynthesis more efficient by unifying four key components: (1) model\narchitectures, (2) learning methods, (3) infill sampling, and, (4) data\ndistributions. Specifically, for the model architecture, we attempt to unify\nencoder and decoder-based models into a single prefix-LM. For learning methods,\n(i) causal language modeling, (ii) span corruption, (iii) infilling are unified\ninto a simple learning algorithm. For infill sampling, we explore the claim of\na \"free lunch\" hypothesis. For data distributions, the effect of a mixture\ndistribution and multi-epoch training of programming and natural languages on\nmodel performance is explored.\n  We conduct a comprehensive series of empirical experiments on 1B LLMs, for\nwhich failures and successes of this exploration are distilled into five\nlessons. We will provide a final recipe for training and release CodeGen2\nmodels in size 1B, 3.7B, 7B, and, 16B parameters, along with the training\nframework as open-source: https://github.com/salesforce/CodeGen."}, "authors": [{"name": "Erik Nijkamp"}, {"name": "Hiroaki Hayashi"}, {"name": "Caiming Xiong"}, {"name": "Silvio Savarese"}, {"name": "Yingbo Zhou"}], "author_detail": {"name": "Yingbo Zhou"}, "author": "Yingbo Zhou", "links": [{"href": "http://arxiv.org/abs/2305.02309v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2305.02309v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}