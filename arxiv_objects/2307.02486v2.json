{"id": "http://arxiv.org/abs/2307.02486v2", "guidislink": true, "link": "http://arxiv.org/abs/2307.02486v2", "updated": "2023-07-19T12:25:35Z", "updated_parsed": [2023, 7, 19, 12, 25, 35, 2, 200, 0], "published": "2023-07-05T17:59:38Z", "published_parsed": [2023, 7, 5, 17, 59, 38, 2, 186, 0], "title": "LongNet: Scaling Transformers to 1,000,000,000 Tokens", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=longnet++scaling+transformers+to+1+000+000+000+tokens&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "LongNet: Scaling Transformers to 1,000,000,000 Tokens"}, "summary": "Scaling sequence length has become a critical demand in the era of large\nlanguage models. However, existing methods struggle with either computational\ncomplexity or model expressivity, rendering the maximum sequence length\nrestricted. To address this issue, we introduce LongNet, a Transformer variant\nthat can scale sequence length to more than 1 billion tokens, without\nsacrificing the performance on shorter sequences. Specifically, we propose\ndilated attention, which expands the attentive field exponentially as the\ndistance grows. LongNet has significant advantages: 1) it has a linear\ncomputation complexity and a logarithm dependency between any two tokens in a\nsequence; 2) it can be served as a distributed trainer for extremely long\nsequences; 3) its dilated attention is a drop-in replacement for standard\nattention, which can be seamlessly integrated with the existing\nTransformer-based optimization. Experiments results demonstrate that LongNet\nyields strong performance on both long-sequence modeling and general language\ntasks. Our work opens up new possibilities for modeling very long sequences,\ne.g., treating a whole corpus or even the entire Internet as a sequence.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=longnet++scaling+transformers+to+1+000+000+000+tokens&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Scaling sequence length has become a critical demand in the era of large\nlanguage models. However, existing methods struggle with either computational\ncomplexity or model expressivity, rendering the maximum sequence length\nrestricted. To address this issue, we introduce LongNet, a Transformer variant\nthat can scale sequence length to more than 1 billion tokens, without\nsacrificing the performance on shorter sequences. Specifically, we propose\ndilated attention, which expands the attentive field exponentially as the\ndistance grows. LongNet has significant advantages: 1) it has a linear\ncomputation complexity and a logarithm dependency between any two tokens in a\nsequence; 2) it can be served as a distributed trainer for extremely long\nsequences; 3) its dilated attention is a drop-in replacement for standard\nattention, which can be seamlessly integrated with the existing\nTransformer-based optimization. Experiments results demonstrate that LongNet\nyields strong performance on both long-sequence modeling and general language\ntasks. Our work opens up new possibilities for modeling very long sequences,\ne.g., treating a whole corpus or even the entire Internet as a sequence."}, "authors": [{"name": "Jiayu Ding"}, {"name": "Shuming Ma"}, {"name": "Li Dong"}, {"name": "Xingxing Zhang"}, {"name": "Shaohan Huang"}, {"name": "Wenhui Wang"}, {"name": "Nanning Zheng"}, {"name": "Furu Wei"}], "author_detail": {"name": "Furu Wei"}, "author": "Furu Wei", "arxiv_comment": "Work in progress", "links": [{"href": "http://arxiv.org/abs/2307.02486v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.02486v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}