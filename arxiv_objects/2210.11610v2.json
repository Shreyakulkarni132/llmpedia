{"id": "http://arxiv.org/abs/2210.11610v2", "guidislink": true, "link": "http://arxiv.org/abs/2210.11610v2", "updated": "2022-10-25T17:45:17Z", "updated_parsed": [2022, 10, 25, 17, 45, 17, 1, 298, 0], "published": "2022-10-20T21:53:54Z", "published_parsed": [2022, 10, 20, 21, 53, 54, 3, 293, 0], "title": "Large Language Models Can Self-Improve", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=large+language+models+can+self+improve&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Large Language Models Can Self-Improve"}, "summary": "Large Language Models (LLMs) have achieved excellent performances in various\ntasks. However, fine-tuning an LLM requires extensive supervision. Human, on\nthe other hand, may improve their reasoning abilities by self-thinking without\nexternal inputs. In this work, we demonstrate that an LLM is also capable of\nself-improving with only unlabeled datasets. We use a pre-trained LLM to\ngenerate \"high-confidence\" rationale-augmented answers for unlabeled questions\nusing Chain-of-Thought prompting and self-consistency, and fine-tune the LLM\nusing those self-generated solutions as target outputs. We show that our\napproach improves the general reasoning ability of a 540B-parameter LLM\n(74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and\n63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance,\nwithout any ground truth label. We conduct ablation studies and show that\nfine-tuning on reasoning is critical for self-improvement.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=large+language+models+can+self+improve&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Large Language Models (LLMs) have achieved excellent performances in various\ntasks. However, fine-tuning an LLM requires extensive supervision. Human, on\nthe other hand, may improve their reasoning abilities by self-thinking without\nexternal inputs. In this work, we demonstrate that an LLM is also capable of\nself-improving with only unlabeled datasets. We use a pre-trained LLM to\ngenerate \"high-confidence\" rationale-augmented answers for unlabeled questions\nusing Chain-of-Thought prompting and self-consistency, and fine-tune the LLM\nusing those self-generated solutions as target outputs. We show that our\napproach improves the general reasoning ability of a 540B-parameter LLM\n(74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and\n63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance,\nwithout any ground truth label. We conduct ablation studies and show that\nfine-tuning on reasoning is critical for self-improvement."}, "authors": [{"name": "Jiaxin Huang"}, {"name": "Shixiang Shane Gu"}, {"name": "Le Hou"}, {"name": "Yuexin Wu"}, {"name": "Xuezhi Wang"}, {"name": "Hongkun Yu"}, {"name": "Jiawei Han"}], "author_detail": {"name": "Jiawei Han"}, "author": "Jiawei Han", "links": [{"href": "http://arxiv.org/abs/2210.11610v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2210.11610v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}