{"id": "http://arxiv.org/abs/2307.06018v1", "guidislink": true, "link": "http://arxiv.org/abs/2307.06018v1", "updated": "2023-07-12T09:00:37Z", "updated_parsed": [2023, 7, 12, 9, 0, 37, 2, 193, 0], "published": "2023-07-12T09:00:37Z", "published_parsed": [2023, 7, 12, 9, 0, 37, 2, 193, 0], "title": "PolyLM: An Open Source Polyglot Large Language Model", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=polylm++an+open+source+polyglot+large+language+model&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "PolyLM: An Open Source Polyglot Large Language Model"}, "summary": "Large language models (LLMs) demonstrate remarkable ability to comprehend,\nreason, and generate following nature language instructions. However, the\ndevelopment of LLMs has been primarily focused on high-resource languages, such\nas English, thereby limiting their applicability and research in other\nlanguages. Consequently, we present PolyLM, a multilingual LLM trained on 640\nbillion (B) tokens, avaliable in two model sizes: 1.7B and 13B. To enhance its\nmultilingual capabilities, we 1) integrate bilingual data into training data;\nand 2) adopt a curriculum learning strategy that increases the proportion of\nnon-English data from 30% in the first stage to 60% in the final stage during\npre-training. Further, we propose a multilingual self-instruct method which\nautomatically generates 132.7K diverse multilingual instructions for model\nfine-tuning. To assess the model's performance, we collect several existing\nmultilingual tasks, including multilingual understanding, question answering,\ngeneration, and translation. Extensive experiments show that PolyLM surpasses\nother open-source models such as LLaMA and BLOOM on multilingual tasks while\nmaintaining comparable performance in English. Our models, alone with the\ninstruction data and multilingual benchmark, are available at:\n\\url{https://modelscope.cn/models/damo/nlp_polylm_13b_text_generation}.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=polylm++an+open+source+polyglot+large+language+model&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Large language models (LLMs) demonstrate remarkable ability to comprehend,\nreason, and generate following nature language instructions. However, the\ndevelopment of LLMs has been primarily focused on high-resource languages, such\nas English, thereby limiting their applicability and research in other\nlanguages. Consequently, we present PolyLM, a multilingual LLM trained on 640\nbillion (B) tokens, avaliable in two model sizes: 1.7B and 13B. To enhance its\nmultilingual capabilities, we 1) integrate bilingual data into training data;\nand 2) adopt a curriculum learning strategy that increases the proportion of\nnon-English data from 30% in the first stage to 60% in the final stage during\npre-training. Further, we propose a multilingual self-instruct method which\nautomatically generates 132.7K diverse multilingual instructions for model\nfine-tuning. To assess the model's performance, we collect several existing\nmultilingual tasks, including multilingual understanding, question answering,\ngeneration, and translation. Extensive experiments show that PolyLM surpasses\nother open-source models such as LLaMA and BLOOM on multilingual tasks while\nmaintaining comparable performance in English. Our models, alone with the\ninstruction data and multilingual benchmark, are available at:\n\\url{https://modelscope.cn/models/damo/nlp_polylm_13b_text_generation}."}, "authors": [{"name": "Xiangpeng Wei"}, {"name": "Haoran Wei"}, {"name": "Huan Lin"}, {"name": "Tianhao Li"}, {"name": "Pei Zhang"}, {"name": "Xingzhang Ren"}, {"name": "Mei Li"}, {"name": "Yu Wan"}, {"name": "Zhiwei Cao"}, {"name": "Binbin Xie"}, {"name": "Tianxiang Hu"}, {"name": "Shangjie Li"}, {"name": "Binyuan Hui"}, {"name": "Bowen Yu"}, {"name": "Dayiheng Liu"}, {"name": "Baosong Yang"}, {"name": "Fei Huang"}, {"name": "Jun Xie"}], "author_detail": {"name": "Jun Xie"}, "author": "Jun Xie", "links": [{"href": "http://arxiv.org/abs/2307.06018v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.06018v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}