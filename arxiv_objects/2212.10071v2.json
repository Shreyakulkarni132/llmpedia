{"id": "http://arxiv.org/abs/2212.10071v2", "guidislink": true, "link": "http://arxiv.org/abs/2212.10071v2", "updated": "2023-06-13T10:55:29Z", "updated_parsed": [2023, 6, 13, 10, 55, 29, 1, 164, 0], "published": "2022-12-20T08:24:45Z", "published_parsed": [2022, 12, 20, 8, 24, 45, 1, 354, 0], "title": "Large Language Models Are Reasoning Teachers", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=large+language+models+are+reasoning+teachers&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Large Language Models Are Reasoning Teachers"}, "summary": "Recent works have shown that chain-of-thought (CoT) prompting can elicit\nlanguage models to solve complex reasoning tasks, step-by-step. However,\nprompt-based CoT methods are dependent on very large models such as GPT-3 175B\nwhich are prohibitive to deploy at scale. In this paper, we use these large\nmodels as reasoning teachers to enable complex reasoning in smaller models and\nreduce model size requirements by several orders of magnitude. We propose\nFine-tune-CoT, a method that generates reasoning samples from very large\nteacher models to fine-tune smaller models. We evaluate our method on a wide\nrange of public models and complex tasks. We find that Fine-tune-CoT enables\nsubstantial reasoning capability in small models, far outperforming\nprompt-based baselines and even the teacher model in many tasks. Additionally,\nwe extend our method by leveraging the teacher model's ability to generate\nmultiple distinct rationales for each original sample. Enriching the\nfine-tuning data with such diverse reasoning results in a substantial\nperformance boost across datasets, even for very small models. We conduct\nablations and sample studies to understand the emergence of reasoning\ncapabilities of student models. Our code implementation and data are available\nat https://github.com/itsnamgyu/reasoning-teacher.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=large+language+models+are+reasoning+teachers&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Recent works have shown that chain-of-thought (CoT) prompting can elicit\nlanguage models to solve complex reasoning tasks, step-by-step. However,\nprompt-based CoT methods are dependent on very large models such as GPT-3 175B\nwhich are prohibitive to deploy at scale. In this paper, we use these large\nmodels as reasoning teachers to enable complex reasoning in smaller models and\nreduce model size requirements by several orders of magnitude. We propose\nFine-tune-CoT, a method that generates reasoning samples from very large\nteacher models to fine-tune smaller models. We evaluate our method on a wide\nrange of public models and complex tasks. We find that Fine-tune-CoT enables\nsubstantial reasoning capability in small models, far outperforming\nprompt-based baselines and even the teacher model in many tasks. Additionally,\nwe extend our method by leveraging the teacher model's ability to generate\nmultiple distinct rationales for each original sample. Enriching the\nfine-tuning data with such diverse reasoning results in a substantial\nperformance boost across datasets, even for very small models. We conduct\nablations and sample studies to understand the emergence of reasoning\ncapabilities of student models. Our code implementation and data are available\nat https://github.com/itsnamgyu/reasoning-teacher."}, "authors": [{"name": "Namgyu Ho"}, {"name": "Laura Schmid"}, {"name": "Se-Young Yun"}], "author_detail": {"name": "Se-Young Yun"}, "author": "Se-Young Yun", "arxiv_comment": "ACL 2023 camera-ready", "links": [{"href": "http://arxiv.org/abs/2212.10071v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2212.10071v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}