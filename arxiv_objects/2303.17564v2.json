{"id": "http://arxiv.org/abs/2303.17564v2", "guidislink": true, "link": "http://arxiv.org/abs/2303.17564v2", "updated": "2023-05-09T16:06:35Z", "updated_parsed": [2023, 5, 9, 16, 6, 35, 1, 129, 0], "published": "2023-03-30T17:30:36Z", "published_parsed": [2023, 3, 30, 17, 30, 36, 3, 89, 0], "title": "BloombergGPT: A Large Language Model for Finance", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bloomberggpt++a+large+language+model+for+finance&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "BloombergGPT: A Large Language Model for Finance"}, "summary": "The use of NLP in the realm of financial technology is broad and complex,\nwith applications ranging from sentiment analysis and named entity recognition\nto question answering. Large Language Models (LLMs) have been shown to be\neffective on a variety of tasks; however, no LLM specialized for the financial\ndomain has been reported in literature. In this work, we present BloombergGPT,\na 50 billion parameter language model that is trained on a wide range of\nfinancial data. We construct a 363 billion token dataset based on Bloomberg's\nextensive data sources, perhaps the largest domain-specific dataset yet,\naugmented with 345 billion tokens from general purpose datasets. We validate\nBloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite\nof internal benchmarks that most accurately reflect our intended usage. Our\nmixed dataset training leads to a model that outperforms existing models on\nfinancial tasks by significant margins without sacrificing performance on\ngeneral LLM benchmarks. Additionally, we explain our modeling choices, training\nprocess, and evaluation methodology. We release Training Chronicles (Appendix\nC) detailing our experience in training BloombergGPT.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bloomberggpt++a+large+language+model+for+finance&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "The use of NLP in the realm of financial technology is broad and complex,\nwith applications ranging from sentiment analysis and named entity recognition\nto question answering. Large Language Models (LLMs) have been shown to be\neffective on a variety of tasks; however, no LLM specialized for the financial\ndomain has been reported in literature. In this work, we present BloombergGPT,\na 50 billion parameter language model that is trained on a wide range of\nfinancial data. We construct a 363 billion token dataset based on Bloomberg's\nextensive data sources, perhaps the largest domain-specific dataset yet,\naugmented with 345 billion tokens from general purpose datasets. We validate\nBloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite\nof internal benchmarks that most accurately reflect our intended usage. Our\nmixed dataset training leads to a model that outperforms existing models on\nfinancial tasks by significant margins without sacrificing performance on\ngeneral LLM benchmarks. Additionally, we explain our modeling choices, training\nprocess, and evaluation methodology. We release Training Chronicles (Appendix\nC) detailing our experience in training BloombergGPT."}, "authors": [{"name": "Shijie Wu"}, {"name": "Ozan Irsoy"}, {"name": "Steven Lu"}, {"name": "Vadim Dabravolski"}, {"name": "Mark Dredze"}, {"name": "Sebastian Gehrmann"}, {"name": "Prabhanjan Kambadur"}, {"name": "David Rosenberg"}, {"name": "Gideon Mann"}], "author_detail": {"name": "Gideon Mann"}, "author": "Gideon Mann", "arxiv_comment": "Updated to include Training Chronicles (Appendix C)", "links": [{"href": "http://arxiv.org/abs/2303.17564v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2303.17564v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "q-fin.GN", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}