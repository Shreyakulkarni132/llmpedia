{"id": "http://arxiv.org/abs/2301.11916v2", "guidislink": true, "link": "http://arxiv.org/abs/2301.11916v2", "updated": "2023-05-04T15:09:50Z", "updated_parsed": [2023, 5, 4, 15, 9, 50, 3, 124, 0], "published": "2023-01-27T18:59:01Z", "published_parsed": [2023, 1, 27, 18, 59, 1, 4, 27, 0], "title": "Large Language Models Are Implicitly Topic Models: Explaining and\n  Finding Good Demonstrations for In-Context Learning", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=large+language+models+are+implicitly+topic+models++explaining+and+finding+good+demonstrations+for+in+context+learning&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Large Language Models Are Implicitly Topic Models: Explaining and\n  Finding Good Demonstrations for In-Context Learning"}, "summary": "In recent years, pre-trained large language models have demonstrated\nremarkable efficiency in achieving an inference-time few-shot learning\ncapability known as in-context learning. However, existing literature has\nhighlighted the sensitivity of this capability to the selection of few-shot\ndemonstrations. The underlying mechanisms by which this capability arises from\nregular language model pretraining objectives remain poorly understood. In this\nstudy, we aim to examine the in-context learning phenomenon through a Bayesian\nlens, viewing large language models as topic models that implicitly infer\ntask-related information from demonstrations. On this premise, we propose an\nalgorithm for selecting optimal demonstrations from a set of annotated data and\ndemonstrate a significant 12.5% improvement relative to the random selection\nbaseline, averaged over eight GPT2 and GPT3 models on eight different\nreal-world text classification datasets. Our empirical findings support our\nhypothesis that large language models implicitly infer a latent concept\nvariable.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=large+language+models+are+implicitly+topic+models++explaining+and+finding+good+demonstrations+for+in+context+learning&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "In recent years, pre-trained large language models have demonstrated\nremarkable efficiency in achieving an inference-time few-shot learning\ncapability known as in-context learning. However, existing literature has\nhighlighted the sensitivity of this capability to the selection of few-shot\ndemonstrations. The underlying mechanisms by which this capability arises from\nregular language model pretraining objectives remain poorly understood. In this\nstudy, we aim to examine the in-context learning phenomenon through a Bayesian\nlens, viewing large language models as topic models that implicitly infer\ntask-related information from demonstrations. On this premise, we propose an\nalgorithm for selecting optimal demonstrations from a set of annotated data and\ndemonstrate a significant 12.5% improvement relative to the random selection\nbaseline, averaged over eight GPT2 and GPT3 models on eight different\nreal-world text classification datasets. Our empirical findings support our\nhypothesis that large language models implicitly infer a latent concept\nvariable."}, "authors": [{"name": "Xinyi Wang"}, {"name": "Wanrong Zhu"}, {"name": "Michael Saxon"}, {"name": "Mark Steyvers"}, {"name": "William Yang Wang"}], "author_detail": {"name": "William Yang Wang"}, "author": "William Yang Wang", "arxiv_comment": "code at:\n  https://github.com/WANGXinyiLinda/concept-based-demonstration-selection", "links": [{"href": "http://arxiv.org/abs/2301.11916v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2301.11916v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}