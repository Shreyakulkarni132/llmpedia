{"id": "http://arxiv.org/abs/2207.08143v3", "guidislink": true, "link": "http://arxiv.org/abs/2207.08143v3", "updated": "2023-01-24T12:23:48Z", "updated_parsed": [2023, 1, 24, 12, 23, 48, 1, 24, 0], "published": "2022-07-17T11:24:44Z", "published_parsed": [2022, 7, 17, 11, 24, 44, 6, 198, 0], "title": "Can large language models reason about medical questions?", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=can+large+language+models+reason+about+medical+questions+&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Can large language models reason about medical questions?"}, "summary": "Although large language models (LLMs) often produce impressive outputs, it\nremains unclear how they perform in real-world scenarios requiring strong\nreasoning skills and expert domain knowledge. We set out to investigate whether\nGPT-3.5 (Codex and InstructGPT) can be applied to answer and reason about\ndifficult real-world-based questions. We utilize two multiple-choice medical\nexam questions (USMLE and MedMCQA) and a medical reading comprehension dataset\n(PubMedQA). We investigate multiple prompting scenarios: Chain-of-Thought (CoT,\nthink step-by-step), zero- and few-shot (prepending the question with\nquestion-answer exemplars) and retrieval augmentation (injecting Wikipedia\npassages into the prompt). For a subset of the USMLE questions, a medical\nexpert reviewed and annotated the model's CoT. We found that InstructGPT can\noften read, reason and recall expert knowledge. Failure are primarily due to\nlack of knowledge and reasoning errors and trivial guessing heuristics are\nobserved, e.g.\\ too often predicting labels A and D on USMLE. Sampling and\ncombining many completions overcome some of these limitations. Using 100\nsamples, Codex 5-shot CoT not only gives close to well-calibrated predictive\nprobability but also achieves human-level performances on the three datasets.\nUSMLE: 60.2%, MedMCQA: 62.7% and PubMedQA: 78.2%.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=can+large+language+models+reason+about+medical+questions+&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Although large language models (LLMs) often produce impressive outputs, it\nremains unclear how they perform in real-world scenarios requiring strong\nreasoning skills and expert domain knowledge. We set out to investigate whether\nGPT-3.5 (Codex and InstructGPT) can be applied to answer and reason about\ndifficult real-world-based questions. We utilize two multiple-choice medical\nexam questions (USMLE and MedMCQA) and a medical reading comprehension dataset\n(PubMedQA). We investigate multiple prompting scenarios: Chain-of-Thought (CoT,\nthink step-by-step), zero- and few-shot (prepending the question with\nquestion-answer exemplars) and retrieval augmentation (injecting Wikipedia\npassages into the prompt). For a subset of the USMLE questions, a medical\nexpert reviewed and annotated the model's CoT. We found that InstructGPT can\noften read, reason and recall expert knowledge. Failure are primarily due to\nlack of knowledge and reasoning errors and trivial guessing heuristics are\nobserved, e.g.\\ too often predicting labels A and D on USMLE. Sampling and\ncombining many completions overcome some of these limitations. Using 100\nsamples, Codex 5-shot CoT not only gives close to well-calibrated predictive\nprobability but also achieves human-level performances on the three datasets.\nUSMLE: 60.2%, MedMCQA: 62.7% and PubMedQA: 78.2%."}, "authors": [{"name": "Valentin Li\u00e9vin"}, {"name": "Christoffer Egeberg Hother"}, {"name": "Ole Winther"}], "author_detail": {"name": "Ole Winther"}, "author": "Ole Winther", "arxiv_comment": "33 pages, 6 figures, to be submitted. v1: results using InstructGPT,\n  v2: added the Codex experiments, v3: added the missing test MedMCQA results\n  for Codex 5-shot CoT and using k=100 samples", "links": [{"href": "http://arxiv.org/abs/2207.08143v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2207.08143v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "I.2.1; I.2.7", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}