{"id": "http://arxiv.org/abs/2307.06945v1", "guidislink": true, "link": "http://arxiv.org/abs/2307.06945v1", "updated": "2023-07-13T17:59:21Z", "updated_parsed": [2023, 7, 13, 17, 59, 21, 3, 194, 0], "published": "2023-07-13T17:59:21Z", "published_parsed": [2023, 7, 13, 17, 59, 21, 3, 194, 0], "title": "In-context Autoencoder for Context Compression in a Large Language Model", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=in+context+autoencoder+for+context+compression+in+a+large+language+model&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "In-context Autoencoder for Context Compression in a Large Language Model"}, "summary": "We propose the In-context Autoencoder (ICAE) for context compression in a\nlarge language model (LLM). The ICAE has two modules: a learnable encoder\nadapted with LoRA from an LLM for compressing a long context into a limited\nnumber of memory slots, and a fixed decoder which is the target LLM that can\ncondition on the memory slots for various purposes. We first pretrain the ICAE\nusing both autoencoding and language modeling objectives on massive text data,\nenabling it to generate memory slots that accurately and comprehensively\nrepresent the original context. Then, we fine-tune the pretrained ICAE on a\nsmall amount of instruct data to enhance its interaction with various prompts\nfor producing desirable responses. Our experimental results demonstrate that\nthe ICAE learned with our proposed pretraining and fine-tuning paradigm can\neffectively produce memory slots with $4\\times$ context compression, which can\nbe well conditioned on by the target LLM to respond to various prompts. The\npromising results demonstrate significant implications of the ICAE for its\nnovel approach to the long context problem and its potential to reduce\ncomputation and memory overheads for LLM inference in practice, suggesting\nfurther research effort in context management for an LLM. Our code and data\nwill be released shortly.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=in+context+autoencoder+for+context+compression+in+a+large+language+model&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "We propose the In-context Autoencoder (ICAE) for context compression in a\nlarge language model (LLM). The ICAE has two modules: a learnable encoder\nadapted with LoRA from an LLM for compressing a long context into a limited\nnumber of memory slots, and a fixed decoder which is the target LLM that can\ncondition on the memory slots for various purposes. We first pretrain the ICAE\nusing both autoencoding and language modeling objectives on massive text data,\nenabling it to generate memory slots that accurately and comprehensively\nrepresent the original context. Then, we fine-tune the pretrained ICAE on a\nsmall amount of instruct data to enhance its interaction with various prompts\nfor producing desirable responses. Our experimental results demonstrate that\nthe ICAE learned with our proposed pretraining and fine-tuning paradigm can\neffectively produce memory slots with $4\\times$ context compression, which can\nbe well conditioned on by the target LLM to respond to various prompts. The\npromising results demonstrate significant implications of the ICAE for its\nnovel approach to the long context problem and its potential to reduce\ncomputation and memory overheads for LLM inference in practice, suggesting\nfurther research effort in context management for an LLM. Our code and data\nwill be released shortly."}, "authors": [{"name": "Tao Ge"}, {"name": "Jing Hu"}, {"name": "Xun Wang"}, {"name": "Si-Qing Chen"}, {"name": "Furu Wei"}], "author_detail": {"name": "Furu Wei"}, "author": "Furu Wei", "arxiv_comment": "Work in progress", "links": [{"href": "http://arxiv.org/abs/2307.06945v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.06945v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}