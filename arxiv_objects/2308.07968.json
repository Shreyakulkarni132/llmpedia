{"id": "http://arxiv.org/abs/2308.07968v1", "guidislink": true, "link": "http://arxiv.org/abs/2308.07968v1", "updated": "2023-08-15T18:06:23Z", "updated_parsed": [2023, 8, 15, 18, 6, 23, 1, 227, 0], "published": "2023-08-15T18:06:23Z", "published_parsed": [2023, 8, 15, 18, 6, 23, 1, 227, 0], "title": "Teach LLMs to Personalize -- An Approach inspired by Writing Education", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=teach+llms+to+personalize++++an+approach+inspired+by+writing+education&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Teach LLMs to Personalize -- An Approach inspired by Writing Education"}, "summary": "Personalized text generation is an emerging research area that has attracted\nmuch attention in recent years. Most studies in this direction focus on a\nparticular domain by designing bespoke features or models. In this work, we\npropose a general approach for personalized text generation using large\nlanguage models (LLMs). Inspired by the practice of writing education, we\ndevelop a multistage and multitask framework to teach LLMs for personalized\ngeneration. In writing instruction, the task of writing from sources is often\ndecomposed into multiple steps that involve finding, evaluating, summarizing,\nsynthesizing, and integrating information. Analogously, our approach to\npersonalized text generation consists of multiple stages: retrieval, ranking,\nsummarization, synthesis, and generation. In addition, we introduce a multitask\nsetting that helps the model improve its generation ability further, which is\ninspired by the observation in education that a student's reading proficiency\nand writing ability are often correlated. We evaluate our approach on three\npublic datasets, each of which covers a different and representative domain.\nOur results show significant improvements over a variety of baselines.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=teach+llms+to+personalize++++an+approach+inspired+by+writing+education&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Personalized text generation is an emerging research area that has attracted\nmuch attention in recent years. Most studies in this direction focus on a\nparticular domain by designing bespoke features or models. In this work, we\npropose a general approach for personalized text generation using large\nlanguage models (LLMs). Inspired by the practice of writing education, we\ndevelop a multistage and multitask framework to teach LLMs for personalized\ngeneration. In writing instruction, the task of writing from sources is often\ndecomposed into multiple steps that involve finding, evaluating, summarizing,\nsynthesizing, and integrating information. Analogously, our approach to\npersonalized text generation consists of multiple stages: retrieval, ranking,\nsummarization, synthesis, and generation. In addition, we introduce a multitask\nsetting that helps the model improve its generation ability further, which is\ninspired by the observation in education that a student's reading proficiency\nand writing ability are often correlated. We evaluate our approach on three\npublic datasets, each of which covers a different and representative domain.\nOur results show significant improvements over a variety of baselines."}, "authors": [{"name": "Cheng Li"}, {"name": "Mingyang Zhang"}, {"name": "Qiaozhu Mei"}, {"name": "Yaqing Wang"}, {"name": "Spurthi Amba Hombaiah"}, {"name": "Yi Liang"}, {"name": "Michael Bendersky"}], "author_detail": {"name": "Michael Bendersky"}, "author": "Michael Bendersky", "links": [{"href": "http://arxiv.org/abs/2308.07968v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2308.07968v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}