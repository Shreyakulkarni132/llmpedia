{"id": "http://arxiv.org/abs/2303.07678v1", "guidislink": true, "link": "http://arxiv.org/abs/2303.07678v1", "updated": "2023-03-14T07:27:30Z", "updated_parsed": [2023, 3, 14, 7, 27, 30, 1, 73, 0], "published": "2023-03-14T07:27:30Z", "published_parsed": [2023, 3, 14, 7, 27, 30, 1, 73, 0], "title": "Query2doc: Query Expansion with Large Language Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=query2doc++query+expansion+with+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Query2doc: Query Expansion with Large Language Models"}, "summary": "This paper introduces a simple yet effective query expansion approach,\ndenoted as query2doc, to improve both sparse and dense retrieval systems. The\nproposed method first generates pseudo-documents by few-shot prompting large\nlanguage models (LLMs), and then expands the query with generated\npseudo-documents. LLMs are trained on web-scale text corpora and are adept at\nknowledge memorization. The pseudo-documents from LLMs often contain highly\nrelevant information that can aid in query disambiguation and guide the\nretrievers. Experimental results demonstrate that query2doc boosts the\nperformance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and\nTREC DL, without any model fine-tuning. Furthermore, our method also benefits\nstate-of-the-art dense retrievers in terms of both in-domain and out-of-domain\nresults.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=query2doc++query+expansion+with+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "This paper introduces a simple yet effective query expansion approach,\ndenoted as query2doc, to improve both sparse and dense retrieval systems. The\nproposed method first generates pseudo-documents by few-shot prompting large\nlanguage models (LLMs), and then expands the query with generated\npseudo-documents. LLMs are trained on web-scale text corpora and are adept at\nknowledge memorization. The pseudo-documents from LLMs often contain highly\nrelevant information that can aid in query disambiguation and guide the\nretrievers. Experimental results demonstrate that query2doc boosts the\nperformance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and\nTREC DL, without any model fine-tuning. Furthermore, our method also benefits\nstate-of-the-art dense retrievers in terms of both in-domain and out-of-domain\nresults."}, "authors": [{"name": "Liang Wang"}, {"name": "Nan Yang"}, {"name": "Furu Wei"}], "author_detail": {"name": "Furu Wei"}, "author": "Furu Wei", "arxiv_comment": "9 pages", "links": [{"href": "http://arxiv.org/abs/2303.07678v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2303.07678v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}