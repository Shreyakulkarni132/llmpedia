{"id": "http://arxiv.org/abs/2308.03688v1", "guidislink": true, "link": "http://arxiv.org/abs/2308.03688v1", "updated": "2023-08-07T16:08:11Z", "updated_parsed": [2023, 8, 7, 16, 8, 11, 0, 219, 0], "published": "2023-08-07T16:08:11Z", "published_parsed": [2023, 8, 7, 16, 8, 11, 0, 219, 0], "title": "AgentBench: Evaluating LLMs as Agents", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=agentbench++evaluating+llms+as+agents&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "AgentBench: Evaluating LLMs as Agents"}, "summary": "Large Language Models (LLMs) are becoming increasingly smart and autonomous,\ntargeting real-world pragmatic missions beyond traditional NLP tasks. As a\nresult, there has been an urgent need to evaluate LLMs as agents on challenging\ntasks in interactive environments. We present AgentBench, a multi-dimensional\nevolving benchmark that currently consists of 8 distinct environments to assess\nLLM-as-Agent's reasoning and decision-making abilities in a multi-turn\nopen-ended generation setting. Our extensive test over 25 LLMs (including APIs\nand open-sourced models) shows that, while top commercial LLMs present a strong\nability of acting as agents in complex environments, there is a significant\ndisparity in performance between them and open-sourced competitors. It also\nserves as a component of an ongoing project with wider coverage and deeper\nconsideration towards systematic LLM evaluation. Datasets, environments, and an\nintegrated evaluation package for AgentBench are released at\nhttps://github.com/THUDM/AgentBench", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=agentbench++evaluating+llms+as+agents&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Large Language Models (LLMs) are becoming increasingly smart and autonomous,\ntargeting real-world pragmatic missions beyond traditional NLP tasks. As a\nresult, there has been an urgent need to evaluate LLMs as agents on challenging\ntasks in interactive environments. We present AgentBench, a multi-dimensional\nevolving benchmark that currently consists of 8 distinct environments to assess\nLLM-as-Agent's reasoning and decision-making abilities in a multi-turn\nopen-ended generation setting. Our extensive test over 25 LLMs (including APIs\nand open-sourced models) shows that, while top commercial LLMs present a strong\nability of acting as agents in complex environments, there is a significant\ndisparity in performance between them and open-sourced competitors. It also\nserves as a component of an ongoing project with wider coverage and deeper\nconsideration towards systematic LLM evaluation. Datasets, environments, and an\nintegrated evaluation package for AgentBench are released at\nhttps://github.com/THUDM/AgentBench"}, "authors": [{"name": "Xiao Liu"}, {"name": "Hao Yu"}, {"name": "Hanchen Zhang"}, {"name": "Yifan Xu"}, {"name": "Xuanyu Lei"}, {"name": "Hanyu Lai"}, {"name": "Yu Gu"}, {"name": "Hangliang Ding"}, {"name": "Kaiwen Men"}, {"name": "Kejuan Yang"}, {"name": "Shudan Zhang"}, {"name": "Xiang Deng"}, {"name": "Aohan Zeng"}, {"name": "Zhengxiao Du"}, {"name": "Chenhui Zhang"}, {"name": "Sheng Shen"}, {"name": "Tianjun Zhang"}, {"name": "Yu Su"}, {"name": "Huan Sun"}, {"name": "Minlie Huang"}, {"name": "Yuxiao Dong"}, {"name": "Jie Tang"}], "author_detail": {"name": "Jie Tang"}, "author": "Jie Tang", "arxiv_comment": "38 pages", "links": [{"href": "http://arxiv.org/abs/2308.03688v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2308.03688v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}