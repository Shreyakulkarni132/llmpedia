{"id": "http://arxiv.org/abs/2305.08596v2", "guidislink": true, "link": "http://arxiv.org/abs/2305.08596v2", "updated": "2023-05-18T05:02:29Z", "updated_parsed": [2023, 5, 18, 5, 2, 29, 3, 138, 0], "published": "2023-05-15T12:23:10Z", "published_parsed": [2023, 5, 15, 12, 23, 10, 0, 135, 0], "title": "DarkBERT: A Language Model for the Dark Side of the Internet", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=darkbert++a+language+model+for+the+dark+side+of+the+internet&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "DarkBERT: A Language Model for the Dark Side of the Internet"}, "summary": "Recent research has suggested that there are clear differences in the\nlanguage used in the Dark Web compared to that of the Surface Web. As studies\non the Dark Web commonly require textual analysis of the domain, language\nmodels specific to the Dark Web may provide valuable insights to researchers.\nIn this work, we introduce DarkBERT, a language model pretrained on Dark Web\ndata. We describe the steps taken to filter and compile the text data used to\ntrain DarkBERT to combat the extreme lexical and structural diversity of the\nDark Web that may be detrimental to building a proper representation of the\ndomain. We evaluate DarkBERT and its vanilla counterpart along with other\nwidely used language models to validate the benefits that a Dark Web domain\nspecific model offers in various use cases. Our evaluations show that DarkBERT\noutperforms current language models and may serve as a valuable resource for\nfuture research on the Dark Web.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=darkbert++a+language+model+for+the+dark+side+of+the+internet&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Recent research has suggested that there are clear differences in the\nlanguage used in the Dark Web compared to that of the Surface Web. As studies\non the Dark Web commonly require textual analysis of the domain, language\nmodels specific to the Dark Web may provide valuable insights to researchers.\nIn this work, we introduce DarkBERT, a language model pretrained on Dark Web\ndata. We describe the steps taken to filter and compile the text data used to\ntrain DarkBERT to combat the extreme lexical and structural diversity of the\nDark Web that may be detrimental to building a proper representation of the\ndomain. We evaluate DarkBERT and its vanilla counterpart along with other\nwidely used language models to validate the benefits that a Dark Web domain\nspecific model offers in various use cases. Our evaluations show that DarkBERT\noutperforms current language models and may serve as a valuable resource for\nfuture research on the Dark Web."}, "authors": [{"name": "Youngjin Jin"}, {"name": "Eugene Jang"}, {"name": "Jian Cui"}, {"name": "Jin-Woo Chung"}, {"name": "Yongjae Lee"}, {"name": "Seungwon Shin"}], "author_detail": {"name": "Seungwon Shin"}, "author": "Seungwon Shin", "arxiv_comment": "9 pages (main paper), 17 pages (including bibliography and appendix),\n  to appear at the ACL 2023 Main Conference", "links": [{"href": "http://arxiv.org/abs/2305.08596v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2305.08596v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "I.2.7", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}