{"id": "http://arxiv.org/abs/2306.06156v1", "guidislink": true, "link": "http://arxiv.org/abs/2306.06156v1", "updated": "2023-06-09T16:06:36Z", "updated_parsed": [2023, 6, 9, 16, 6, 36, 4, 160, 0], "published": "2023-06-09T16:06:36Z", "published_parsed": [2023, 6, 9, 16, 6, 36, 4, 160, 0], "title": "PoET: A generative model of protein families as sequences-of-sequences", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=poet++a+generative+model+of+protein+families+as+sequences+of+sequences&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "PoET: A generative model of protein families as sequences-of-sequences"}, "summary": "Generative protein language models are a natural way to design new proteins\nwith desired functions. However, current models are either difficult to direct\nto produce a protein from a specific family of interest, or must be trained on\na large multiple sequence alignment (MSA) from the specific family of interest,\nmaking them unable to benefit from transfer learning across families. To\naddress this, we propose $\\textbf{P}$r$\\textbf{o}$tein $\\textbf{E}$volutionary\n$\\textbf{T}$ransformer (PoET), an autoregressive generative model of whole\nprotein families that learns to generate sets of related proteins as\nsequences-of-sequences across tens of millions of natural protein sequence\nclusters. PoET can be used as a retrieval-augmented language model to generate\nand score arbitrary modifications conditioned on any protein family of\ninterest, and can extrapolate from short context lengths to generalize well\neven for small families. This is enabled by a unique Transformer layer; we\nmodel tokens sequentially within sequences while attending between sequences\norder invariantly, allowing PoET to scale to context lengths beyond those used\nduring training. PoET outperforms existing protein language models and\nevolutionary sequence models for variant function prediction in extensive\nexperiments on deep mutational scanning datasets, improving variant effect\nprediction across proteins of all MSA depths.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=poet++a+generative+model+of+protein+families+as+sequences+of+sequences&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Generative protein language models are a natural way to design new proteins\nwith desired functions. However, current models are either difficult to direct\nto produce a protein from a specific family of interest, or must be trained on\na large multiple sequence alignment (MSA) from the specific family of interest,\nmaking them unable to benefit from transfer learning across families. To\naddress this, we propose $\\textbf{P}$r$\\textbf{o}$tein $\\textbf{E}$volutionary\n$\\textbf{T}$ransformer (PoET), an autoregressive generative model of whole\nprotein families that learns to generate sets of related proteins as\nsequences-of-sequences across tens of millions of natural protein sequence\nclusters. PoET can be used as a retrieval-augmented language model to generate\nand score arbitrary modifications conditioned on any protein family of\ninterest, and can extrapolate from short context lengths to generalize well\neven for small families. This is enabled by a unique Transformer layer; we\nmodel tokens sequentially within sequences while attending between sequences\norder invariantly, allowing PoET to scale to context lengths beyond those used\nduring training. PoET outperforms existing protein language models and\nevolutionary sequence models for variant function prediction in extensive\nexperiments on deep mutational scanning datasets, improving variant effect\nprediction across proteins of all MSA depths."}, "authors": [{"name": "Timothy F. Truong Jr"}, {"name": "Tristan Bepler"}], "author_detail": {"name": "Tristan Bepler"}, "author": "Tristan Bepler", "links": [{"href": "http://arxiv.org/abs/2306.06156v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2306.06156v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "q-bio.QM", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "q-bio.QM", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}