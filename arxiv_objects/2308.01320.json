{"id": "http://arxiv.org/abs/2308.01320v1", "guidislink": true, "link": "http://arxiv.org/abs/2308.01320v1", "updated": "2023-08-02T18:49:57Z", "updated_parsed": [2023, 8, 2, 18, 49, 57, 2, 214, 0], "published": "2023-08-02T18:49:57Z", "published_parsed": [2023, 8, 2, 18, 49, 57, 2, 214, 0], "title": "DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like\n  Models at All Scales", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=deepspeed+chat++easy++fast+and+affordable+rlhf+training+of+chatgpt+like+models+at+all+scales&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like\n  Models at All Scales"}, "summary": "ChatGPT-like models have revolutionized various applications in artificial\nintelligence, from summarization and coding to translation, matching or even\nsurpassing human performance. However, the current landscape lacks an\naccessible, efficient, and cost-effective end-to-end RLHF (Reinforcement\nLearning with Human Feedback) training pipeline for these powerful models,\nparticularly when training at the scale of billions of parameters. This paper\nintroduces DeepSpeed-Chat, a novel system that democratizes RLHF training,\nmaking it accessible to the AI community. DeepSpeed-Chat offers three key\ncapabilities: an easy-to-use training and inference experience for ChatGPT-like\nmodels, a DeepSpeed-RLHF pipeline that replicates the training pipeline from\nInstructGPT, and a robust DeepSpeed-RLHF system that combines various\noptimizations for training and inference in a unified way. The system delivers\nunparalleled efficiency and scalability, enabling training of models with\nhundreds of billions of parameters in record time and at a fraction of the\ncost. With this development, DeepSpeed-Chat paves the way for broader access to\nadvanced RLHF training, even for data scientists with limited resources,\nthereby fostering innovation and further development in the field of AI.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=deepspeed+chat++easy++fast+and+affordable+rlhf+training+of+chatgpt+like+models+at+all+scales&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "ChatGPT-like models have revolutionized various applications in artificial\nintelligence, from summarization and coding to translation, matching or even\nsurpassing human performance. However, the current landscape lacks an\naccessible, efficient, and cost-effective end-to-end RLHF (Reinforcement\nLearning with Human Feedback) training pipeline for these powerful models,\nparticularly when training at the scale of billions of parameters. This paper\nintroduces DeepSpeed-Chat, a novel system that democratizes RLHF training,\nmaking it accessible to the AI community. DeepSpeed-Chat offers three key\ncapabilities: an easy-to-use training and inference experience for ChatGPT-like\nmodels, a DeepSpeed-RLHF pipeline that replicates the training pipeline from\nInstructGPT, and a robust DeepSpeed-RLHF system that combines various\noptimizations for training and inference in a unified way. The system delivers\nunparalleled efficiency and scalability, enabling training of models with\nhundreds of billions of parameters in record time and at a fraction of the\ncost. With this development, DeepSpeed-Chat paves the way for broader access to\nadvanced RLHF training, even for data scientists with limited resources,\nthereby fostering innovation and further development in the field of AI."}, "authors": [{"name": "Zhewei Yao"}, {"name": "Reza Yazdani Aminabadi"}, {"name": "Olatunji Ruwase"}, {"name": "Samyam Rajbhandari"}, {"name": "Xiaoxia Wu"}, {"name": "Ammar Ahmad Awan"}, {"name": "Jeff Rasley"}, {"name": "Minjia Zhang"}, {"name": "Conglong Li"}, {"name": "Connor Holmes"}, {"name": "Zhongzhu Zhou"}, {"name": "Michael Wyatt"}, {"name": "Molly Smith"}, {"name": "Lev Kurilenko"}, {"name": "Heyang Qin"}, {"name": "Masahiro Tanaka"}, {"name": "Shuai Che"}, {"name": "Shuaiwen Leon Song"}, {"name": "Yuxiong He"}], "author_detail": {"name": "Yuxiong He"}, "author": "Yuxiong He", "arxiv_comment": "14 pages, 7 figures", "links": [{"href": "http://arxiv.org/abs/2308.01320v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2308.01320v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}