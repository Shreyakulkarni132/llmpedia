{"id": "http://arxiv.org/abs/2306.12509v1", "guidislink": true, "link": "http://arxiv.org/abs/2306.12509v1", "updated": "2023-06-21T18:45:56Z", "updated_parsed": [2023, 6, 21, 18, 45, 56, 2, 172, 0], "published": "2023-06-21T18:45:56Z", "published_parsed": [2023, 6, 21, 18, 45, 56, 2, 172, 0], "title": "Deep Language Networks: Joint Prompt Training of Stacked LLMs using\n  Variational Inference", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=deep+language+networks++joint+prompt+training+of+stacked+llms+using+variational+inference&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Deep Language Networks: Joint Prompt Training of Stacked LLMs using\n  Variational Inference"}, "summary": "We view large language models (LLMs) as stochastic \\emph{language layers} in\na network, where the learnable parameters are the natural language\n\\emph{prompts} at each layer. We stack two such layers, feeding the output of\none layer to the next. We call the stacked architecture a \\emph{Deep Language\nNetwork} (DLN). We first show how to effectively perform prompt optimization\nfor a 1-Layer language network (DLN-1). We then show how to train 2-layer DLNs\n(DLN-2), where two prompts must be learnt. We consider the output of the first\nlayer as a latent variable to marginalize, and devise a variational inference\nalgorithm for joint prompt training. A DLN-2 reaches higher performance than a\nsingle layer, sometimes comparable to few-shot GPT-4 even when each LLM in the\nnetwork is smaller and less powerful. The DLN code is open source:\nhttps://github.com/microsoft/deep-language-networks .", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=deep+language+networks++joint+prompt+training+of+stacked+llms+using+variational+inference&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "We view large language models (LLMs) as stochastic \\emph{language layers} in\na network, where the learnable parameters are the natural language\n\\emph{prompts} at each layer. We stack two such layers, feeding the output of\none layer to the next. We call the stacked architecture a \\emph{Deep Language\nNetwork} (DLN). We first show how to effectively perform prompt optimization\nfor a 1-Layer language network (DLN-1). We then show how to train 2-layer DLNs\n(DLN-2), where two prompts must be learnt. We consider the output of the first\nlayer as a latent variable to marginalize, and devise a variational inference\nalgorithm for joint prompt training. A DLN-2 reaches higher performance than a\nsingle layer, sometimes comparable to few-shot GPT-4 even when each LLM in the\nnetwork is smaller and less powerful. The DLN code is open source:\nhttps://github.com/microsoft/deep-language-networks ."}, "authors": [{"name": "Alessandro Sordoni"}, {"name": "Xingdi Yuan"}, {"name": "Marc-Alexandre C\u00f4t\u00e9"}, {"name": "Matheus Pereira"}, {"name": "Adam Trischler"}, {"name": "Ziang Xiao"}, {"name": "Arian Hosseini"}, {"name": "Friederike Niedtner"}, {"name": "Nicolas Le Roux"}], "author_detail": {"name": "Nicolas Le Roux"}, "author": "Nicolas Le Roux", "links": [{"href": "http://arxiv.org/abs/2306.12509v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2306.12509v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}