{"id": "http://arxiv.org/abs/2110.08413v2", "guidislink": true, "link": "http://arxiv.org/abs/2110.08413v2", "updated": "2022-11-14T22:11:19Z", "updated_parsed": [2022, 11, 14, 22, 11, 19, 0, 318, 0], "published": "2021-10-16T00:03:19Z", "published_parsed": [2021, 10, 16, 0, 3, 19, 5, 289, 0], "title": "Invariant Language Modeling", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=invariant+language+modeling&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Invariant Language Modeling"}, "summary": "Large pretrained language models are critical components of modern NLP\npipelines. Yet, they suffer from spurious correlations, poor out-of-domain\ngeneralization, and biases. Inspired by recent progress in causal machine\nlearning, in particular the invariant risk minimization (IRM) paradigm, we\npropose invariant language modeling, a framework for learning invariant\nrepresentations that generalize better across multiple environments. In\nparticular, we adapt a game-theoretic formulation of IRM (IRM-games) to\nlanguage models, where the invariance emerges from a specific training schedule\nin which all the environments compete to optimize their own\nenvironment-specific loss by updating subsets of the model in a round-robin\nfashion. We focus on controlled experiments to precisely demonstrate the\nability of our method to (i) remove structured noise, (ii) ignore specific\nspurious correlations without affecting global performance, and (iii) achieve\nbetter out-of-domain generalization. These benefits come with a negligible\ncomputational overhead compared to standard training, do not require changing\nthe local loss, and can be applied to any language model. We believe this\nframework is promising to help mitigate spurious correlations and biases in\nlanguage models.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=invariant+language+modeling&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Large pretrained language models are critical components of modern NLP\npipelines. Yet, they suffer from spurious correlations, poor out-of-domain\ngeneralization, and biases. Inspired by recent progress in causal machine\nlearning, in particular the invariant risk minimization (IRM) paradigm, we\npropose invariant language modeling, a framework for learning invariant\nrepresentations that generalize better across multiple environments. In\nparticular, we adapt a game-theoretic formulation of IRM (IRM-games) to\nlanguage models, where the invariance emerges from a specific training schedule\nin which all the environments compete to optimize their own\nenvironment-specific loss by updating subsets of the model in a round-robin\nfashion. We focus on controlled experiments to precisely demonstrate the\nability of our method to (i) remove structured noise, (ii) ignore specific\nspurious correlations without affecting global performance, and (iii) achieve\nbetter out-of-domain generalization. These benefits come with a negligible\ncomputational overhead compared to standard training, do not require changing\nthe local loss, and can be applied to any language model. We believe this\nframework is promising to help mitigate spurious correlations and biases in\nlanguage models."}, "authors": [{"name": "Maxime Peyrard"}, {"name": "Sarvjeet Singh Ghotra"}, {"name": "Martin Josifoski"}, {"name": "Vidhan Agarwal"}, {"name": "Barun Patra"}, {"name": "Dean Carignan"}, {"name": "Emre Kiciman"}, {"name": "Robert West"}], "author_detail": {"name": "Robert West"}, "author": "Robert West", "arxiv_comment": "Published at EMNLP 2022", "links": [{"href": "http://arxiv.org/abs/2110.08413v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2110.08413v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}