{"id": "http://arxiv.org/abs/2212.08410v3", "guidislink": true, "link": "http://arxiv.org/abs/2212.08410v3", "updated": "2023-06-01T12:17:01Z", "updated_parsed": [2023, 6, 1, 12, 17, 1, 3, 152, 0], "published": "2022-12-16T11:24:42Z", "published_parsed": [2022, 12, 16, 11, 24, 42, 4, 350, 0], "title": "Teaching Small Language Models to Reason", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=teaching+small+language+models+to+reason&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Teaching Small Language Models to Reason"}, "summary": "Chain of thought prompting successfully improves the reasoning capabilities\nof large language models, achieving state of the art results on a range of\ndatasets. However, these reasoning capabilities only appear to emerge in models\nwith a size of over 100 billion parameters. In this paper, we explore the\ntransfer of such reasoning capabilities to models with less than 100 billion\nparameters via knowledge distillation. Specifically, we finetune a student\nmodel on the chain of thought outputs generated by a larger teacher model. Our\nexperiments show that the proposed method improves task performance across\narithmetic, commonsense and symbolic reasoning datasets. For example, the\naccuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned on\nPaLM-540B generated chains of thought.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=teaching+small+language+models+to+reason&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Chain of thought prompting successfully improves the reasoning capabilities\nof large language models, achieving state of the art results on a range of\ndatasets. However, these reasoning capabilities only appear to emerge in models\nwith a size of over 100 billion parameters. In this paper, we explore the\ntransfer of such reasoning capabilities to models with less than 100 billion\nparameters via knowledge distillation. Specifically, we finetune a student\nmodel on the chain of thought outputs generated by a larger teacher model. Our\nexperiments show that the proposed method improves task performance across\narithmetic, commonsense and symbolic reasoning datasets. For example, the\naccuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned on\nPaLM-540B generated chains of thought."}, "authors": [{"name": "Lucie Charlotte Magister"}, {"name": "Jonathan Mallinson"}, {"name": "Jakub Adamek"}, {"name": "Eric Malmi"}, {"name": "Aliaksei Severyn"}], "author_detail": {"name": "Aliaksei Severyn"}, "author": "Aliaksei Severyn", "links": [{"href": "http://arxiv.org/abs/2212.08410v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2212.08410v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}