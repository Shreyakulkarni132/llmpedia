{"id": "http://arxiv.org/abs/2306.12420v1", "guidislink": true, "link": "http://arxiv.org/abs/2306.12420v1", "updated": "2023-06-21T17:58:25Z", "updated_parsed": [2023, 6, 21, 17, 58, 25, 2, 172, 0], "published": "2023-06-21T17:58:25Z", "published_parsed": [2023, 6, 21, 17, 58, 25, 2, 172, 0], "title": "LMFlow: An Extensible Toolkit for Finetuning and Inference of Large\n  Foundation Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=lmflow++an+extensible+toolkit+for+finetuning+and+inference+of+large+foundation+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "LMFlow: An Extensible Toolkit for Finetuning and Inference of Large\n  Foundation Models"}, "summary": "Large foundation models have demonstrated a great ability to achieve general\nhuman-level intelligence far beyond traditional approaches. As the technique\nkeeps attracting attention from the AI community, more and more large\nfoundation models have become publically available. However, most of those\nmodels exhibit a major deficiency in specialized-task applications, where the\nstep of finetuning is still required for obtaining satisfactory performance. As\nthe number of available models and specialized tasks keeps growing, the job of\ngeneral finetuning becomes highly nontrivial. In this paper, we take the first\nstep to address this issue. We introduce an extensible and lightweight toolkit,\nLMFlow, which aims to simplify the finetuning and inference of general large\nfoundation models. LMFlow offers a complete finetuning workflow for a large\nfoundation model to support personalized training with limited computing\nresources. Furthermore, it supports continuous pretraining, instruction tuning,\nparameter-efficient finetuning, alignment tuning, and large model inference,\nalong with carefully designed and extensible APIs. This toolkit has been\nthoroughly tested and is available at https://github.com/OptimalScale/LMFlow.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=lmflow++an+extensible+toolkit+for+finetuning+and+inference+of+large+foundation+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Large foundation models have demonstrated a great ability to achieve general\nhuman-level intelligence far beyond traditional approaches. As the technique\nkeeps attracting attention from the AI community, more and more large\nfoundation models have become publically available. However, most of those\nmodels exhibit a major deficiency in specialized-task applications, where the\nstep of finetuning is still required for obtaining satisfactory performance. As\nthe number of available models and specialized tasks keeps growing, the job of\ngeneral finetuning becomes highly nontrivial. In this paper, we take the first\nstep to address this issue. We introduce an extensible and lightweight toolkit,\nLMFlow, which aims to simplify the finetuning and inference of general large\nfoundation models. LMFlow offers a complete finetuning workflow for a large\nfoundation model to support personalized training with limited computing\nresources. Furthermore, it supports continuous pretraining, instruction tuning,\nparameter-efficient finetuning, alignment tuning, and large model inference,\nalong with carefully designed and extensible APIs. This toolkit has been\nthoroughly tested and is available at https://github.com/OptimalScale/LMFlow."}, "authors": [{"name": "Shizhe Diao"}, {"name": "Rui Pan"}, {"name": "Hanze Dong"}, {"name": "Ka Shun Shum"}, {"name": "Jipeng Zhang"}, {"name": "Wei Xiong"}, {"name": "Tong Zhang"}], "author_detail": {"name": "Tong Zhang"}, "author": "Tong Zhang", "arxiv_comment": "13 pages, 3 figures", "links": [{"href": "http://arxiv.org/abs/2306.12420v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2306.12420v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}