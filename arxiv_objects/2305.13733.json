{"id": "http://arxiv.org/abs/2305.13733v1", "guidislink": true, "link": "http://arxiv.org/abs/2305.13733v1", "updated": "2023-05-23T06:38:20Z", "updated_parsed": [2023, 5, 23, 6, 38, 20, 1, 143, 0], "published": "2023-05-23T06:38:20Z", "published_parsed": [2023, 5, 23, 6, 38, 20, 1, 143, 0], "title": "Self-Critique Prompting with Large Language Models for Inductive\n  Instructions", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=self+critique+prompting+with+large+language+models+for+inductive+instructions&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Self-Critique Prompting with Large Language Models for Inductive\n  Instructions"}, "summary": "Numerous works are proposed to improve or evaluate the capabilities of Large\nlanguage models (LLMs) to fulfill user instructions. However, they neglect the\npossibility that user inputs may inherently contain incorrect information due\nto users' false beliefs or malicious intents. In this way, blindly adhering to\nusers' false content will cause deception and harm. To address this problem, we\npropose a challenging benchmark consisting of Inductive Instructions (INDust)\nto evaluate whether LLMs could resist these instructions. The INDust includes\n15K instructions across three categories: Fact-Checking Instructions, Questions\nbased on False Premises, and Creative Instructions based on False Premises. Our\nexperiments on several strong LLMs reveal that current LLMs can be easily\ndeceived by INDust into generating misleading and malicious statements. Hence\nwe employ Self-Critique prompting to encourage LLMs to not only critique\nthemselves like in previous works but also the users, which show remarkable\nimprovement in handling inductive instructions under both zero-shot and\nfew-shot settings.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=self+critique+prompting+with+large+language+models+for+inductive+instructions&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Numerous works are proposed to improve or evaluate the capabilities of Large\nlanguage models (LLMs) to fulfill user instructions. However, they neglect the\npossibility that user inputs may inherently contain incorrect information due\nto users' false beliefs or malicious intents. In this way, blindly adhering to\nusers' false content will cause deception and harm. To address this problem, we\npropose a challenging benchmark consisting of Inductive Instructions (INDust)\nto evaluate whether LLMs could resist these instructions. The INDust includes\n15K instructions across three categories: Fact-Checking Instructions, Questions\nbased on False Premises, and Creative Instructions based on False Premises. Our\nexperiments on several strong LLMs reveal that current LLMs can be easily\ndeceived by INDust into generating misleading and malicious statements. Hence\nwe employ Self-Critique prompting to encourage LLMs to not only critique\nthemselves like in previous works but also the users, which show remarkable\nimprovement in handling inductive instructions under both zero-shot and\nfew-shot settings."}, "authors": [{"name": "Rui Wang"}, {"name": "Hongru Wang"}, {"name": "Fei Mi"}, {"name": "Yi Chen"}, {"name": "Ruifeng Xu"}, {"name": "Kam-Fai Wong"}], "author_detail": {"name": "Kam-Fai Wong"}, "author": "Kam-Fai Wong", "links": [{"href": "http://arxiv.org/abs/2305.13733v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2305.13733v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}