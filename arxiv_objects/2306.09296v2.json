{"id": "http://arxiv.org/abs/2306.09296v2", "guidislink": true, "link": "http://arxiv.org/abs/2306.09296v2", "updated": "2023-07-06T17:25:10Z", "updated_parsed": [2023, 7, 6, 17, 25, 10, 3, 187, 0], "published": "2023-06-15T17:20:46Z", "published_parsed": [2023, 6, 15, 17, 20, 46, 3, 166, 0], "title": "KoLA: Carefully Benchmarking World Knowledge of Large Language Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=kola++carefully+benchmarking+world+knowledge+of+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "KoLA: Carefully Benchmarking World Knowledge of Large Language Models"}, "summary": "The unprecedented performance of large language models (LLMs) necessitates\nimprovements in evaluations. Rather than merely exploring the breadth of LLM\nabilities, we believe meticulous and thoughtful designs are essential to\nthorough, unbiased, and applicable evaluations. Given the importance of world\nknowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark\n(KoLA), in which we carefully design three crucial factors: (1) For ability\nmodeling, we mimic human cognition to form a four-level taxonomy of\nknowledge-related abilities, covering $19$ tasks. (2) For data, to ensure fair\ncomparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs,\nalong with continuously collected emerging corpora, aiming to evaluate the\ncapacity to handle unseen data and evolving knowledge. (3) For evaluation\ncriteria, we adopt a contrastive system, including overall standard scores for\nbetter numerical comparability across tasks and models and a unique\nself-contrast metric for automatically evaluating knowledge hallucination. We\nevaluate $21$ open-source and commercial LLMs and obtain some intriguing\nfindings. The KoLA dataset and open-participation leaderboard are publicly\nreleased at https://kola.xlore.cn and will be continuously updated to provide\nreferences for developing LLMs and knowledge-related systems.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=kola++carefully+benchmarking+world+knowledge+of+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "The unprecedented performance of large language models (LLMs) necessitates\nimprovements in evaluations. Rather than merely exploring the breadth of LLM\nabilities, we believe meticulous and thoughtful designs are essential to\nthorough, unbiased, and applicable evaluations. Given the importance of world\nknowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark\n(KoLA), in which we carefully design three crucial factors: (1) For ability\nmodeling, we mimic human cognition to form a four-level taxonomy of\nknowledge-related abilities, covering $19$ tasks. (2) For data, to ensure fair\ncomparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs,\nalong with continuously collected emerging corpora, aiming to evaluate the\ncapacity to handle unseen data and evolving knowledge. (3) For evaluation\ncriteria, we adopt a contrastive system, including overall standard scores for\nbetter numerical comparability across tasks and models and a unique\nself-contrast metric for automatically evaluating knowledge hallucination. We\nevaluate $21$ open-source and commercial LLMs and obtain some intriguing\nfindings. The KoLA dataset and open-participation leaderboard are publicly\nreleased at https://kola.xlore.cn and will be continuously updated to provide\nreferences for developing LLMs and knowledge-related systems."}, "authors": [{"name": "Jifan Yu"}, {"name": "Xiaozhi Wang"}, {"name": "Shangqing Tu"}, {"name": "Shulin Cao"}, {"name": "Daniel Zhang-Li"}, {"name": "Xin Lv"}, {"name": "Hao Peng"}, {"name": "Zijun Yao"}, {"name": "Xiaohan Zhang"}, {"name": "Hanming Li"}, {"name": "Chunyang Li"}, {"name": "Zheyuan Zhang"}, {"name": "Yushi Bai"}, {"name": "Yantao Liu"}, {"name": "Amy Xin"}, {"name": "Nianyi Lin"}, {"name": "Kaifeng Yun"}, {"name": "Linlu Gong"}, {"name": "Jianhui Chen"}, {"name": "Zhili Wu"}, {"name": "Yunjia Qi"}, {"name": "Weikai Li"}, {"name": "Yong Guan"}, {"name": "Kaisheng Zeng"}, {"name": "Ji Qi"}, {"name": "Hailong Jin"}, {"name": "Jinxin Liu"}, {"name": "Yu Gu"}, {"name": "Yuan Yao"}, {"name": "Ning Ding"}, {"name": "Lei Hou"}, {"name": "Zhiyuan Liu"}, {"name": "Bin Xu"}, {"name": "Jie Tang"}, {"name": "Juanzi Li"}], "author_detail": {"name": "Juanzi Li"}, "author": "Juanzi Li", "links": [{"href": "http://arxiv.org/abs/2306.09296v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2306.09296v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}