{"id": "http://arxiv.org/abs/2308.07922v1", "guidislink": true, "link": "http://arxiv.org/abs/2308.07922v1", "updated": "2023-08-15T17:59:18Z", "updated_parsed": [2023, 8, 15, 17, 59, 18, 1, 227, 0], "published": "2023-08-15T17:59:18Z", "published_parsed": [2023, 8, 15, 17, 59, 18, 1, 227, 0], "title": "RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder\n  Language Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=raven++in+context+learning+with+retrieval+augmented+encoder+decoder+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder\n  Language Models"}, "summary": "In this paper, we investigate the in-context learning ability of\nretrieval-augmented encoder-decoder language models. We first conduct a\ncomprehensive analysis of the state-of-the-art ATLAS model and identify its\nlimitations in in-context learning, primarily due to a mismatch between\npretraining and testing, as well as a restricted context length. To address\nthese issues, we propose RAVEN, a model that combines retrieval-augmented\nmasked language modeling and prefix language modeling. We further introduce\nFusion-in-Context Learning to enhance the few-shot performance by enabling the\nmodel to leverage more in-context examples without requiring additional\ntraining or model modifications. Through extensive experiments, we demonstrate\nthat RAVEN significantly outperforms ATLAS and achieves results comparable to\nthe most advanced language models in certain scenarios, despite having\nsubstantially fewer parameters. Our work underscores the potential of\nretrieval-augmented encoder-decoder language models for in-context learning and\nencourages further research in this direction.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=raven++in+context+learning+with+retrieval+augmented+encoder+decoder+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "In this paper, we investigate the in-context learning ability of\nretrieval-augmented encoder-decoder language models. We first conduct a\ncomprehensive analysis of the state-of-the-art ATLAS model and identify its\nlimitations in in-context learning, primarily due to a mismatch between\npretraining and testing, as well as a restricted context length. To address\nthese issues, we propose RAVEN, a model that combines retrieval-augmented\nmasked language modeling and prefix language modeling. We further introduce\nFusion-in-Context Learning to enhance the few-shot performance by enabling the\nmodel to leverage more in-context examples without requiring additional\ntraining or model modifications. Through extensive experiments, we demonstrate\nthat RAVEN significantly outperforms ATLAS and achieves results comparable to\nthe most advanced language models in certain scenarios, despite having\nsubstantially fewer parameters. Our work underscores the potential of\nretrieval-augmented encoder-decoder language models for in-context learning and\nencourages further research in this direction."}, "authors": [{"name": "Jie Huang"}, {"name": "Wei Ping"}, {"name": "Peng Xu"}, {"name": "Mohammad Shoeybi"}, {"name": "Kevin Chen-Chuan Chang"}, {"name": "Bryan Catanzaro"}], "author_detail": {"name": "Bryan Catanzaro"}, "author": "Bryan Catanzaro", "links": [{"href": "http://arxiv.org/abs/2308.07922v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2308.07922v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}