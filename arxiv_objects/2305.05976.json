{"id": "http://arxiv.org/abs/2305.05976v2", "guidislink": true, "link": "http://arxiv.org/abs/2305.05976v2", "updated": "2023-05-13T13:34:04Z", "updated_parsed": [2023, 5, 13, 13, 34, 4, 5, 133, 0], "published": "2023-05-10T08:35:50Z", "published_parsed": [2023, 5, 10, 8, 35, 50, 2, 130, 0], "title": "Say What You Mean! Large Language Models Speak Too Positively about\n  Negative Commonsense Knowledge", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=say+what+you+mean++large+language+models+speak+too+positively+about+negative+commonsense+knowledge&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Say What You Mean! Large Language Models Speak Too Positively about\n  Negative Commonsense Knowledge"}, "summary": "Large language models (LLMs) have been widely studied for their ability to\nstore and utilize positive knowledge. However, negative knowledge, such as\n\"lions don't live in the ocean\", is also ubiquitous in the world but rarely\nmentioned explicitly in the text. What do LLMs know about negative knowledge?\nThis work examines the ability of LLMs to negative commonsense knowledge. We\ndesign a constrained keywords-to-sentence generation task (CG) and a Boolean\nquestion-answering task (QA) to probe LLMs. Our experiments reveal that LLMs\nfrequently fail to generate valid sentences grounded in negative commonsense\nknowledge, yet they can correctly answer polar yes-or-no questions. We term\nthis phenomenon the belief conflict of LLMs. Our further analysis shows that\nstatistical shortcuts and negation reporting bias from language modeling\npre-training cause this conflict.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=say+what+you+mean++large+language+models+speak+too+positively+about+negative+commonsense+knowledge&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Large language models (LLMs) have been widely studied for their ability to\nstore and utilize positive knowledge. However, negative knowledge, such as\n\"lions don't live in the ocean\", is also ubiquitous in the world but rarely\nmentioned explicitly in the text. What do LLMs know about negative knowledge?\nThis work examines the ability of LLMs to negative commonsense knowledge. We\ndesign a constrained keywords-to-sentence generation task (CG) and a Boolean\nquestion-answering task (QA) to probe LLMs. Our experiments reveal that LLMs\nfrequently fail to generate valid sentences grounded in negative commonsense\nknowledge, yet they can correctly answer polar yes-or-no questions. We term\nthis phenomenon the belief conflict of LLMs. Our further analysis shows that\nstatistical shortcuts and negation reporting bias from language modeling\npre-training cause this conflict."}, "authors": [{"name": "Jiangjie Chen"}, {"name": "Wei Shi"}, {"name": "Ziquan Fu"}, {"name": "Sijie Cheng"}, {"name": "Lei Li"}, {"name": "Yanghua Xiao"}], "author_detail": {"name": "Yanghua Xiao"}, "author": "Yanghua Xiao", "arxiv_comment": "Accepted to ACL 2023", "links": [{"href": "http://arxiv.org/abs/2305.05976v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2305.05976v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}