{"id": "http://arxiv.org/abs/2305.15324v1", "guidislink": true, "link": "http://arxiv.org/abs/2305.15324v1", "updated": "2023-05-24T16:38:43Z", "updated_parsed": [2023, 5, 24, 16, 38, 43, 2, 144, 0], "published": "2023-05-24T16:38:43Z", "published_parsed": [2023, 5, 24, 16, 38, 43, 2, 144, 0], "title": "Model evaluation for extreme risks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=model+evaluation+for+extreme+risks&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Model evaluation for extreme risks"}, "summary": "Current approaches to building general-purpose AI systems tend to produce\nsystems with both beneficial and harmful capabilities. Further progress in AI\ndevelopment could lead to capabilities that pose extreme risks, such as\noffensive cyber capabilities or strong manipulation skills. We explain why\nmodel evaluation is critical for addressing extreme risks. Developers must be\nable to identify dangerous capabilities (through \"dangerous capability\nevaluations\") and the propensity of models to apply their capabilities for harm\n(through \"alignment evaluations\"). These evaluations will become critical for\nkeeping policymakers and other stakeholders informed, and for making\nresponsible decisions about model training, deployment, and security.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=model+evaluation+for+extreme+risks&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Current approaches to building general-purpose AI systems tend to produce\nsystems with both beneficial and harmful capabilities. Further progress in AI\ndevelopment could lead to capabilities that pose extreme risks, such as\noffensive cyber capabilities or strong manipulation skills. We explain why\nmodel evaluation is critical for addressing extreme risks. Developers must be\nable to identify dangerous capabilities (through \"dangerous capability\nevaluations\") and the propensity of models to apply their capabilities for harm\n(through \"alignment evaluations\"). These evaluations will become critical for\nkeeping policymakers and other stakeholders informed, and for making\nresponsible decisions about model training, deployment, and security."}, "authors": [{"name": "Toby Shevlane"}, {"name": "Sebastian Farquhar"}, {"name": "Ben Garfinkel"}, {"name": "Mary Phuong"}, {"name": "Jess Whittlestone"}, {"name": "Jade Leung"}, {"name": "Daniel Kokotajlo"}, {"name": "Nahema Marchal"}, {"name": "Markus Anderljung"}, {"name": "Noam Kolt"}, {"name": "Lewis Ho"}, {"name": "Divya Siddarth"}, {"name": "Shahar Avin"}, {"name": "Will Hawkins"}, {"name": "Been Kim"}, {"name": "Iason Gabriel"}, {"name": "Vijay Bolina"}, {"name": "Jack Clark"}, {"name": "Yoshua Bengio"}, {"name": "Paul Christiano"}, {"name": "Allan Dafoe"}], "author_detail": {"name": "Allan Dafoe"}, "author": "Allan Dafoe", "links": [{"href": "http://arxiv.org/abs/2305.15324v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2305.15324v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "K.4.1", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}