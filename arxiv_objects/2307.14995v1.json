{"id": "http://arxiv.org/abs/2307.14995v1", "guidislink": true, "link": "http://arxiv.org/abs/2307.14995v1", "updated": "2023-07-27T16:45:33Z", "updated_parsed": [2023, 7, 27, 16, 45, 33, 3, 208, 0], "published": "2023-07-27T16:45:33Z", "published_parsed": [2023, 7, 27, 16, 45, 33, 3, 208, 0], "title": "Scaling TransNormer to 175 Billion Parameters", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=scaling+transnormer+to+175+billion+parameters&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Scaling TransNormer to 175 Billion Parameters"}, "summary": "We present TransNormerLLM, the first linear attention-based Large Language\nModel (LLM) that outperforms conventional softmax attention-based models in\nterms of both accuracy and efficiency. TransNormerLLM evolves from the previous\nlinear attention architecture TransNormer by making advanced modifications that\ninclude positional embedding, linear attention acceleration, gating mechanism,\ntensor normalization, inference acceleration and stabilization. Specifically,\nwe use LRPE together with an exponential decay to avoid attention dilution\nissues while allowing the model to retain global interactions between tokens.\nAdditionally, we propose Lightning Attention, a cutting-edge technique that\naccelerates linear attention by more than twice in runtime and reduces memory\nusage by a remarkable four times. To further enhance the performance of\nTransNormer, we leverage a gating mechanism to smooth training and a new tensor\nnormalization scheme to accelerate the model, resulting in an impressive\nacceleration of over 20%. Furthermore, we have developed a robust inference\nalgorithm that ensures numerical stability and consistent inference speed,\nregardless of the sequence length, showcasing superior efficiency during both\ntraining and inference stages. Scalability is at the heart of our model's\ndesign, enabling seamless deployment on large-scale clusters and facilitating\nexpansion to even more extensive models, all while maintaining outstanding\nperformance metrics. Rigorous validation of our model design is achieved\nthrough a series of comprehensive experiments on our self-collected corpus,\nboasting a size exceeding 6TB and containing over 2 trillion tokens. To ensure\ndata quality and relevance, we implement a new self-cleaning strategy to filter\nour collected data. Our pre-trained models will be released to foster community\nadvancements in efficient LLMs.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=scaling+transnormer+to+175+billion+parameters&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "We present TransNormerLLM, the first linear attention-based Large Language\nModel (LLM) that outperforms conventional softmax attention-based models in\nterms of both accuracy and efficiency. TransNormerLLM evolves from the previous\nlinear attention architecture TransNormer by making advanced modifications that\ninclude positional embedding, linear attention acceleration, gating mechanism,\ntensor normalization, inference acceleration and stabilization. Specifically,\nwe use LRPE together with an exponential decay to avoid attention dilution\nissues while allowing the model to retain global interactions between tokens.\nAdditionally, we propose Lightning Attention, a cutting-edge technique that\naccelerates linear attention by more than twice in runtime and reduces memory\nusage by a remarkable four times. To further enhance the performance of\nTransNormer, we leverage a gating mechanism to smooth training and a new tensor\nnormalization scheme to accelerate the model, resulting in an impressive\nacceleration of over 20%. Furthermore, we have developed a robust inference\nalgorithm that ensures numerical stability and consistent inference speed,\nregardless of the sequence length, showcasing superior efficiency during both\ntraining and inference stages. Scalability is at the heart of our model's\ndesign, enabling seamless deployment on large-scale clusters and facilitating\nexpansion to even more extensive models, all while maintaining outstanding\nperformance metrics. Rigorous validation of our model design is achieved\nthrough a series of comprehensive experiments on our self-collected corpus,\nboasting a size exceeding 6TB and containing over 2 trillion tokens. To ensure\ndata quality and relevance, we implement a new self-cleaning strategy to filter\nour collected data. Our pre-trained models will be released to foster community\nadvancements in efficient LLMs."}, "authors": [{"name": "Zhen Qin"}, {"name": "Dong Li"}, {"name": "Weigao Sun"}, {"name": "Weixuan Sun"}, {"name": "Xuyang Shen"}, {"name": "Xiaodong Han"}, {"name": "Yunshen Wei"}, {"name": "Baohong Lv"}, {"name": "Fei Yuan"}, {"name": "Xiao Luo"}, {"name": "Yu Qiao"}, {"name": "Yiran Zhong"}], "author_detail": {"name": "Yiran Zhong"}, "author": "Yiran Zhong", "arxiv_comment": "Technical Report. Yiran Zhong is the corresponding author. Zhen Qin,\n  Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen contribute equally to this\n  paper", "links": [{"href": "http://arxiv.org/abs/2307.14995v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.14995v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}