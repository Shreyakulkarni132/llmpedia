{"id": "http://arxiv.org/abs/2308.06912v1", "guidislink": true, "link": "http://arxiv.org/abs/2308.06912v1", "updated": "2023-08-14T03:14:38Z", "updated_parsed": [2023, 8, 14, 3, 14, 38, 0, 226, 0], "published": "2023-08-14T03:14:38Z", "published_parsed": [2023, 8, 14, 3, 14, 38, 0, 226, 0], "title": "CausalLM is not optimal for in-context learning", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=causallm+is+not+optimal+for+in+context+learning&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "CausalLM is not optimal for in-context learning"}, "summary": "Recent empirical evidence indicates that transformer based in-context\nlearning performs better when using a prefix language model (prefixLM), in\nwhich in-context samples can all attend to each other, compared to causal\nlanguage models (causalLM), which use auto-regressive attention that prohibits\nin-context samples to attend to future samples. While this result is intuitive,\nit is not understood from a theoretical perspective. In this paper we take a\ntheoretical approach and analyze the convergence behavior of prefixLM and\ncausalLM under a certain parameter construction. Our analysis shows that both\nLM types converge to their stationary points at a linear rate, but that while\nprefixLM converges to the optimal solution of linear regression, causalLM\nconvergence dynamics follows that of an online gradient descent algorithm,\nwhich is not guaranteed to be optimal even as the number of samples grows\ninfinitely. We supplement our theoretical claims with empirical experiments\nover synthetic and real tasks and using various types of transformers. Our\nexperiments verify that causalLM consistently underperforms prefixLM in all\nsettings.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=causallm+is+not+optimal+for+in+context+learning&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Recent empirical evidence indicates that transformer based in-context\nlearning performs better when using a prefix language model (prefixLM), in\nwhich in-context samples can all attend to each other, compared to causal\nlanguage models (causalLM), which use auto-regressive attention that prohibits\nin-context samples to attend to future samples. While this result is intuitive,\nit is not understood from a theoretical perspective. In this paper we take a\ntheoretical approach and analyze the convergence behavior of prefixLM and\ncausalLM under a certain parameter construction. Our analysis shows that both\nLM types converge to their stationary points at a linear rate, but that while\nprefixLM converges to the optimal solution of linear regression, causalLM\nconvergence dynamics follows that of an online gradient descent algorithm,\nwhich is not guaranteed to be optimal even as the number of samples grows\ninfinitely. We supplement our theoretical claims with empirical experiments\nover synthetic and real tasks and using various types of transformers. Our\nexperiments verify that causalLM consistently underperforms prefixLM in all\nsettings."}, "authors": [{"name": "Nan Ding"}, {"name": "Tomer Levinboim"}, {"name": "Jialin Wu"}, {"name": "Sebastian Goodman"}, {"name": "Radu Soricut"}], "author_detail": {"name": "Radu Soricut"}, "author": "Radu Soricut", "links": [{"href": "http://arxiv.org/abs/2308.06912v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2308.06912v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}