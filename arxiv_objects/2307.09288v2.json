{"id": "http://arxiv.org/abs/2307.09288v2", "guidislink": true, "link": "http://arxiv.org/abs/2307.09288v2", "updated": "2023-07-19T17:08:59Z", "updated_parsed": [2023, 7, 19, 17, 8, 59, 2, 200, 0], "published": "2023-07-18T14:31:57Z", "published_parsed": [2023, 7, 18, 14, 31, 57, 1, 199, 0], "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=llama+2++open+foundation+and+fine+tuned+chat+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, "summary": "In this work, we develop and release Llama 2, a collection of pretrained and\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\ndialogue use cases. Our models outperform open-source chat models on most\nbenchmarks we tested, and based on our human evaluations for helpfulness and\nsafety, may be a suitable substitute for closed-source models. We provide a\ndetailed description of our approach to fine-tuning and safety improvements of\nLlama 2-Chat in order to enable the community to build on our work and\ncontribute to the responsible development of LLMs.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=llama+2++open+foundation+and+fine+tuned+chat+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "In this work, we develop and release Llama 2, a collection of pretrained and\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\ndialogue use cases. Our models outperform open-source chat models on most\nbenchmarks we tested, and based on our human evaluations for helpfulness and\nsafety, may be a suitable substitute for closed-source models. We provide a\ndetailed description of our approach to fine-tuning and safety improvements of\nLlama 2-Chat in order to enable the community to build on our work and\ncontribute to the responsible development of LLMs."}, "authors": [{"name": "Hugo Touvron"}, {"name": "Louis Martin"}, {"name": "Kevin Stone"}, {"name": "Peter Albert"}, {"name": "Amjad Almahairi"}, {"name": "Yasmine Babaei"}, {"name": "Nikolay Bashlykov"}, {"name": "Soumya Batra"}, {"name": "Prajjwal Bhargava"}, {"name": "Shruti Bhosale"}, {"name": "Dan Bikel"}, {"name": "Lukas Blecher"}, {"name": "Cristian Canton Ferrer"}, {"name": "Moya Chen"}, {"name": "Guillem Cucurull"}, {"name": "David Esiobu"}, {"name": "Jude Fernandes"}, {"name": "Jeremy Fu"}, {"name": "Wenyin Fu"}, {"name": "Brian Fuller"}, {"name": "Cynthia Gao"}, {"name": "Vedanuj Goswami"}, {"name": "Naman Goyal"}, {"name": "Anthony Hartshorn"}, {"name": "Saghar Hosseini"}, {"name": "Rui Hou"}, {"name": "Hakan Inan"}, {"name": "Marcin Kardas"}, {"name": "Viktor Kerkez"}, {"name": "Madian Khabsa"}, {"name": "Isabel Kloumann"}, {"name": "Artem Korenev"}, {"name": "Punit Singh Koura"}, {"name": "Marie-Anne Lachaux"}, {"name": "Thibaut Lavril"}, {"name": "Jenya Lee"}, {"name": "Diana Liskovich"}, {"name": "Yinghai Lu"}, {"name": "Yuning Mao"}, {"name": "Xavier Martinet"}, {"name": "Todor Mihaylov"}, {"name": "Pushkar Mishra"}, {"name": "Igor Molybog"}, {"name": "Yixin Nie"}, {"name": "Andrew Poulton"}, {"name": "Jeremy Reizenstein"}, {"name": "Rashi Rungta"}, {"name": "Kalyan Saladi"}, {"name": "Alan Schelten"}, {"name": "Ruan Silva"}, {"name": "Eric Michael Smith"}, {"name": "Ranjan Subramanian"}, {"name": "Xiaoqing Ellen Tan"}, {"name": "Binh Tang"}, {"name": "Ross Taylor"}, {"name": "Adina Williams"}, {"name": "Jian Xiang Kuan"}, {"name": "Puxin Xu"}, {"name": "Zheng Yan"}, {"name": "Iliyan Zarov"}, {"name": "Yuchen Zhang"}, {"name": "Angela Fan"}, {"name": "Melanie Kambadur"}, {"name": "Sharan Narang"}, {"name": "Aurelien Rodriguez"}, {"name": "Robert Stojnic"}, {"name": "Sergey Edunov"}, {"name": "Thomas Scialom"}], "author_detail": {"name": "Thomas Scialom"}, "author": "Thomas Scialom", "links": [{"href": "http://arxiv.org/abs/2307.09288v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.09288v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}