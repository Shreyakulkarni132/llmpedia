{"id": "http://arxiv.org/abs/2306.11644v1", "guidislink": true, "link": "http://arxiv.org/abs/2306.11644v1", "updated": "2023-06-20T16:14:25Z", "updated_parsed": [2023, 6, 20, 16, 14, 25, 1, 171, 0], "published": "2023-06-20T16:14:25Z", "published_parsed": [2023, 6, 20, 16, 14, 25, 1, 171, 0], "title": "Textbooks Are All You Need", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=textbooks+are+all+you+need&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Textbooks Are All You Need"}, "summary": "We introduce phi-1, a new large language model for code, with significantly\nsmaller size than competing models: phi-1 is a Transformer-based model with\n1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook\nquality\" data from the web (6B tokens) and synthetically generated textbooks\nand exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains\npass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays\nsurprising emergent properties compared to phi-1-base, our model before our\nfinetuning stage on a dataset of coding exercises, and phi-1-small, a smaller\nmodel with 350M parameters trained with the same pipeline as phi-1 that still\nachieves 45% on HumanEval.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=textbooks+are+all+you+need&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "We introduce phi-1, a new large language model for code, with significantly\nsmaller size than competing models: phi-1 is a Transformer-based model with\n1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook\nquality\" data from the web (6B tokens) and synthetically generated textbooks\nand exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains\npass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays\nsurprising emergent properties compared to phi-1-base, our model before our\nfinetuning stage on a dataset of coding exercises, and phi-1-small, a smaller\nmodel with 350M parameters trained with the same pipeline as phi-1 that still\nachieves 45% on HumanEval."}, "authors": [{"name": "Suriya Gunasekar"}, {"name": "Yi Zhang"}, {"name": "Jyoti Aneja"}, {"name": "Caio C\u00e9sar Teodoro Mendes"}, {"name": "Allie Del Giorno"}, {"name": "Sivakanth Gopi"}, {"name": "Mojan Javaheripi"}, {"name": "Piero Kauffmann"}, {"name": "Gustavo de Rosa"}, {"name": "Olli Saarikivi"}, {"name": "Adil Salim"}, {"name": "Shital Shah"}, {"name": "Harkirat Singh Behl"}, {"name": "Xin Wang"}, {"name": "S\u00e9bastien Bubeck"}, {"name": "Ronen Eldan"}, {"name": "Adam Tauman Kalai"}, {"name": "Yin Tat Lee"}, {"name": "Yuanzhi Li"}], "author_detail": {"name": "Yuanzhi Li"}, "author": "Yuanzhi Li", "arxiv_comment": "26 pages", "links": [{"href": "http://arxiv.org/abs/2306.11644v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2306.11644v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}