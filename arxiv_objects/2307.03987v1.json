{"id": "http://arxiv.org/abs/2307.03987v1", "guidislink": true, "link": "http://arxiv.org/abs/2307.03987v1", "updated": "2023-07-08T14:25:57Z", "updated_parsed": [2023, 7, 8, 14, 25, 57, 5, 189, 0], "published": "2023-07-08T14:25:57Z", "published_parsed": [2023, 7, 8, 14, 25, 57, 5, 189, 0], "title": "A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of\n  LLMs by Validating Low-Confidence Generation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=a+stitch+in+time+saves+nine++detecting+and+mitigating+hallucinations+of+llms+by+validating+low+confidence+generation&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of\n  LLMs by Validating Low-Confidence Generation"}, "summary": "Recently developed large language models have achieved remarkable success in\ngenerating fluent and coherent text. However, these models often tend to\n'hallucinate' which critically hampers their reliability. In this work, we\naddress this crucial problem and propose an approach that actively detects and\nmitigates hallucinations during the generation process. Specifically, we first\nidentify the candidates of potential hallucination leveraging the model's logit\noutput values, check their correctness through a validation procedure, mitigate\nthe detected hallucinations, and then continue with the generation process.\nThrough extensive experiments with the 'article generation task', we first\ndemonstrate the individual efficacy of our detection and mitigation techniques.\nSpecifically, the detection technique achieves a recall of 88% and the\nmitigation technique successfully mitigates 57.6% of the correctly detected\nhallucinations. Importantly, our mitigation technique does not introduce new\nhallucinations even in the case of incorrectly detected hallucinations, i.e.,\nfalse positives. Then, we show that the proposed active detection and\nmitigation approach successfully reduces the hallucinations of the GPT-3 model\nfrom 47.5% to 14.5% on average. In summary, our work contributes to improving\nthe reliability and trustworthiness of large language models, a crucial step en\nroute to enabling their widespread adoption in real-world applications.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=a+stitch+in+time+saves+nine++detecting+and+mitigating+hallucinations+of+llms+by+validating+low+confidence+generation&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Recently developed large language models have achieved remarkable success in\ngenerating fluent and coherent text. However, these models often tend to\n'hallucinate' which critically hampers their reliability. In this work, we\naddress this crucial problem and propose an approach that actively detects and\nmitigates hallucinations during the generation process. Specifically, we first\nidentify the candidates of potential hallucination leveraging the model's logit\noutput values, check their correctness through a validation procedure, mitigate\nthe detected hallucinations, and then continue with the generation process.\nThrough extensive experiments with the 'article generation task', we first\ndemonstrate the individual efficacy of our detection and mitigation techniques.\nSpecifically, the detection technique achieves a recall of 88% and the\nmitigation technique successfully mitigates 57.6% of the correctly detected\nhallucinations. Importantly, our mitigation technique does not introduce new\nhallucinations even in the case of incorrectly detected hallucinations, i.e.,\nfalse positives. Then, we show that the proposed active detection and\nmitigation approach successfully reduces the hallucinations of the GPT-3 model\nfrom 47.5% to 14.5% on average. In summary, our work contributes to improving\nthe reliability and trustworthiness of large language models, a crucial step en\nroute to enabling their widespread adoption in real-world applications."}, "authors": [{"name": "Neeraj Varshney"}, {"name": "Wenlin Yao"}, {"name": "Hongming Zhang"}, {"name": "Jianshu Chen"}, {"name": "Dong Yu"}], "author_detail": {"name": "Dong Yu"}, "author": "Dong Yu", "links": [{"href": "http://arxiv.org/abs/2307.03987v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.03987v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}