{"id": "http://arxiv.org/abs/2307.06962v1", "guidislink": true, "link": "http://arxiv.org/abs/2307.06962v1", "updated": "2023-07-13T05:03:26Z", "updated_parsed": [2023, 7, 13, 5, 3, 26, 3, 194, 0], "published": "2023-07-13T05:03:26Z", "published_parsed": [2023, 7, 13, 5, 3, 26, 3, 194, 0], "title": "Copy Is All You Need", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=copy+is+all+you+need&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Copy Is All You Need"}, "summary": "The dominant text generation models compose the output by sequentially\nselecting words from a fixed vocabulary. In this paper, we formulate text\ngeneration as progressively copying text segments (e.g., words or phrases) from\nan existing text collection. We compute the contextualized representations of\nmeaningful text segments and index them using efficient vector search toolkits.\nThe task of text generation is then decomposed into a series of copy-and-paste\noperations: at each time step, we seek suitable text spans from the text\ncollection rather than selecting from a standalone vocabulary. Experiments on\nthe standard language modeling benchmark (WikiText-103) show that our approach\nachieves better generation quality according to both automatic and human\nevaluations. Besides, its inference efficiency is comparable to token-level\nautoregressive models thanks to the reduction of decoding steps. We also show\nthat our approach allows for effective domain adaptation by simply switching to\ndomain-specific text collection without extra training. Finally, we observe\nthat our approach attains additional performance gains by simply scaling up to\nlarger text collections, again without further training.\\footnote{Our source\ncodes are publicly available at\n\\url{https://github.com/gmftbyGMFTBY/Copyisallyouneed}.}", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=copy+is+all+you+need&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "The dominant text generation models compose the output by sequentially\nselecting words from a fixed vocabulary. In this paper, we formulate text\ngeneration as progressively copying text segments (e.g., words or phrases) from\nan existing text collection. We compute the contextualized representations of\nmeaningful text segments and index them using efficient vector search toolkits.\nThe task of text generation is then decomposed into a series of copy-and-paste\noperations: at each time step, we seek suitable text spans from the text\ncollection rather than selecting from a standalone vocabulary. Experiments on\nthe standard language modeling benchmark (WikiText-103) show that our approach\nachieves better generation quality according to both automatic and human\nevaluations. Besides, its inference efficiency is comparable to token-level\nautoregressive models thanks to the reduction of decoding steps. We also show\nthat our approach allows for effective domain adaptation by simply switching to\ndomain-specific text collection without extra training. Finally, we observe\nthat our approach attains additional performance gains by simply scaling up to\nlarger text collections, again without further training.\\footnote{Our source\ncodes are publicly available at\n\\url{https://github.com/gmftbyGMFTBY/Copyisallyouneed}.}"}, "authors": [{"name": "Tian Lan"}, {"name": "Deng Cai"}, {"name": "Yan Wang"}, {"name": "Heyan Huang"}, {"name": "Xian-Ling Mao"}], "author_detail": {"name": "Xian-Ling Mao"}, "author": "Xian-Ling Mao", "arxiv_journal_ref": "The Eleventh International Conference on Learning Representations\n  (ICLR 2023)", "links": [{"href": "http://arxiv.org/abs/2307.06962v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.06962v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}