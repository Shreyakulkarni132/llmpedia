{"id": "http://arxiv.org/abs/2202.13169v3", "guidislink": true, "link": "http://arxiv.org/abs/2202.13169v3", "updated": "2022-05-04T16:08:31Z", "updated_parsed": [2022, 5, 4, 16, 8, 31, 2, 124, 0], "published": "2022-02-26T15:53:55Z", "published_parsed": [2022, 2, 26, 15, 53, 55, 5, 57, 0], "title": "A Systematic Evaluation of Large Language Models of Code", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=a+systematic+evaluation+of+large+language+models+of+code&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "A Systematic Evaluation of Large Language Models of Code"}, "summary": "Large language models (LMs) of code have recently shown tremendous promise in\ncompleting code and synthesizing code from natural language descriptions.\nHowever, the current state-of-the-art code LMs (e.g., Codex (Chen et al.,\n2021)) are not publicly available, leaving many questions about their model and\ndata design decisions. We aim to fill in some of these blanks through a\nsystematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo,\nGPT-NeoX-20B, and CodeParrot, across various programming languages. Although\nCodex itself is not open-source, we find that existing open-source models do\nachieve close results in some programming languages, although targeted mainly\nfor natural language modeling. We further identify an important missing piece\nin the form of a large open-source model trained exclusively on a multi-lingual\ncorpus of code. We release a new model, PolyCoder, with 2.7B parameters based\non the GPT-2 architecture, which was trained on 249GB of code across 12\nprogramming languages on a single machine. In the C programming language,\nPolyCoder outperforms all models including Codex. Our trained models are\nopen-source and publicly available at https://github.com/VHellendoorn/Code-LMs,\nwhich enables future research and application in this area.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=a+systematic+evaluation+of+large+language+models+of+code&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Large language models (LMs) of code have recently shown tremendous promise in\ncompleting code and synthesizing code from natural language descriptions.\nHowever, the current state-of-the-art code LMs (e.g., Codex (Chen et al.,\n2021)) are not publicly available, leaving many questions about their model and\ndata design decisions. We aim to fill in some of these blanks through a\nsystematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo,\nGPT-NeoX-20B, and CodeParrot, across various programming languages. Although\nCodex itself is not open-source, we find that existing open-source models do\nachieve close results in some programming languages, although targeted mainly\nfor natural language modeling. We further identify an important missing piece\nin the form of a large open-source model trained exclusively on a multi-lingual\ncorpus of code. We release a new model, PolyCoder, with 2.7B parameters based\non the GPT-2 architecture, which was trained on 249GB of code across 12\nprogramming languages on a single machine. In the C programming language,\nPolyCoder outperforms all models including Codex. Our trained models are\nopen-source and publicly available at https://github.com/VHellendoorn/Code-LMs,\nwhich enables future research and application in this area."}, "authors": [{"name": "Frank F. Xu"}, {"name": "Uri Alon"}, {"name": "Graham Neubig"}, {"name": "Vincent J. Hellendoorn"}], "author_detail": {"name": "Vincent J. Hellendoorn"}, "author": "Vincent J. Hellendoorn", "arxiv_comment": "DL4C@ICLR 2022, and MAPS@PLDI 2022", "links": [{"href": "http://arxiv.org/abs/2202.13169v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2202.13169v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.PL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.PL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}