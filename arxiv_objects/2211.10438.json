{"id": "http://arxiv.org/abs/2211.10438v5", "guidislink": true, "link": "http://arxiv.org/abs/2211.10438v5", "updated": "2023-06-05T21:21:28Z", "updated_parsed": [2023, 6, 5, 21, 21, 28, 0, 156, 0], "published": "2022-11-18T18:59:33Z", "published_parsed": [2022, 11, 18, 18, 59, 33, 4, 322, 0], "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large\n  Language Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=smoothquant++accurate+and+efficient+post+training+quantization+for+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large\n  Language Models"}, "summary": "Large language models (LLMs) show excellent performance but are compute- and\nmemory-intensive. Quantization can reduce memory and accelerate inference.\nHowever, existing methods cannot maintain accuracy and hardware efficiency at\nthe same time. We propose SmoothQuant, a training-free, accuracy-preserving,\nand general-purpose post-training quantization (PTQ) solution to enable 8-bit\nweight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that\nweights are easy to quantize while activations are not, SmoothQuant smooths the\nactivation outliers by offline migrating the quantization difficulty from\nactivations to weights with a mathematically equivalent transformation.\nSmoothQuant enables an INT8 quantization of both weights and activations for\nall the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, and\nLLaMA family. We demonstrate up to 1.56x speedup and 2x memory reduction for\nLLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM\nwithin a single node. Our work offers a turn-key solution that reduces hardware\ncosts and democratizes LLMs. Code is available at\nhttps://github.com/mit-han-lab/smoothquant.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=smoothquant++accurate+and+efficient+post+training+quantization+for+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Large language models (LLMs) show excellent performance but are compute- and\nmemory-intensive. Quantization can reduce memory and accelerate inference.\nHowever, existing methods cannot maintain accuracy and hardware efficiency at\nthe same time. We propose SmoothQuant, a training-free, accuracy-preserving,\nand general-purpose post-training quantization (PTQ) solution to enable 8-bit\nweight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that\nweights are easy to quantize while activations are not, SmoothQuant smooths the\nactivation outliers by offline migrating the quantization difficulty from\nactivations to weights with a mathematically equivalent transformation.\nSmoothQuant enables an INT8 quantization of both weights and activations for\nall the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, and\nLLaMA family. We demonstrate up to 1.56x speedup and 2x memory reduction for\nLLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM\nwithin a single node. Our work offers a turn-key solution that reduces hardware\ncosts and democratizes LLMs. Code is available at\nhttps://github.com/mit-han-lab/smoothquant."}, "authors": [{"name": "Guangxuan Xiao"}, {"name": "Ji Lin"}, {"name": "Mickael Seznec"}, {"name": "Hao Wu"}, {"name": "Julien Demouth"}, {"name": "Song Han"}], "author_detail": {"name": "Song Han"}, "author": "Song Han", "arxiv_comment": "ICML 2023. First two authors contributed equally to this work", "links": [{"href": "http://arxiv.org/abs/2211.10438v5", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2211.10438v5", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}