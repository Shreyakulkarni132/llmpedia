{"id": "http://arxiv.org/abs/2308.01825v1", "guidislink": true, "link": "http://arxiv.org/abs/2308.01825v1", "updated": "2023-08-03T15:34:01Z", "updated_parsed": [2023, 8, 3, 15, 34, 1, 3, 215, 0], "published": "2023-08-03T15:34:01Z", "published_parsed": [2023, 8, 3, 15, 34, 1, 3, 215, 0], "title": "Scaling Relationship on Learning Mathematical Reasoning with Large\n  Language Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=scaling+relationship+on+learning+mathematical+reasoning+with+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Scaling Relationship on Learning Mathematical Reasoning with Large\n  Language Models"}, "summary": "Mathematical reasoning is a challenging task for large language models\n(LLMs), while the scaling relationship of it with respect to LLM capacity is\nunder-explored. In this paper, we investigate how the pre-training loss,\nsupervised data amount, and augmented data amount influence the reasoning\nperformances of a supervised LLM. We find that pre-training loss is a better\nindicator of the model's performance than the model's parameter count. We apply\nsupervised fine-tuning (SFT) with different amounts of supervised data and\nempirically find a log-linear relation between data amount and model\nperformance, and we find better models improve less with enlarged supervised\ndatasets. To augment more data samples for improving model performances without\nany human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFT\nuses supervised models to generate and collect correct reasoning paths as\naugmented fine-tuning datasets. We find with augmented samples containing more\ndistinct reasoning paths, RFT improves mathematical reasoning performance more\nfor LLMs. We also find RFT brings more improvement for less performant LLMs.\nFurthermore, we combine rejection samples from multiple models which push\nLLaMA-7B to an accuracy of 49.3% and outperforms the supervised fine-tuning\n(SFT) accuracy of 35.9% significantly.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=scaling+relationship+on+learning+mathematical+reasoning+with+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Mathematical reasoning is a challenging task for large language models\n(LLMs), while the scaling relationship of it with respect to LLM capacity is\nunder-explored. In this paper, we investigate how the pre-training loss,\nsupervised data amount, and augmented data amount influence the reasoning\nperformances of a supervised LLM. We find that pre-training loss is a better\nindicator of the model's performance than the model's parameter count. We apply\nsupervised fine-tuning (SFT) with different amounts of supervised data and\nempirically find a log-linear relation between data amount and model\nperformance, and we find better models improve less with enlarged supervised\ndatasets. To augment more data samples for improving model performances without\nany human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFT\nuses supervised models to generate and collect correct reasoning paths as\naugmented fine-tuning datasets. We find with augmented samples containing more\ndistinct reasoning paths, RFT improves mathematical reasoning performance more\nfor LLMs. We also find RFT brings more improvement for less performant LLMs.\nFurthermore, we combine rejection samples from multiple models which push\nLLaMA-7B to an accuracy of 49.3% and outperforms the supervised fine-tuning\n(SFT) accuracy of 35.9% significantly."}, "authors": [{"name": "Zheng Yuan"}, {"name": "Hongyi Yuan"}, {"name": "Chengpeng Li"}, {"name": "Guanting Dong"}, {"name": "Chuanqi Tan"}, {"name": "Chang Zhou"}], "author_detail": {"name": "Chang Zhou"}, "author": "Chang Zhou", "arxiv_comment": "Working in Progress", "links": [{"href": "http://arxiv.org/abs/2308.01825v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2308.01825v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}