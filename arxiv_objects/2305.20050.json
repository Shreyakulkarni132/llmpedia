{"id": "http://arxiv.org/abs/2305.20050v1", "guidislink": true, "link": "http://arxiv.org/abs/2305.20050v1", "updated": "2023-05-31T17:24:00Z", "updated_parsed": [2023, 5, 31, 17, 24, 0, 2, 151, 0], "published": "2023-05-31T17:24:00Z", "published_parsed": [2023, 5, 31, 17, 24, 0, 2, 151, 0], "title": "Let's Verify Step by Step", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=let+s+verify+step+by+step&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Let's Verify Step by Step"}, "summary": "In recent years, large language models have greatly improved in their ability\nto perform complex multi-step reasoning. However, even state-of-the-art models\nstill regularly produce logical mistakes. To train more reliable models, we can\nturn either to outcome supervision, which provides feedback for a final result,\nor process supervision, which provides feedback for each intermediate reasoning\nstep. Given the importance of training reliable models, and given the high cost\nof human feedback, it is important to carefully compare the both methods.\nRecent work has already begun this comparison, but many questions still remain.\nWe conduct our own investigation, finding that process supervision\nsignificantly outperforms outcome supervision for training models to solve\nproblems from the challenging MATH dataset. Our process-supervised model solves\n78% of problems from a representative subset of the MATH test set.\nAdditionally, we show that active learning significantly improves the efficacy\nof process supervision. To support related research, we also release PRM800K,\nthe complete dataset of 800,000 step-level human feedback labels used to train\nour best reward model.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=let+s+verify+step+by+step&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "In recent years, large language models have greatly improved in their ability\nto perform complex multi-step reasoning. However, even state-of-the-art models\nstill regularly produce logical mistakes. To train more reliable models, we can\nturn either to outcome supervision, which provides feedback for a final result,\nor process supervision, which provides feedback for each intermediate reasoning\nstep. Given the importance of training reliable models, and given the high cost\nof human feedback, it is important to carefully compare the both methods.\nRecent work has already begun this comparison, but many questions still remain.\nWe conduct our own investigation, finding that process supervision\nsignificantly outperforms outcome supervision for training models to solve\nproblems from the challenging MATH dataset. Our process-supervised model solves\n78% of problems from a representative subset of the MATH test set.\nAdditionally, we show that active learning significantly improves the efficacy\nof process supervision. To support related research, we also release PRM800K,\nthe complete dataset of 800,000 step-level human feedback labels used to train\nour best reward model."}, "authors": [{"name": "Hunter Lightman"}, {"name": "Vineet Kosaraju"}, {"name": "Yura Burda"}, {"name": "Harri Edwards"}, {"name": "Bowen Baker"}, {"name": "Teddy Lee"}, {"name": "Jan Leike"}, {"name": "John Schulman"}, {"name": "Ilya Sutskever"}, {"name": "Karl Cobbe"}], "author_detail": {"name": "Karl Cobbe"}, "author": "Karl Cobbe", "links": [{"href": "http://arxiv.org/abs/2305.20050v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2305.20050v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}