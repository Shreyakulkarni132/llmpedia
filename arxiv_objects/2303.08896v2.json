{"id": "http://arxiv.org/abs/2303.08896v2", "guidislink": true, "link": "http://arxiv.org/abs/2303.08896v2", "updated": "2023-05-08T00:52:42Z", "updated_parsed": [2023, 5, 8, 0, 52, 42, 0, 128, 0], "published": "2023-03-15T19:31:21Z", "published_parsed": [2023, 3, 15, 19, 31, 21, 2, 74, 0], "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for\n  Generative Large Language Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=selfcheckgpt++zero+resource+black+box+hallucination+detection+for+generative+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for\n  Generative Large Language Models"}, "summary": "Generative Large Language Models (LLMs) such as GPT-3 are capable of\ngenerating highly fluent responses to a wide variety of user prompts. However,\nLLMs are known to hallucinate facts and make non-factual statements which can\nundermine trust in their output. Existing fact-checking approaches either\nrequire access to the output probability distribution (which may not be\navailable for systems such as ChatGPT) or external databases that are\ninterfaced via separate, often complex, modules. In this work, we propose\n\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check\nblack-box models in a zero-resource fashion, i.e. without an external database.\nSelfCheckGPT leverages the simple idea that if a LLM has knowledge of a given\nconcept, sampled responses are likely to be similar and contain consistent\nfacts. However, for hallucinated facts, stochastically sampled responses are\nlikely to diverge and contradict one another. We investigate this approach by\nusing GPT-3 to generate passages about individuals from the WikiBio dataset,\nand manually annotate the factuality of the generated passages. We demonstrate\nthat SelfCheckGPT can: i) detect non-factual and factual sentences; and ii)\nrank passages in terms of factuality. We compare our approach to several\nbaselines and show that in sentence hallucination detection, our approach has\nAUC-PR scores comparable to or better than grey-box methods, while SelfCheckGPT\nis best at passage factuality assessment.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=selfcheckgpt++zero+resource+black+box+hallucination+detection+for+generative+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Generative Large Language Models (LLMs) such as GPT-3 are capable of\ngenerating highly fluent responses to a wide variety of user prompts. However,\nLLMs are known to hallucinate facts and make non-factual statements which can\nundermine trust in their output. Existing fact-checking approaches either\nrequire access to the output probability distribution (which may not be\navailable for systems such as ChatGPT) or external databases that are\ninterfaced via separate, often complex, modules. In this work, we propose\n\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check\nblack-box models in a zero-resource fashion, i.e. without an external database.\nSelfCheckGPT leverages the simple idea that if a LLM has knowledge of a given\nconcept, sampled responses are likely to be similar and contain consistent\nfacts. However, for hallucinated facts, stochastically sampled responses are\nlikely to diverge and contradict one another. We investigate this approach by\nusing GPT-3 to generate passages about individuals from the WikiBio dataset,\nand manually annotate the factuality of the generated passages. We demonstrate\nthat SelfCheckGPT can: i) detect non-factual and factual sentences; and ii)\nrank passages in terms of factuality. We compare our approach to several\nbaselines and show that in sentence hallucination detection, our approach has\nAUC-PR scores comparable to or better than grey-box methods, while SelfCheckGPT\nis best at passage factuality assessment."}, "authors": [{"name": "Potsawee Manakul"}, {"name": "Adian Liusie"}, {"name": "Mark J. F. Gales"}], "author_detail": {"name": "Mark J. F. Gales"}, "author": "Mark J. F. Gales", "arxiv_comment": "13 pages, update to include additional results and larger dataset", "links": [{"href": "http://arxiv.org/abs/2303.08896v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2303.08896v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}