{"id": "http://arxiv.org/abs/2308.03279v1", "guidislink": true, "link": "http://arxiv.org/abs/2308.03279v1", "updated": "2023-08-07T03:39:52Z", "updated_parsed": [2023, 8, 7, 3, 39, 52, 0, 219, 0], "published": "2023-08-07T03:39:52Z", "published_parsed": [2023, 8, 7, 3, 39, 52, 0, 219, 0], "title": "UniversalNER: Targeted Distillation from Large Language Models for Open\n  Named Entity Recognition", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=universalner++targeted+distillation+from+large+language+models+for+open+named+entity+recognition&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "UniversalNER: Targeted Distillation from Large Language Models for Open\n  Named Entity Recognition"}, "summary": "Large language models (LLMs) have demonstrated remarkable generalizability,\nsuch as understanding arbitrary entities and relations. Instruction tuning has\nproven effective for distilling LLMs into more cost-efficient models such as\nAlpaca and Vicuna. Yet such student models still trail the original LLMs by\nlarge margins in downstream applications. In this paper, we explore targeted\ndistillation with mission-focused instruction tuning to train student models\nthat can excel in a broad application class such as open information\nextraction. Using named entity recognition (NER) for case study, we show how\nChatGPT can be distilled into much smaller UniversalNER models for open NER.\nFor evaluation, we assemble the largest NER benchmark to date, comprising 43\ndatasets across 9 diverse domains such as biomedicine, programming, social\nmedia, law, finance. Without using any direct supervision, UniversalNER attains\nremarkable NER accuracy across tens of thousands of entity types, outperforming\ngeneral instruction-tuned models such as Alpaca and Vicuna by over 30 absolute\nF1 points in average. With a tiny fraction of parameters, UniversalNER not only\nacquires ChatGPT's capability in recognizing arbitrary entity types, but also\noutperforms its NER accuracy by 7-9 absolute F1 points in average. Remarkably,\nUniversalNER even outperforms by a large margin state-of-the-art multi-task\ninstruction-tuned systems such as InstructUIE, which uses supervised NER\nexamples. We also conduct thorough ablation studies to assess the impact of\nvarious components in our distillation approach. We will release the\ndistillation recipe, data, and UniversalNER models to facilitate future\nresearch on targeted distillation.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=universalner++targeted+distillation+from+large+language+models+for+open+named+entity+recognition&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Large language models (LLMs) have demonstrated remarkable generalizability,\nsuch as understanding arbitrary entities and relations. Instruction tuning has\nproven effective for distilling LLMs into more cost-efficient models such as\nAlpaca and Vicuna. Yet such student models still trail the original LLMs by\nlarge margins in downstream applications. In this paper, we explore targeted\ndistillation with mission-focused instruction tuning to train student models\nthat can excel in a broad application class such as open information\nextraction. Using named entity recognition (NER) for case study, we show how\nChatGPT can be distilled into much smaller UniversalNER models for open NER.\nFor evaluation, we assemble the largest NER benchmark to date, comprising 43\ndatasets across 9 diverse domains such as biomedicine, programming, social\nmedia, law, finance. Without using any direct supervision, UniversalNER attains\nremarkable NER accuracy across tens of thousands of entity types, outperforming\ngeneral instruction-tuned models such as Alpaca and Vicuna by over 30 absolute\nF1 points in average. With a tiny fraction of parameters, UniversalNER not only\nacquires ChatGPT's capability in recognizing arbitrary entity types, but also\noutperforms its NER accuracy by 7-9 absolute F1 points in average. Remarkably,\nUniversalNER even outperforms by a large margin state-of-the-art multi-task\ninstruction-tuned systems such as InstructUIE, which uses supervised NER\nexamples. We also conduct thorough ablation studies to assess the impact of\nvarious components in our distillation approach. We will release the\ndistillation recipe, data, and UniversalNER models to facilitate future\nresearch on targeted distillation."}, "authors": [{"name": "Wenxuan Zhou"}, {"name": "Sheng Zhang"}, {"name": "Yu Gu"}, {"name": "Muhao Chen"}, {"name": "Hoifung Poon"}], "author_detail": {"name": "Hoifung Poon"}, "author": "Hoifung Poon", "arxiv_comment": "Project page: https://universal-ner.github.io/", "links": [{"href": "http://arxiv.org/abs/2308.03279v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2308.03279v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}