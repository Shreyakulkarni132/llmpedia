{"id": "http://arxiv.org/abs/2209.12356v2", "guidislink": true, "link": "http://arxiv.org/abs/2209.12356v2", "updated": "2023-05-23T22:52:18Z", "updated_parsed": [2023, 5, 23, 22, 52, 18, 1, 143, 0], "published": "2022-09-26T01:04:52Z", "published_parsed": [2022, 9, 26, 1, 4, 52, 0, 269, 0], "title": "News Summarization and Evaluation in the Era of GPT-3", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=news+summarization+and+evaluation+in+the+era+of+gpt+3&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "News Summarization and Evaluation in the Era of GPT-3"}, "summary": "The recent success of prompting large language models like GPT-3 has led to a\nparadigm shift in NLP research. In this paper, we study its impact on text\nsummarization, focusing on the classic benchmark domain of news summarization.\nFirst, we investigate how GPT-3 compares against fine-tuned models trained on\nlarge summarization datasets. We show that not only do humans overwhelmingly\nprefer GPT-3 summaries, prompted using only a task description, but these also\ndo not suffer from common dataset-specific issues such as poor factuality.\nNext, we study what this means for evaluation, particularly the role of gold\nstandard test sets. Our experiments show that both reference-based and\nreference-free automatic metrics cannot reliably evaluate GPT-3 summaries.\nFinally, we evaluate models on a setting beyond generic summarization,\nspecifically keyword-based summarization, and show how dominant fine-tuning\napproaches compare to prompting.\n  To support further research, we release: (a) a corpus of 10K generated\nsummaries from fine-tuned and prompt-based models across 4 standard\nsummarization benchmarks, (b) 1K human preference judgments comparing different\nsystems for generic- and keyword-based summarization.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=news+summarization+and+evaluation+in+the+era+of+gpt+3&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "The recent success of prompting large language models like GPT-3 has led to a\nparadigm shift in NLP research. In this paper, we study its impact on text\nsummarization, focusing on the classic benchmark domain of news summarization.\nFirst, we investigate how GPT-3 compares against fine-tuned models trained on\nlarge summarization datasets. We show that not only do humans overwhelmingly\nprefer GPT-3 summaries, prompted using only a task description, but these also\ndo not suffer from common dataset-specific issues such as poor factuality.\nNext, we study what this means for evaluation, particularly the role of gold\nstandard test sets. Our experiments show that both reference-based and\nreference-free automatic metrics cannot reliably evaluate GPT-3 summaries.\nFinally, we evaluate models on a setting beyond generic summarization,\nspecifically keyword-based summarization, and show how dominant fine-tuning\napproaches compare to prompting.\n  To support further research, we release: (a) a corpus of 10K generated\nsummaries from fine-tuned and prompt-based models across 4 standard\nsummarization benchmarks, (b) 1K human preference judgments comparing different\nsystems for generic- and keyword-based summarization."}, "authors": [{"name": "Tanya Goyal"}, {"name": "Junyi Jessy Li"}, {"name": "Greg Durrett"}], "author_detail": {"name": "Greg Durrett"}, "author": "Greg Durrett", "arxiv_comment": "All data shared at:\n  https://tagoyal.github.io/zeroshot-news-annotations.html", "links": [{"href": "http://arxiv.org/abs/2209.12356v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2209.12356v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}