{"id": "http://arxiv.org/abs/2308.04265v1", "guidislink": true, "link": "http://arxiv.org/abs/2308.04265v1", "updated": "2023-08-08T14:03:08Z", "updated_parsed": [2023, 8, 8, 14, 3, 8, 1, 220, 0], "published": "2023-08-08T14:03:08Z", "published_parsed": [2023, 8, 8, 14, 3, 8, 1, 220, 0], "title": "FLIRT: Feedback Loop In-context Red Teaming", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=flirt++feedback+loop+in+context+red+teaming&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "FLIRT: Feedback Loop In-context Red Teaming"}, "summary": "Warning: this paper contains content that may be inappropriate or offensive.\n  As generative models become available for public use in various applications,\ntesting and analyzing vulnerabilities of these models has become a priority.\nHere we propose an automatic red teaming framework that evaluates a given model\nand exposes its vulnerabilities against unsafe and inappropriate content\ngeneration. Our framework uses in-context learning in a feedback loop to red\nteam models and trigger them into unsafe content generation. We propose\ndifferent in-context attack strategies to automatically learn effective and\ndiverse adversarial prompts for text-to-image models. Our experiments\ndemonstrate that compared to baseline approaches, our proposed strategy is\nsignificantly more effective in exposing vulnerabilities in Stable Diffusion\n(SD) model, even when the latter is enhanced with safety features. Furthermore,\nwe demonstrate that the proposed framework is effective for red teaming\ntext-to-text models, resulting in significantly higher toxic response\ngeneration rate compared to previously reported numbers.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=flirt++feedback+loop+in+context+red+teaming&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Warning: this paper contains content that may be inappropriate or offensive.\n  As generative models become available for public use in various applications,\ntesting and analyzing vulnerabilities of these models has become a priority.\nHere we propose an automatic red teaming framework that evaluates a given model\nand exposes its vulnerabilities against unsafe and inappropriate content\ngeneration. Our framework uses in-context learning in a feedback loop to red\nteam models and trigger them into unsafe content generation. We propose\ndifferent in-context attack strategies to automatically learn effective and\ndiverse adversarial prompts for text-to-image models. Our experiments\ndemonstrate that compared to baseline approaches, our proposed strategy is\nsignificantly more effective in exposing vulnerabilities in Stable Diffusion\n(SD) model, even when the latter is enhanced with safety features. Furthermore,\nwe demonstrate that the proposed framework is effective for red teaming\ntext-to-text models, resulting in significantly higher toxic response\ngeneration rate compared to previously reported numbers."}, "authors": [{"name": "Ninareh Mehrabi"}, {"name": "Palash Goyal"}, {"name": "Christophe Dupuy"}, {"name": "Qian Hu"}, {"name": "Shalini Ghosh"}, {"name": "Richard Zemel"}, {"name": "Kai-Wei Chang"}, {"name": "Aram Galstyan"}, {"name": "Rahul Gupta"}], "author_detail": {"name": "Rahul Gupta"}, "author": "Rahul Gupta", "links": [{"href": "http://arxiv.org/abs/2308.04265v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2308.04265v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}