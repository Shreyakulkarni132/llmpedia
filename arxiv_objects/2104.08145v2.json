{"id": "http://arxiv.org/abs/2104.08145v2", "guidislink": true, "link": "http://arxiv.org/abs/2104.08145v2", "updated": "2021-09-03T06:41:39Z", "updated_parsed": [2021, 9, 3, 6, 41, 39, 4, 246, 0], "published": "2021-04-09T16:15:31Z", "published_parsed": [2021, 4, 9, 16, 15, 31, 4, 99, 0], "title": "KI-BERT: Infusing Knowledge Context for Better Language and Domain\n  Understanding", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=ki+bert++infusing+knowledge+context+for+better+language+and+domain+understanding&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "KI-BERT: Infusing Knowledge Context for Better Language and Domain\n  Understanding"}, "summary": "Contextualized entity representations learned by state-of-the-art\ntransformer-based language models (TLMs) like BERT, GPT, T5, etc., leverage the\nattention mechanism to learn the data context from training data corpus.\nHowever, these models do not use the knowledge context. Knowledge context can\nbe understood as semantics about entities and their relationship with\nneighboring entities in knowledge graphs. We propose a novel and effective\ntechnique to infuse knowledge context from multiple knowledge graphs for\nconceptual and ambiguous entities into TLMs during fine-tuning. It projects\nknowledge graph embeddings in the homogeneous vector-space, introduces new\ntoken-types for entities, aligns entity position ids, and a selective attention\nmechanism. We take BERT as a baseline model and implement the\n\"Knowledge-Infused BERT\" by infusing knowledge context from ConceptNet and\nWordNet, which significantly outperforms BERT and other recent knowledge-aware\nBERT variants like ERNIE, SenseBERT, and BERT_CS over eight different subtasks\nof GLUE benchmark. The KI-BERT-base model even significantly outperforms\nBERT-large for domain-specific tasks like SciTail and academic subsets of QQP,\nQNLI, and MNLI.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=ki+bert++infusing+knowledge+context+for+better+language+and+domain+understanding&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Contextualized entity representations learned by state-of-the-art\ntransformer-based language models (TLMs) like BERT, GPT, T5, etc., leverage the\nattention mechanism to learn the data context from training data corpus.\nHowever, these models do not use the knowledge context. Knowledge context can\nbe understood as semantics about entities and their relationship with\nneighboring entities in knowledge graphs. We propose a novel and effective\ntechnique to infuse knowledge context from multiple knowledge graphs for\nconceptual and ambiguous entities into TLMs during fine-tuning. It projects\nknowledge graph embeddings in the homogeneous vector-space, introduces new\ntoken-types for entities, aligns entity position ids, and a selective attention\nmechanism. We take BERT as a baseline model and implement the\n\"Knowledge-Infused BERT\" by infusing knowledge context from ConceptNet and\nWordNet, which significantly outperforms BERT and other recent knowledge-aware\nBERT variants like ERNIE, SenseBERT, and BERT_CS over eight different subtasks\nof GLUE benchmark. The KI-BERT-base model even significantly outperforms\nBERT-large for domain-specific tasks like SciTail and academic subsets of QQP,\nQNLI, and MNLI."}, "authors": [{"name": "Keyur Faldu"}, {"name": "Amit Sheth"}, {"name": "Prashant Kikani"}, {"name": "Hemang Akbari"}], "author_detail": {"name": "Hemang Akbari"}, "author": "Hemang Akbari", "arxiv_comment": "10 pages, 4 figures, 4 tables", "links": [{"href": "http://arxiv.org/abs/2104.08145v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2104.08145v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}