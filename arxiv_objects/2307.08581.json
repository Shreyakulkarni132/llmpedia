{"id": "http://arxiv.org/abs/2307.08581v1", "guidislink": true, "link": "http://arxiv.org/abs/2307.08581v1", "updated": "2023-07-17T15:51:47Z", "updated_parsed": [2023, 7, 17, 15, 51, 47, 0, 198, 0], "published": "2023-07-17T15:51:47Z", "published_parsed": [2023, 7, 17, 15, 51, 47, 0, 198, 0], "title": "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bubogpt++enabling+visual+grounding+in+multi+modal+llms&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs"}, "summary": "LLMs have demonstrated remarkable abilities at interacting with humans\nthrough language, especially with the usage of instruction-following data.\nRecent advancements in LLMs, such as MiniGPT-4, LLaVA, and X-LLM, further\nenlarge their abilities by incorporating multi-modal inputs, including image,\nvideo, and speech. Despite their effectiveness at generating precise and\ndetailed language understanding of the given modality signal, these LLMs give\nup the ability to ground specific parts of inputs, thus only constructing a\ncoarse-grained mapping. However, explicit and informative correspondence\nbetween text and other modalities will not only improve the user experience but\nalso help to expand the application scenario of multi-modal LLMs. Therefore, we\npropose BuboGPT, a multi-modal LLM with visual grounding that can perform\ncross-modal interaction between vision, audio and language, providing\nfine-grained understanding of visual objects and other given modalities. As a\nresult, BuboGPT is able to point out the specific location of an object in the\nimage, when it is generating response or description for that object. Our\ncontributions are two-fold: 1) An off-the-shelf visual grounding module based\non SAM that extracts entities in a sentence and find corresponding masks in the\nimage. 2) A two-stage training scheme and instruction dataset to endow joint\ntext-image-audio understanding. Our experiments show that BuboGPT achieves\nimpressive multi-modality understanding and visual grounding abilities during\nthe interaction with human. It performs consistently well when provided by\narbitrary modality combinations (either aligned or unaligned). Our code, model\nand dataset are available at https://bubo-gpt.github.io .", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bubogpt++enabling+visual+grounding+in+multi+modal+llms&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "LLMs have demonstrated remarkable abilities at interacting with humans\nthrough language, especially with the usage of instruction-following data.\nRecent advancements in LLMs, such as MiniGPT-4, LLaVA, and X-LLM, further\nenlarge their abilities by incorporating multi-modal inputs, including image,\nvideo, and speech. Despite their effectiveness at generating precise and\ndetailed language understanding of the given modality signal, these LLMs give\nup the ability to ground specific parts of inputs, thus only constructing a\ncoarse-grained mapping. However, explicit and informative correspondence\nbetween text and other modalities will not only improve the user experience but\nalso help to expand the application scenario of multi-modal LLMs. Therefore, we\npropose BuboGPT, a multi-modal LLM with visual grounding that can perform\ncross-modal interaction between vision, audio and language, providing\nfine-grained understanding of visual objects and other given modalities. As a\nresult, BuboGPT is able to point out the specific location of an object in the\nimage, when it is generating response or description for that object. Our\ncontributions are two-fold: 1) An off-the-shelf visual grounding module based\non SAM that extracts entities in a sentence and find corresponding masks in the\nimage. 2) A two-stage training scheme and instruction dataset to endow joint\ntext-image-audio understanding. Our experiments show that BuboGPT achieves\nimpressive multi-modality understanding and visual grounding abilities during\nthe interaction with human. It performs consistently well when provided by\narbitrary modality combinations (either aligned or unaligned). Our code, model\nand dataset are available at https://bubo-gpt.github.io ."}, "authors": [{"name": "Yang Zhao"}, {"name": "Zhijie Lin"}, {"name": "Daquan Zhou"}, {"name": "Zilong Huang"}, {"name": "Jiashi Feng"}, {"name": "Bingyi Kang"}], "author_detail": {"name": "Bingyi Kang"}, "author": "Bingyi Kang", "links": [{"href": "http://arxiv.org/abs/2307.08581v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.08581v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}