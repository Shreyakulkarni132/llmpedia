{"id": "http://arxiv.org/abs/2308.04592v1", "guidislink": true, "link": "http://arxiv.org/abs/2308.04592v1", "updated": "2023-08-08T21:23:23Z", "updated_parsed": [2023, 8, 8, 21, 23, 23, 1, 220, 0], "published": "2023-08-08T21:23:23Z", "published_parsed": [2023, 8, 8, 21, 23, 23, 1, 220, 0], "title": "Shepherd: A Critic for Language Model Generation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=shepherd++a+critic+for+language+model+generation&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Shepherd: A Critic for Language Model Generation"}, "summary": "As large language models improve, there is increasing interest in techniques\nthat leverage these models' capabilities to refine their own outputs. In this\nwork, we introduce Shepherd, a language model specifically tuned to critique\nresponses and suggest refinements, extending beyond the capabilities of an\nuntuned model to identify diverse errors and provide suggestions to remedy\nthem. At the core of our approach is a high quality feedback dataset, which we\ncurate from community feedback and human annotations. Even though Shepherd is\nsmall (7B parameters), its critiques are either equivalent or preferred to\nthose from established models including ChatGPT. Using GPT-4 for evaluation,\nShepherd reaches an average win-rate of 53-87% compared to competitive\nalternatives. In human evaluation, Shepherd strictly outperforms other models\nand on average closely ties with ChatGPT.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=shepherd++a+critic+for+language+model+generation&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "As large language models improve, there is increasing interest in techniques\nthat leverage these models' capabilities to refine their own outputs. In this\nwork, we introduce Shepherd, a language model specifically tuned to critique\nresponses and suggest refinements, extending beyond the capabilities of an\nuntuned model to identify diverse errors and provide suggestions to remedy\nthem. At the core of our approach is a high quality feedback dataset, which we\ncurate from community feedback and human annotations. Even though Shepherd is\nsmall (7B parameters), its critiques are either equivalent or preferred to\nthose from established models including ChatGPT. Using GPT-4 for evaluation,\nShepherd reaches an average win-rate of 53-87% compared to competitive\nalternatives. In human evaluation, Shepherd strictly outperforms other models\nand on average closely ties with ChatGPT."}, "authors": [{"name": "Tianlu Wang"}, {"name": "Ping Yu"}, {"name": "Xiaoqing Ellen Tan"}, {"name": "Sean O'Brien"}, {"name": "Ramakanth Pasunuru"}, {"name": "Jane Dwivedi-Yu"}, {"name": "Olga Golovneva"}, {"name": "Luke Zettlemoyer"}, {"name": "Maryam Fazel-Zarandi"}, {"name": "Asli Celikyilmaz"}], "author_detail": {"name": "Asli Celikyilmaz"}, "author": "Asli Celikyilmaz", "arxiv_comment": "7 figures, 7 tables", "links": [{"href": "http://arxiv.org/abs/2308.04592v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2308.04592v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}