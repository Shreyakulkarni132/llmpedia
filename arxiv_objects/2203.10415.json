{"id": "http://arxiv.org/abs/2203.10415v1", "guidislink": true, "link": "http://arxiv.org/abs/2203.10415v1", "updated": "2022-03-20T00:02:10Z", "updated_parsed": [2022, 3, 20, 0, 2, 10, 6, 79, 0], "published": "2022-03-20T00:02:10Z", "published_parsed": [2022, 3, 20, 0, 2, 10, 6, 79, 0], "title": "How does the pre-training objective affect what large language models\n  learn about linguistic properties?", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=how+does+the+pre+training+objective+affect+what+large+language+models+learn+about+linguistic+properties+&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "How does the pre-training objective affect what large language models\n  learn about linguistic properties?"}, "summary": "Several pre-training objectives, such as masked language modeling (MLM), have\nbeen proposed to pre-train language models (e.g. BERT) with the aim of learning\nbetter language representations. However, to the best of our knowledge, no\nprevious work so far has investigated how different pre-training objectives\naffect what BERT learns about linguistics properties. We hypothesize that\nlinguistically motivated objectives such as MLM should help BERT to acquire\nbetter linguistic knowledge compared to other non-linguistically motivated\nobjectives that are not intuitive or hard for humans to guess the association\nbetween the input and the label to be predicted. To this end, we pre-train BERT\nwith two linguistically motivated objectives and three non-linguistically\nmotivated ones. We then probe for linguistic characteristics encoded in the\nrepresentation of the resulting models. We find strong evidence that there are\nonly small differences in probing performance between the representations\nlearned by the two different types of objectives. These surprising results\nquestion the dominant narrative of linguistically informed pre-training.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=how+does+the+pre+training+objective+affect+what+large+language+models+learn+about+linguistic+properties+&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Several pre-training objectives, such as masked language modeling (MLM), have\nbeen proposed to pre-train language models (e.g. BERT) with the aim of learning\nbetter language representations. However, to the best of our knowledge, no\nprevious work so far has investigated how different pre-training objectives\naffect what BERT learns about linguistics properties. We hypothesize that\nlinguistically motivated objectives such as MLM should help BERT to acquire\nbetter linguistic knowledge compared to other non-linguistically motivated\nobjectives that are not intuitive or hard for humans to guess the association\nbetween the input and the label to be predicted. To this end, we pre-train BERT\nwith two linguistically motivated objectives and three non-linguistically\nmotivated ones. We then probe for linguistic characteristics encoded in the\nrepresentation of the resulting models. We find strong evidence that there are\nonly small differences in probing performance between the representations\nlearned by the two different types of objectives. These surprising results\nquestion the dominant narrative of linguistically informed pre-training."}, "authors": [{"name": "Ahmed Alajrami"}, {"name": "Nikolaos Aletras"}], "author_detail": {"name": "Nikolaos Aletras"}, "author": "Nikolaos Aletras", "arxiv_comment": "Accepted at ACL 2022", "links": [{"href": "http://arxiv.org/abs/2203.10415v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2203.10415v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}