{"id": "http://arxiv.org/abs/2304.01938v1", "guidislink": true, "link": "http://arxiv.org/abs/2304.01938v1", "updated": "2023-04-01T06:04:58Z", "updated_parsed": [2023, 4, 1, 6, 4, 58, 5, 91, 0], "published": "2023-04-01T06:04:58Z", "published_parsed": [2023, 4, 1, 6, 4, 58, 5, 91, 0], "title": "Evaluating Large Language Models on a Highly-specialized Topic,\n  Radiation Oncology Physics", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=evaluating+large+language+models+on+a+highly+specialized+topic++radiation+oncology+physics&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Evaluating Large Language Models on a Highly-specialized Topic,\n  Radiation Oncology Physics"}, "summary": "We present the first study to investigate Large Language Models (LLMs) in\nanswering radiation oncology physics questions. Because popular exams like AP\nPhysics, LSAT, and GRE have large test-taker populations and ample test\npreparation resources in circulation, they may not allow for accurately\nassessing the true potential of LLMs. This paper proposes evaluating LLMs on a\nhighly-specialized topic, radiation oncology physics, which may be more\npertinent to scientific and medical communities in addition to being a valuable\nbenchmark of LLMs. We developed an exam consisting of 100 radiation oncology\nphysics questions based on our expertise at Mayo Clinic. Four LLMs, ChatGPT\n(GPT-3.5), ChatGPT (GPT-4), Bard (LaMDA), and BLOOMZ, were evaluated against\nmedical physicists and non-experts. ChatGPT (GPT-4) outperformed all other LLMs\nas well as medical physicists, on average. The performance of ChatGPT (GPT-4)\nwas further improved when prompted to explain first, then answer. ChatGPT\n(GPT-3.5 and GPT-4) showed a high level of consistency in its answer choices\nacross a number of trials, whether correct or incorrect, a characteristic that\nwas not observed in the human test groups. In evaluating ChatGPTs (GPT-4)\ndeductive reasoning ability using a novel approach (substituting the correct\nanswer with \"None of the above choices is the correct answer.\"), ChatGPT\n(GPT-4) demonstrated surprising accuracy, suggesting the potential presence of\nan emergent ability. Finally, although ChatGPT (GPT-4) performed well overall,\nits intrinsic properties did not allow for further improvement when scoring\nbased on a majority vote across trials. In contrast, a team of medical\nphysicists were able to greatly outperform ChatGPT (GPT-4) using a majority\nvote. This study suggests a great potential for LLMs to work alongside\nradiation oncology experts as highly knowledgeable assistants.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=evaluating+large+language+models+on+a+highly+specialized+topic++radiation+oncology+physics&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "We present the first study to investigate Large Language Models (LLMs) in\nanswering radiation oncology physics questions. Because popular exams like AP\nPhysics, LSAT, and GRE have large test-taker populations and ample test\npreparation resources in circulation, they may not allow for accurately\nassessing the true potential of LLMs. This paper proposes evaluating LLMs on a\nhighly-specialized topic, radiation oncology physics, which may be more\npertinent to scientific and medical communities in addition to being a valuable\nbenchmark of LLMs. We developed an exam consisting of 100 radiation oncology\nphysics questions based on our expertise at Mayo Clinic. Four LLMs, ChatGPT\n(GPT-3.5), ChatGPT (GPT-4), Bard (LaMDA), and BLOOMZ, were evaluated against\nmedical physicists and non-experts. ChatGPT (GPT-4) outperformed all other LLMs\nas well as medical physicists, on average. The performance of ChatGPT (GPT-4)\nwas further improved when prompted to explain first, then answer. ChatGPT\n(GPT-3.5 and GPT-4) showed a high level of consistency in its answer choices\nacross a number of trials, whether correct or incorrect, a characteristic that\nwas not observed in the human test groups. In evaluating ChatGPTs (GPT-4)\ndeductive reasoning ability using a novel approach (substituting the correct\nanswer with \"None of the above choices is the correct answer.\"), ChatGPT\n(GPT-4) demonstrated surprising accuracy, suggesting the potential presence of\nan emergent ability. Finally, although ChatGPT (GPT-4) performed well overall,\nits intrinsic properties did not allow for further improvement when scoring\nbased on a majority vote across trials. In contrast, a team of medical\nphysicists were able to greatly outperform ChatGPT (GPT-4) using a majority\nvote. This study suggests a great potential for LLMs to work alongside\nradiation oncology experts as highly knowledgeable assistants."}, "authors": [{"name": "Jason Holmes"}, {"name": "Zhengliang Liu"}, {"name": "Lian Zhang"}, {"name": "Yuzhen Ding"}, {"name": "Terence T. Sio"}, {"name": "Lisa A. McGee"}, {"name": "Jonathan B. Ashman"}, {"name": "Xiang Li"}, {"name": "Tianming Liu"}, {"name": "Jiajian Shen"}, {"name": "Wei Liu"}], "author_detail": {"name": "Wei Liu"}, "author": "Wei Liu", "links": [{"href": "http://arxiv.org/abs/2304.01938v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2304.01938v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "physics.med-ph", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "physics.med-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "physics.ed-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}