{"id": "http://arxiv.org/abs/2210.05359v1", "guidislink": true, "link": "http://arxiv.org/abs/2210.05359v1", "updated": "2022-10-11T11:39:23Z", "updated_parsed": [2022, 10, 11, 11, 39, 23, 1, 284, 0], "published": "2022-10-11T11:39:23Z", "published_parsed": [2022, 10, 11, 11, 39, 23, 1, 284, 0], "title": "Mind's Eye: Grounded Language Model Reasoning through Simulation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=mind+s+eye++grounded+language+model+reasoning+through+simulation&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Mind's Eye: Grounded Language Model Reasoning through Simulation"}, "summary": "Successful and effective communication between humans and AI relies on a\nshared experience of the world. By training solely on written text, current\nlanguage models (LMs) miss the grounded experience of humans in the real-world\n-- their failure to relate language to the physical world causes knowledge to\nbe misrepresented and obvious mistakes in their reasoning. We present Mind's\nEye, a paradigm to ground language model reasoning in the physical world. Given\na physical reasoning question, we use a computational physics engine\n(DeepMind's MuJoCo) to simulate the possible outcomes, and then use the\nsimulation results as part of the input, which enables language models to\nperform reasoning. Experiments on 39 tasks in a physics alignment benchmark\ndemonstrate that Mind's Eye can improve reasoning ability by a large margin\n(27.9% zero-shot, and 46.0% few-shot absolute accuracy improvement on average).\nSmaller language models armed with Mind's Eye can obtain similar performance to\nmodels that are 100x larger. Finally, we confirm the robustness of Mind's Eye\nthrough ablation studies.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=mind+s+eye++grounded+language+model+reasoning+through+simulation&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Successful and effective communication between humans and AI relies on a\nshared experience of the world. By training solely on written text, current\nlanguage models (LMs) miss the grounded experience of humans in the real-world\n-- their failure to relate language to the physical world causes knowledge to\nbe misrepresented and obvious mistakes in their reasoning. We present Mind's\nEye, a paradigm to ground language model reasoning in the physical world. Given\na physical reasoning question, we use a computational physics engine\n(DeepMind's MuJoCo) to simulate the possible outcomes, and then use the\nsimulation results as part of the input, which enables language models to\nperform reasoning. Experiments on 39 tasks in a physics alignment benchmark\ndemonstrate that Mind's Eye can improve reasoning ability by a large margin\n(27.9% zero-shot, and 46.0% few-shot absolute accuracy improvement on average).\nSmaller language models armed with Mind's Eye can obtain similar performance to\nmodels that are 100x larger. Finally, we confirm the robustness of Mind's Eye\nthrough ablation studies."}, "authors": [{"name": "Ruibo Liu"}, {"name": "Jason Wei"}, {"name": "Shixiang Shane Gu"}, {"name": "Te-Yen Wu"}, {"name": "Soroush Vosoughi"}, {"name": "Claire Cui"}, {"name": "Denny Zhou"}, {"name": "Andrew M. Dai"}], "author_detail": {"name": "Andrew M. Dai"}, "author": "Andrew M. Dai", "links": [{"href": "http://arxiv.org/abs/2210.05359v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2210.05359v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}