{"id": "http://arxiv.org/abs/2212.10846v3", "guidislink": true, "link": "http://arxiv.org/abs/2212.10846v3", "updated": "2023-05-08T06:04:04Z", "updated_parsed": [2023, 5, 8, 6, 4, 4, 0, 128, 0], "published": "2022-12-21T08:39:36Z", "published_parsed": [2022, 12, 21, 8, 39, 36, 2, 355, 0], "title": "From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language\n  Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=from+images+to+textual+prompts++zero+shot+vqa+with+frozen+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language\n  Models"}, "summary": "Large language models (LLMs) have demonstrated excellent zero-shot\ngeneralization to new language tasks. However, effective utilization of LLMs\nfor zero-shot visual question-answering (VQA) remains challenging, primarily\ndue to the modality disconnection and task disconnection between LLM and VQA\ntask. End-to-end training on vision and language data may bridge the\ndisconnections, but is inflexible and computationally expensive. To address\nthis issue, we propose \\emph{Img2Prompt}, a plug-and-play module that provides\nthe prompts that can bridge the aforementioned modality and task\ndisconnections, so that LLMs can perform zero-shot VQA tasks without end-to-end\ntraining. In order to provide such prompts, we further employ LLM-agnostic\nmodels to provide prompts that can describe image content and self-constructed\nquestion-answer pairs, which can effectively guide LLM to perform zero-shot VQA\ntasks. Img2Prompt offers the following benefits: 1) It can flexibly work with\nvarious LLMs to perform VQA. 2)~Without the needing of end-to-end training, it\nsignificantly reduces the cost of deploying LLM for zero-shot VQA tasks. 3) It\nachieves comparable or better performance than methods relying on end-to-end\ntraining. For example, we outperform Flamingo \\cite{Deepmind:Flamingo2022} by\n5.6\\% on VQAv2. On the challenging A-OKVQA dataset, our method even outperforms\nfew-shot methods by as much as 20\\%.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=from+images+to+textual+prompts++zero+shot+vqa+with+frozen+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Large language models (LLMs) have demonstrated excellent zero-shot\ngeneralization to new language tasks. However, effective utilization of LLMs\nfor zero-shot visual question-answering (VQA) remains challenging, primarily\ndue to the modality disconnection and task disconnection between LLM and VQA\ntask. End-to-end training on vision and language data may bridge the\ndisconnections, but is inflexible and computationally expensive. To address\nthis issue, we propose \\emph{Img2Prompt}, a plug-and-play module that provides\nthe prompts that can bridge the aforementioned modality and task\ndisconnections, so that LLMs can perform zero-shot VQA tasks without end-to-end\ntraining. In order to provide such prompts, we further employ LLM-agnostic\nmodels to provide prompts that can describe image content and self-constructed\nquestion-answer pairs, which can effectively guide LLM to perform zero-shot VQA\ntasks. Img2Prompt offers the following benefits: 1) It can flexibly work with\nvarious LLMs to perform VQA. 2)~Without the needing of end-to-end training, it\nsignificantly reduces the cost of deploying LLM for zero-shot VQA tasks. 3) It\nachieves comparable or better performance than methods relying on end-to-end\ntraining. For example, we outperform Flamingo \\cite{Deepmind:Flamingo2022} by\n5.6\\% on VQAv2. On the challenging A-OKVQA dataset, our method even outperforms\nfew-shot methods by as much as 20\\%."}, "authors": [{"name": "Jiaxian Guo"}, {"name": "Junnan Li"}, {"name": "Dongxu Li"}, {"name": "Anthony Meng Huat Tiong"}, {"name": "Boyang Li"}, {"name": "Dacheng Tao"}, {"name": "Steven C. H. Hoi"}], "author_detail": {"name": "Steven C. H. Hoi"}, "author": "Steven C. H. Hoi", "arxiv_comment": "CVPR 2023 Camera Ready Version", "links": [{"href": "http://arxiv.org/abs/2212.10846v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2212.10846v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.MM", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}