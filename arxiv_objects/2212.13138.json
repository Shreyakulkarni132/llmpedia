{"id": "http://arxiv.org/abs/2212.13138v1", "guidislink": true, "link": "http://arxiv.org/abs/2212.13138v1", "updated": "2022-12-26T14:28:24Z", "updated_parsed": [2022, 12, 26, 14, 28, 24, 0, 360, 0], "published": "2022-12-26T14:28:24Z", "published_parsed": [2022, 12, 26, 14, 28, 24, 0, 360, 0], "title": "Large Language Models Encode Clinical Knowledge", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=large+language+models+encode+clinical+knowledge&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Large Language Models Encode Clinical Knowledge"}, "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but the quality bar for medical\nand clinical applications is high. Today, attempts to assess models' clinical\nknowledge typically rely on automated evaluations on limited benchmarks. There\nis no standard to evaluate model predictions and reasoning across a breadth of\ntasks. To address this, we present MultiMedQA, a benchmark combining six\nexisting open question answering datasets spanning professional medical exams,\nresearch, and consumer queries; and HealthSearchQA, a new free-response dataset\nof medical questions searched online. We propose a framework for human\nevaluation of model answers along multiple axes including factuality,\nprecision, possible harm, and bias. In addition, we evaluate PaLM (a\n540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, on\nMultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves\nstate-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA,\nMedMCQA, PubMedQA, MMLU clinical topics), including 67.6% accuracy on MedQA (US\nMedical License Exam questions), surpassing prior state-of-the-art by over 17%.\nHowever, human evaluation reveals key gaps in Flan-PaLM responses. To resolve\nthis we introduce instruction prompt tuning, a parameter-efficient approach for\naligning LLMs to new domains using a few exemplars. The resulting model,\nMed-PaLM, performs encouragingly, but remains inferior to clinicians. We show\nthat comprehension, recall of knowledge, and medical reasoning improve with\nmodel scale and instruction prompt tuning, suggesting the potential utility of\nLLMs in medicine. Our human evaluations reveal important limitations of today's\nmodels, reinforcing the importance of both evaluation frameworks and method\ndevelopment in creating safe, helpful LLM models for clinical applications.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=large+language+models+encode+clinical+knowledge&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but the quality bar for medical\nand clinical applications is high. Today, attempts to assess models' clinical\nknowledge typically rely on automated evaluations on limited benchmarks. There\nis no standard to evaluate model predictions and reasoning across a breadth of\ntasks. To address this, we present MultiMedQA, a benchmark combining six\nexisting open question answering datasets spanning professional medical exams,\nresearch, and consumer queries; and HealthSearchQA, a new free-response dataset\nof medical questions searched online. We propose a framework for human\nevaluation of model answers along multiple axes including factuality,\nprecision, possible harm, and bias. In addition, we evaluate PaLM (a\n540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, on\nMultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves\nstate-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA,\nMedMCQA, PubMedQA, MMLU clinical topics), including 67.6% accuracy on MedQA (US\nMedical License Exam questions), surpassing prior state-of-the-art by over 17%.\nHowever, human evaluation reveals key gaps in Flan-PaLM responses. To resolve\nthis we introduce instruction prompt tuning, a parameter-efficient approach for\naligning LLMs to new domains using a few exemplars. The resulting model,\nMed-PaLM, performs encouragingly, but remains inferior to clinicians. We show\nthat comprehension, recall of knowledge, and medical reasoning improve with\nmodel scale and instruction prompt tuning, suggesting the potential utility of\nLLMs in medicine. Our human evaluations reveal important limitations of today's\nmodels, reinforcing the importance of both evaluation frameworks and method\ndevelopment in creating safe, helpful LLM models for clinical applications."}, "authors": [{"name": "Karan Singhal"}, {"name": "Shekoofeh Azizi"}, {"name": "Tao Tu"}, {"name": "S. Sara Mahdavi"}, {"name": "Jason Wei"}, {"name": "Hyung Won Chung"}, {"name": "Nathan Scales"}, {"name": "Ajay Tanwani"}, {"name": "Heather Cole-Lewis"}, {"name": "Stephen Pfohl"}, {"name": "Perry Payne"}, {"name": "Martin Seneviratne"}, {"name": "Paul Gamble"}, {"name": "Chris Kelly"}, {"name": "Nathaneal Scharli"}, {"name": "Aakanksha Chowdhery"}, {"name": "Philip Mansfield"}, {"name": "Blaise Aguera y Arcas"}, {"name": "Dale Webster"}, {"name": "Greg S. Corrado"}, {"name": "Yossi Matias"}, {"name": "Katherine Chou"}, {"name": "Juraj Gottweis"}, {"name": "Nenad Tomasev"}, {"name": "Yun Liu"}, {"name": "Alvin Rajkomar"}, {"name": "Joelle Barral"}, {"name": "Christopher Semturs"}, {"name": "Alan Karthikesalingam"}, {"name": "Vivek Natarajan"}], "author_detail": {"name": "Vivek Natarajan"}, "author": "Vivek Natarajan", "links": [{"href": "http://arxiv.org/abs/2212.13138v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2212.13138v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}