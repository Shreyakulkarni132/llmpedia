{"id": "http://arxiv.org/abs/2307.11088v2", "guidislink": true, "link": "http://arxiv.org/abs/2307.11088v2", "updated": "2023-07-31T17:19:52Z", "updated_parsed": [2023, 7, 31, 17, 19, 52, 0, 212, 0], "published": "2023-07-20T17:59:41Z", "published_parsed": [2023, 7, 20, 17, 59, 41, 3, 201, 0], "title": "L-Eval: Instituting Standardized Evaluation for Long Context Language\n  Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=l+eval++instituting+standardized+evaluation+for+long+context+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "L-Eval: Instituting Standardized Evaluation for Long Context Language\n  Models"}, "summary": "Recently, there has been growing interest in extending the context length of\ninstruction-following models in order to effectively process single-turn long\ninput (e.g. summarizing a paper) and conversations with more extensive\nhistories. While proprietary models such as GPT-4 and Claude have shown\nsignificant strides in handling extremely lengthy input, open-sourced models\nare still in the early stages of experimentation. It also remains unclear\nwhether extending the context can offer substantial gains over traditional\nmethods such as retrieval, and to what extent it improves upon their regular\ncounterparts in practical downstream tasks. To address this challenge, we\npropose instituting standardized evaluation for long context language models.\nConcretely, we develop L-Eval which contains 411 long documents and over 2,000\nhuman-labeled query-response pairs encompassing areas such as law, finance,\nschool lectures, lengthy conversations, news, long-form novels, and meetings.\nL-Eval also adopts diverse evaluation methods and instruction styles, enabling\na more reliable assessment of Long Context Language Models (LCLMs). Our\nfindings indicate that while open-source models typically lag behind commercial\nmodels, they still exhibit impressive performance compared with their regular\nversions. LLaMA2-13B achieves the best results on both open-ended tasks (win\n\\textbf{42}\\% vs turbo-16k-0613) and closed-ended tasks with only 4k context\nlength. We release our new evaluation suite, code, and all generation results\nincluding predictions from all open-sourced LCLMs, GPT4-32k, Cluade-100k at\n{\\url{https://github.com/OpenLMLab/LEval}}.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=l+eval++instituting+standardized+evaluation+for+long+context+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Recently, there has been growing interest in extending the context length of\ninstruction-following models in order to effectively process single-turn long\ninput (e.g. summarizing a paper) and conversations with more extensive\nhistories. While proprietary models such as GPT-4 and Claude have shown\nsignificant strides in handling extremely lengthy input, open-sourced models\nare still in the early stages of experimentation. It also remains unclear\nwhether extending the context can offer substantial gains over traditional\nmethods such as retrieval, and to what extent it improves upon their regular\ncounterparts in practical downstream tasks. To address this challenge, we\npropose instituting standardized evaluation for long context language models.\nConcretely, we develop L-Eval which contains 411 long documents and over 2,000\nhuman-labeled query-response pairs encompassing areas such as law, finance,\nschool lectures, lengthy conversations, news, long-form novels, and meetings.\nL-Eval also adopts diverse evaluation methods and instruction styles, enabling\na more reliable assessment of Long Context Language Models (LCLMs). Our\nfindings indicate that while open-source models typically lag behind commercial\nmodels, they still exhibit impressive performance compared with their regular\nversions. LLaMA2-13B achieves the best results on both open-ended tasks (win\n\\textbf{42}\\% vs turbo-16k-0613) and closed-ended tasks with only 4k context\nlength. We release our new evaluation suite, code, and all generation results\nincluding predictions from all open-sourced LCLMs, GPT4-32k, Cluade-100k at\n{\\url{https://github.com/OpenLMLab/LEval}}."}, "authors": [{"name": "Chenxin An"}, {"name": "Shansan Gong"}, {"name": "Ming Zhong"}, {"name": "Mukai Li"}, {"name": "Jun Zhang"}, {"name": "Lingpeng Kong"}, {"name": "Xipeng Qiu"}], "author_detail": {"name": "Xipeng Qiu"}, "author": "Xipeng Qiu", "links": [{"href": "http://arxiv.org/abs/2307.11088v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.11088v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}