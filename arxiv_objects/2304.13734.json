{"id": "http://arxiv.org/abs/2304.13734v1", "guidislink": true, "link": "http://arxiv.org/abs/2304.13734v1", "updated": "2023-04-26T02:49:38Z", "updated_parsed": [2023, 4, 26, 2, 49, 38, 2, 116, 0], "published": "2023-04-26T02:49:38Z", "published_parsed": [2023, 4, 26, 2, 49, 38, 2, 116, 0], "title": "The Internal State of an LLM Knows When its Lying", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=the+internal+state+of+an+llm+knows+when+its+lying&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "The Internal State of an LLM Knows When its Lying"}, "summary": "While Large Language Models (LLMs) have shown exceptional performance in\nvarious tasks, their (arguably) most prominent drawback is generating\ninaccurate or false information with a confident tone. In this paper, we\nhypothesize that the LLM's internal state can be used to reveal the\ntruthfulness of a statement. Therefore, we introduce a simple yet effective\nmethod to detect the truthfulness of LLM-generated statements, which utilizes\nthe LLM's hidden layer activations to determine the veracity of statements. To\ntrain and evaluate our method, we compose a dataset of true and false\nstatements in six different topics. A classifier is trained to detect which\nstatement is true or false based on an LLM's activation values. Specifically,\nthe classifier receives as input the activation values from the LLM for each of\nthe statements in the dataset. Our experiments demonstrate that our method for\ndetecting statement veracity significantly outperforms even few-shot prompting\nmethods, highlighting its potential to enhance the reliability of LLM-generated\ncontent and its practical applicability in real-world scenarios.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=the+internal+state+of+an+llm+knows+when+its+lying&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "While Large Language Models (LLMs) have shown exceptional performance in\nvarious tasks, their (arguably) most prominent drawback is generating\ninaccurate or false information with a confident tone. In this paper, we\nhypothesize that the LLM's internal state can be used to reveal the\ntruthfulness of a statement. Therefore, we introduce a simple yet effective\nmethod to detect the truthfulness of LLM-generated statements, which utilizes\nthe LLM's hidden layer activations to determine the veracity of statements. To\ntrain and evaluate our method, we compose a dataset of true and false\nstatements in six different topics. A classifier is trained to detect which\nstatement is true or false based on an LLM's activation values. Specifically,\nthe classifier receives as input the activation values from the LLM for each of\nthe statements in the dataset. Our experiments demonstrate that our method for\ndetecting statement veracity significantly outperforms even few-shot prompting\nmethods, highlighting its potential to enhance the reliability of LLM-generated\ncontent and its practical applicability in real-world scenarios."}, "authors": [{"name": "Amos Azaria"}, {"name": "Tom Mitchell"}], "author_detail": {"name": "Tom Mitchell"}, "author": "Tom Mitchell", "links": [{"href": "http://arxiv.org/abs/2304.13734v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2304.13734v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}