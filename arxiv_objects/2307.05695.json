{"id": "http://arxiv.org/abs/2307.05695v2", "guidislink": true, "link": "http://arxiv.org/abs/2307.05695v2", "updated": "2023-07-13T19:31:52Z", "updated_parsed": [2023, 7, 13, 19, 31, 52, 3, 194, 0], "published": "2023-07-11T18:02:09Z", "published_parsed": [2023, 7, 11, 18, 2, 9, 1, 192, 0], "title": "Stack More Layers Differently: High-Rank Training Through Low-Rank\n  Updates", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=stack+more+layers+differently++high+rank+training+through+low+rank+updates&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Stack More Layers Differently: High-Rank Training Through Low-Rank\n  Updates"}, "summary": "Despite the dominance and effectiveness of scaling, resulting in large\nnetworks with hundreds of billions of parameters, the necessity to train\noverparametrized models remains poorly understood, and alternative approaches\ndo not necessarily make it cheaper to train high-performance models. In this\npaper, we explore low-rank training techniques as an alternative approach to\ntraining large neural networks. We introduce a novel method called ReLoRA,\nwhich utilizes low-rank updates to train high-rank networks. We apply ReLoRA to\npre-training transformer language models with up to 350M parameters and\ndemonstrate comparable performance to regular neural network training.\nFurthermore, we observe that the efficiency of ReLoRA increases with model\nsize, making it a promising approach for training multi-billion-parameter\nnetworks efficiently. Our findings shed light on the potential of low-rank\ntraining techniques and their implications for scaling laws.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=stack+more+layers+differently++high+rank+training+through+low+rank+updates&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Despite the dominance and effectiveness of scaling, resulting in large\nnetworks with hundreds of billions of parameters, the necessity to train\noverparametrized models remains poorly understood, and alternative approaches\ndo not necessarily make it cheaper to train high-performance models. In this\npaper, we explore low-rank training techniques as an alternative approach to\ntraining large neural networks. We introduce a novel method called ReLoRA,\nwhich utilizes low-rank updates to train high-rank networks. We apply ReLoRA to\npre-training transformer language models with up to 350M parameters and\ndemonstrate comparable performance to regular neural network training.\nFurthermore, we observe that the efficiency of ReLoRA increases with model\nsize, making it a promising approach for training multi-billion-parameter\nnetworks efficiently. Our findings shed light on the potential of low-rank\ntraining techniques and their implications for scaling laws."}, "authors": [{"name": "Vladislav Lialin"}, {"name": "Namrata Shivagunde"}, {"name": "Sherin Muckatira"}, {"name": "Anna Rumshisky"}], "author_detail": {"name": "Anna Rumshisky"}, "author": "Anna Rumshisky", "links": [{"href": "http://arxiv.org/abs/2307.05695v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.05695v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}