{"id": "http://arxiv.org/abs/2307.13692v2", "guidislink": true, "link": "http://arxiv.org/abs/2307.13692v2", "updated": "2023-07-28T03:31:08Z", "updated_parsed": [2023, 7, 28, 3, 31, 8, 4, 209, 0], "published": "2023-07-25T17:55:19Z", "published_parsed": [2023, 7, 25, 17, 55, 19, 1, 206, 0], "title": "ARB: Advanced Reasoning Benchmark for Large Language Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=arb++advanced+reasoning+benchmark+for+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "ARB: Advanced Reasoning Benchmark for Large Language Models"}, "summary": "Large Language Models (LLMs) have demonstrated remarkable performance on\nvarious quantitative reasoning and knowledge benchmarks. However, many of these\nbenchmarks are losing utility as LLMs get increasingly high scores, despite not\nyet reaching expert performance in these domains. We introduce ARB, a novel\nbenchmark composed of advanced reasoning problems in multiple fields. ARB\npresents a more challenging test than prior benchmarks, featuring problems in\nmathematics, physics, biology, chemistry, and law. As a subset of ARB, we\nintroduce a challenging set of math and physics problems which require advanced\nsymbolic reasoning and domain knowledge. We evaluate recent models such as\nGPT-4 and Claude on ARB and demonstrate that current models score well below\n50% on more demanding tasks. In order to improve both automatic and assisted\nevaluation capabilities, we introduce a rubric-based evaluation approach,\nallowing GPT-4 to score its own intermediate reasoning steps. Further, we\nconduct a human evaluation of the symbolic subset of ARB, finding promising\nagreement between annotators and GPT-4 rubric evaluation scores.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=arb++advanced+reasoning+benchmark+for+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Large Language Models (LLMs) have demonstrated remarkable performance on\nvarious quantitative reasoning and knowledge benchmarks. However, many of these\nbenchmarks are losing utility as LLMs get increasingly high scores, despite not\nyet reaching expert performance in these domains. We introduce ARB, a novel\nbenchmark composed of advanced reasoning problems in multiple fields. ARB\npresents a more challenging test than prior benchmarks, featuring problems in\nmathematics, physics, biology, chemistry, and law. As a subset of ARB, we\nintroduce a challenging set of math and physics problems which require advanced\nsymbolic reasoning and domain knowledge. We evaluate recent models such as\nGPT-4 and Claude on ARB and demonstrate that current models score well below\n50% on more demanding tasks. In order to improve both automatic and assisted\nevaluation capabilities, we introduce a rubric-based evaluation approach,\nallowing GPT-4 to score its own intermediate reasoning steps. Further, we\nconduct a human evaluation of the symbolic subset of ARB, finding promising\nagreement between annotators and GPT-4 rubric evaluation scores."}, "authors": [{"name": "Tomohiro Sawada"}, {"name": "Daniel Paleka"}, {"name": "Alexander Havrilla"}, {"name": "Pranav Tadepalli"}, {"name": "Paula Vidas"}, {"name": "Alexander Kranias"}, {"name": "John J. Nay"}, {"name": "Kshitij Gupta"}, {"name": "Aran Komatsuzaki"}], "author_detail": {"name": "Aran Komatsuzaki"}, "author": "Aran Komatsuzaki", "arxiv_comment": "Submitted to NeurIPS Datasets and Benchmarks Track", "links": [{"href": "http://arxiv.org/abs/2307.13692v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.13692v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}