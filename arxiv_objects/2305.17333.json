{"id": "http://arxiv.org/abs/2305.17333v1", "guidislink": true, "link": "http://arxiv.org/abs/2305.17333v1", "updated": "2023-05-27T02:28:10Z", "updated_parsed": [2023, 5, 27, 2, 28, 10, 5, 147, 0], "published": "2023-05-27T02:28:10Z", "published_parsed": [2023, 5, 27, 2, 28, 10, 5, 147, 0], "title": "Fine-Tuning Language Models with Just Forward Passes", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=fine+tuning+language+models+with+just+forward+passes&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Fine-Tuning Language Models with Just Forward Passes"}, "summary": "Fine-tuning language models (LMs) has yielded success on diverse downstream\ntasks, but as LMs grow in size, backpropagation requires a prohibitively large\namount of memory. Zeroth-order (ZO) methods can in principle estimate gradients\nusing only two forward passes but are theorized to be catastrophically slow for\noptimizing large models. In this work, we propose a memory-efficient\nzerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate\nin-place, thereby fine-tuning LMs with the same memory footprint as inference.\nFor example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter\nmodel, whereas fine-tuning with backpropagation can train only a 2.7B LM with\nthe same budget. We conduct comprehensive experiments across model types\n(masked and autoregressive LMs), model scales (up to 66B), and downstream tasks\n(classification, multiple-choice, and generation). Our results demonstrate that\n(1) MeZO significantly outperforms in-context learning and linear probing; (2)\nMeZO achieves comparable performance to fine-tuning with backpropagation across\nmultiple tasks, with up to 12x memory reduction; (3) MeZO is compatible with\nboth full-parameter and parameter-efficient tuning techniques such as LoRA and\nprefix tuning; (4) MeZO can effectively optimize non-differentiable objectives\n(e.g., maximizing accuracy or F1). We support our empirical findings with\ntheoretical insights, highlighting how adequate pre-training and task prompts\nenable MeZO to fine-tune huge models, despite classical ZO analyses suggesting\notherwise.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=fine+tuning+language+models+with+just+forward+passes&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Fine-tuning language models (LMs) has yielded success on diverse downstream\ntasks, but as LMs grow in size, backpropagation requires a prohibitively large\namount of memory. Zeroth-order (ZO) methods can in principle estimate gradients\nusing only two forward passes but are theorized to be catastrophically slow for\noptimizing large models. In this work, we propose a memory-efficient\nzerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate\nin-place, thereby fine-tuning LMs with the same memory footprint as inference.\nFor example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter\nmodel, whereas fine-tuning with backpropagation can train only a 2.7B LM with\nthe same budget. We conduct comprehensive experiments across model types\n(masked and autoregressive LMs), model scales (up to 66B), and downstream tasks\n(classification, multiple-choice, and generation). Our results demonstrate that\n(1) MeZO significantly outperforms in-context learning and linear probing; (2)\nMeZO achieves comparable performance to fine-tuning with backpropagation across\nmultiple tasks, with up to 12x memory reduction; (3) MeZO is compatible with\nboth full-parameter and parameter-efficient tuning techniques such as LoRA and\nprefix tuning; (4) MeZO can effectively optimize non-differentiable objectives\n(e.g., maximizing accuracy or F1). We support our empirical findings with\ntheoretical insights, highlighting how adequate pre-training and task prompts\nenable MeZO to fine-tune huge models, despite classical ZO analyses suggesting\notherwise."}, "authors": [{"name": "Sadhika Malladi"}, {"name": "Tianyu Gao"}, {"name": "Eshaan Nichani"}, {"name": "Alex Damian"}, {"name": "Jason D. Lee"}, {"name": "Danqi Chen"}, {"name": "Sanjeev Arora"}], "author_detail": {"name": "Sanjeev Arora"}, "author": "Sanjeev Arora", "arxiv_comment": "Code available at https://github.com/princeton-nlp/MeZO", "links": [{"href": "http://arxiv.org/abs/2305.17333v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2305.17333v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}