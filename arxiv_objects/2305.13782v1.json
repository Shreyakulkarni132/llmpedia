{"id": "http://arxiv.org/abs/2305.13782v1", "guidislink": true, "link": "http://arxiv.org/abs/2305.13782v1", "updated": "2023-05-23T07:50:36Z", "updated_parsed": [2023, 5, 23, 7, 50, 36, 1, 143, 0], "published": "2023-05-23T07:50:36Z", "published_parsed": [2023, 5, 23, 7, 50, 36, 1, 143, 0], "title": "Images in Language Space: Exploring the Suitability of Large Language\n  Models for Vision & Language Tasks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=images+in+language+space++exploring+the+suitability+of+large+language+models+for+vision+++language+tasks&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Images in Language Space: Exploring the Suitability of Large Language\n  Models for Vision & Language Tasks"}, "summary": "Large language models have demonstrated robust performance on various\nlanguage tasks using zero-shot or few-shot learning paradigms. While being\nactively researched, multimodal models that can additionally handle images as\ninput have yet to catch up in size and generality with language-only models. In\nthis work, we ask whether language-only models can be utilised for tasks that\nrequire visual input -- but also, as we argue, often require a strong reasoning\ncomponent. Similar to some recent related work, we make visual information\naccessible to the language model using separate verbalisation models.\nSpecifically, we investigate the performance of open-source, open-access\nlanguage models against GPT-3 on five vision-language tasks when given\ntextually-encoded visual information. Our results suggest that language models\nare effective for solving vision-language tasks even with limited samples. This\napproach also enhances the interpretability of a model's output by providing a\nmeans of tracing the output back through the verbalised image content.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=images+in+language+space++exploring+the+suitability+of+large+language+models+for+vision+++language+tasks&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Large language models have demonstrated robust performance on various\nlanguage tasks using zero-shot or few-shot learning paradigms. While being\nactively researched, multimodal models that can additionally handle images as\ninput have yet to catch up in size and generality with language-only models. In\nthis work, we ask whether language-only models can be utilised for tasks that\nrequire visual input -- but also, as we argue, often require a strong reasoning\ncomponent. Similar to some recent related work, we make visual information\naccessible to the language model using separate verbalisation models.\nSpecifically, we investigate the performance of open-source, open-access\nlanguage models against GPT-3 on five vision-language tasks when given\ntextually-encoded visual information. Our results suggest that language models\nare effective for solving vision-language tasks even with limited samples. This\napproach also enhances the interpretability of a model's output by providing a\nmeans of tracing the output back through the verbalised image content."}, "authors": [{"name": "Sherzod Hakimov"}, {"name": "David Schlangen"}], "author_detail": {"name": "David Schlangen"}, "author": "David Schlangen", "arxiv_comment": "Accepted at ACL 2023 Findings", "links": [{"href": "http://arxiv.org/abs/2305.13782v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2305.13782v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}