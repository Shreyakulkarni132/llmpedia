{"id": "http://arxiv.org/abs/2306.00029v1", "guidislink": true, "link": "http://arxiv.org/abs/2306.00029v1", "updated": "2023-05-31T05:24:48Z", "updated_parsed": [2023, 5, 31, 5, 24, 48, 2, 151, 0], "published": "2023-05-31T05:24:48Z", "published_parsed": [2023, 5, 31, 5, 24, 48, 2, 151, 0], "title": "CodeTF: One-stop Transformer Library for State-of-the-art Code LLM", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=codetf++one+stop+transformer+library+for+state+of+the+art+code+llm&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "CodeTF: One-stop Transformer Library for State-of-the-art Code LLM"}, "summary": "Code intelligence plays a key role in transforming modern software\nengineering. Recently, deep learning-based models, especially Transformer-based\nlarge language models (LLMs), have demonstrated remarkable potential in\ntackling these tasks by leveraging massive open-source code data and\nprogramming language features. However, the development and deployment of such\nmodels often require expertise in both machine learning and software\nengineering, creating a barrier for the model adoption. In this paper, we\npresent CodeTF, an open-source Transformer-based library for state-of-the-art\nCode LLMs and code intelligence. Following the principles of modular design and\nextensible framework, we design CodeTF with a unified interface to enable rapid\naccess and development across different types of models, datasets and tasks.\nOur library supports a collection of pretrained Code LLM models and popular\ncode benchmarks, including a standardized interface to train and serve code\nLLMs efficiently, and data features such as language-specific parsers and\nutility functions for extracting code attributes. In this paper, we describe\nthe design principles, the architecture, key modules and components, and\ncompare with other related library tools. Finally, we hope CodeTF is able to\nbridge the gap between machine learning/generative AI and software engineering,\nproviding a comprehensive open-source solution for developers, researchers, and\npractitioners.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=codetf++one+stop+transformer+library+for+state+of+the+art+code+llm&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Code intelligence plays a key role in transforming modern software\nengineering. Recently, deep learning-based models, especially Transformer-based\nlarge language models (LLMs), have demonstrated remarkable potential in\ntackling these tasks by leveraging massive open-source code data and\nprogramming language features. However, the development and deployment of such\nmodels often require expertise in both machine learning and software\nengineering, creating a barrier for the model adoption. In this paper, we\npresent CodeTF, an open-source Transformer-based library for state-of-the-art\nCode LLMs and code intelligence. Following the principles of modular design and\nextensible framework, we design CodeTF with a unified interface to enable rapid\naccess and development across different types of models, datasets and tasks.\nOur library supports a collection of pretrained Code LLM models and popular\ncode benchmarks, including a standardized interface to train and serve code\nLLMs efficiently, and data features such as language-specific parsers and\nutility functions for extracting code attributes. In this paper, we describe\nthe design principles, the architecture, key modules and components, and\ncompare with other related library tools. Finally, we hope CodeTF is able to\nbridge the gap between machine learning/generative AI and software engineering,\nproviding a comprehensive open-source solution for developers, researchers, and\npractitioners."}, "authors": [{"name": "Nghi D. Q. Bui"}, {"name": "Hung Le"}, {"name": "Yue Wang"}, {"name": "Junnan Li"}, {"name": "Akhilesh Deepak Gotmare"}, {"name": "Steven C. H. Hoi"}], "author_detail": {"name": "Steven C. H. Hoi"}, "author": "Steven C. H. Hoi", "arxiv_comment": "Ongoing work - Draft Preview", "links": [{"href": "http://arxiv.org/abs/2306.00029v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2306.00029v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SE", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SE", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}