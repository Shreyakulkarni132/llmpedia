{"id": "http://arxiv.org/abs/2210.03629v3", "guidislink": true, "link": "http://arxiv.org/abs/2210.03629v3", "updated": "2023-03-10T01:00:17Z", "updated_parsed": [2023, 3, 10, 1, 0, 17, 4, 69, 0], "published": "2022-10-06T01:00:32Z", "published_parsed": [2022, 10, 6, 1, 0, 32, 3, 279, 0], "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=react++synergizing+reasoning+and+acting+in+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "ReAct: Synergizing Reasoning and Acting in Language Models"}, "summary": "While large language models (LLMs) have demonstrated impressive capabilities\nacross tasks in language understanding and interactive decision making, their\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\naction plan generation) have primarily been studied as separate topics. In this\npaper, we explore the use of LLMs to generate both reasoning traces and\ntask-specific actions in an interleaved manner, allowing for greater synergy\nbetween the two: reasoning traces help the model induce, track, and update\naction plans as well as handle exceptions, while actions allow it to interface\nwith external sources, such as knowledge bases or environments, to gather\nadditional information. We apply our approach, named ReAct, to a diverse set of\nlanguage and decision making tasks and demonstrate its effectiveness over\nstate-of-the-art baselines, as well as improved human interpretability and\ntrustworthiness over methods without reasoning or acting components.\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\nReAct overcomes issues of hallucination and error propagation prevalent in\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\ngenerates human-like task-solving trajectories that are more interpretable than\nbaselines without reasoning traces. On two interactive decision making\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\nreinforcement learning methods by an absolute success rate of 34% and 10%\nrespectively, while being prompted with only one or two in-context examples.\nProject site with code: https://react-lm.github.io", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=react++synergizing+reasoning+and+acting+in+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "While large language models (LLMs) have demonstrated impressive capabilities\nacross tasks in language understanding and interactive decision making, their\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\naction plan generation) have primarily been studied as separate topics. In this\npaper, we explore the use of LLMs to generate both reasoning traces and\ntask-specific actions in an interleaved manner, allowing for greater synergy\nbetween the two: reasoning traces help the model induce, track, and update\naction plans as well as handle exceptions, while actions allow it to interface\nwith external sources, such as knowledge bases or environments, to gather\nadditional information. We apply our approach, named ReAct, to a diverse set of\nlanguage and decision making tasks and demonstrate its effectiveness over\nstate-of-the-art baselines, as well as improved human interpretability and\ntrustworthiness over methods without reasoning or acting components.\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\nReAct overcomes issues of hallucination and error propagation prevalent in\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\ngenerates human-like task-solving trajectories that are more interpretable than\nbaselines without reasoning traces. On two interactive decision making\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\nreinforcement learning methods by an absolute success rate of 34% and 10%\nrespectively, while being prompted with only one or two in-context examples.\nProject site with code: https://react-lm.github.io"}, "authors": [{"name": "Shunyu Yao"}, {"name": "Jeffrey Zhao"}, {"name": "Dian Yu"}, {"name": "Nan Du"}, {"name": "Izhak Shafran"}, {"name": "Karthik Narasimhan"}, {"name": "Yuan Cao"}], "author_detail": {"name": "Yuan Cao"}, "author": "Yuan Cao", "arxiv_comment": "v3 is the ICLR camera ready version with some typos fixed. Project\n  site with code: https://react-lm.github.io", "links": [{"href": "http://arxiv.org/abs/2210.03629v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2210.03629v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}