{"id": "http://arxiv.org/abs/2305.04091v3", "guidislink": true, "link": "http://arxiv.org/abs/2305.04091v3", "updated": "2023-05-26T07:06:48Z", "updated_parsed": [2023, 5, 26, 7, 6, 48, 4, 146, 0], "published": "2023-05-06T16:34:37Z", "published_parsed": [2023, 5, 6, 16, 34, 37, 5, 126, 0], "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning\n  by Large Language Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=plan+and+solve+prompting++improving+zero+shot+chain+of+thought+reasoning+by+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning\n  by Large Language Models"}, "summary": "Large language models (LLMs) have recently been shown to deliver impressive\nperformance in various NLP tasks. To tackle multi-step reasoning tasks,\nfew-shot chain-of-thought (CoT) prompting includes a few manually crafted\nstep-by-step reasoning demonstrations which enable LLMs to explicitly generate\nreasoning steps and improve their reasoning task accuracy. To eliminate the\nmanual effort, Zero-shot-CoT concatenates the target problem statement with\n\"Let's think step by step\" as an input prompt to LLMs. Despite the success of\nZero-shot-CoT, it still suffers from three pitfalls: calculation errors,\nmissing-step errors, and semantic misunderstanding errors. To address the\nmissing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of\ntwo components: first, devising a plan to divide the entire task into smaller\nsubtasks, and then carrying out the subtasks according to the plan. To address\nthe calculation errors and improve the quality of generated reasoning steps, we\nextend PS prompting with more detailed instructions and derive PS+ prompting.\nWe evaluate our proposed prompting strategy on ten datasets across three\nreasoning problems. The experimental results over GPT-3 show that our proposed\nzero-shot prompting consistently outperforms Zero-shot-CoT across all datasets\nby a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought\nPrompting, and has comparable performance with 8-shot CoT prompting on the math\nreasoning problem. The code can be found at\nhttps://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=plan+and+solve+prompting++improving+zero+shot+chain+of+thought+reasoning+by+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Large language models (LLMs) have recently been shown to deliver impressive\nperformance in various NLP tasks. To tackle multi-step reasoning tasks,\nfew-shot chain-of-thought (CoT) prompting includes a few manually crafted\nstep-by-step reasoning demonstrations which enable LLMs to explicitly generate\nreasoning steps and improve their reasoning task accuracy. To eliminate the\nmanual effort, Zero-shot-CoT concatenates the target problem statement with\n\"Let's think step by step\" as an input prompt to LLMs. Despite the success of\nZero-shot-CoT, it still suffers from three pitfalls: calculation errors,\nmissing-step errors, and semantic misunderstanding errors. To address the\nmissing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of\ntwo components: first, devising a plan to divide the entire task into smaller\nsubtasks, and then carrying out the subtasks according to the plan. To address\nthe calculation errors and improve the quality of generated reasoning steps, we\nextend PS prompting with more detailed instructions and derive PS+ prompting.\nWe evaluate our proposed prompting strategy on ten datasets across three\nreasoning problems. The experimental results over GPT-3 show that our proposed\nzero-shot prompting consistently outperforms Zero-shot-CoT across all datasets\nby a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought\nPrompting, and has comparable performance with 8-shot CoT prompting on the math\nreasoning problem. The code can be found at\nhttps://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting."}, "authors": [{"name": "Lei Wang"}, {"name": "Wanyu Xu"}, {"name": "Yihuai Lan"}, {"name": "Zhiqiang Hu"}, {"name": "Yunshi Lan"}, {"name": "Roy Ka-Wei Lee"}, {"name": "Ee-Peng Lim"}], "author_detail": {"name": "Ee-Peng Lim"}, "author": "Ee-Peng Lim", "arxiv_comment": "ACL 2023", "links": [{"href": "http://arxiv.org/abs/2305.04091v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2305.04091v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}