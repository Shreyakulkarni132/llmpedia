{"id": "http://arxiv.org/abs/2205.10775v1", "guidislink": true, "link": "http://arxiv.org/abs/2205.10775v1", "updated": "2022-05-22T08:44:26Z", "updated_parsed": [2022, 5, 22, 8, 44, 26, 6, 142, 0], "published": "2022-05-22T08:44:26Z", "published_parsed": [2022, 5, 22, 8, 44, 26, 6, 142, 0], "title": "Ada-Ranker: A Data Distribution Adaptive Ranking Paradigm for Sequential\n  Recommendation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=ada+ranker++a+data+distribution+adaptive+ranking+paradigm+for+sequential+recommendation&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Ada-Ranker: A Data Distribution Adaptive Ranking Paradigm for Sequential\n  Recommendation"}, "summary": "A large-scale recommender system usually consists of recall and ranking\nmodules. The goal of ranking modules (aka rankers) is to elaborately\ndiscriminate users' preference on item candidates proposed by recall modules.\nWith the success of deep learning techniques in various domains, we have\nwitnessed the mainstream rankers evolve from traditional models to deep neural\nmodels. However, the way that we design and use rankers remains unchanged:\noffline training the model, freezing the parameters, and deploying it for\nonline serving. Actually, the candidate items are determined by specific user\nrequests, in which underlying distributions (e.g., the proportion of items for\ndifferent categories, the proportion of popular or new items) are highly\ndifferent from one another in a production environment. The classical\nparameter-frozen inference manner cannot adapt to dynamic serving\ncircumstances, making rankers' performance compromised. In this paper, we\npropose a new training and inference paradigm, termed as Ada-Ranker, to address\nthe challenges of dynamic online serving. Instead of using parameter-frozen\nmodels for universal serving, Ada-Ranker can adaptively modulate parameters of\na ranker according to the data distribution of the current group of item\ncandidates. We first extract distribution patterns from the item candidates.\nThen, we modulate the ranker by the patterns to make the ranker adapt to the\ncurrent data distribution. Finally, we use the revised ranker to score the\ncandidate list. In this way, we empower the ranker with the capacity of\nadapting from a global model to a local model which better handles the current\ntask.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=ada+ranker++a+data+distribution+adaptive+ranking+paradigm+for+sequential+recommendation&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "A large-scale recommender system usually consists of recall and ranking\nmodules. The goal of ranking modules (aka rankers) is to elaborately\ndiscriminate users' preference on item candidates proposed by recall modules.\nWith the success of deep learning techniques in various domains, we have\nwitnessed the mainstream rankers evolve from traditional models to deep neural\nmodels. However, the way that we design and use rankers remains unchanged:\noffline training the model, freezing the parameters, and deploying it for\nonline serving. Actually, the candidate items are determined by specific user\nrequests, in which underlying distributions (e.g., the proportion of items for\ndifferent categories, the proportion of popular or new items) are highly\ndifferent from one another in a production environment. The classical\nparameter-frozen inference manner cannot adapt to dynamic serving\ncircumstances, making rankers' performance compromised. In this paper, we\npropose a new training and inference paradigm, termed as Ada-Ranker, to address\nthe challenges of dynamic online serving. Instead of using parameter-frozen\nmodels for universal serving, Ada-Ranker can adaptively modulate parameters of\na ranker according to the data distribution of the current group of item\ncandidates. We first extract distribution patterns from the item candidates.\nThen, we modulate the ranker by the patterns to make the ranker adapt to the\ncurrent data distribution. Finally, we use the revised ranker to score the\ncandidate list. In this way, we empower the ranker with the capacity of\nadapting from a global model to a local model which better handles the current\ntask."}, "authors": [{"name": "Xinyan Fan"}, {"name": "Jianxun Lian"}, {"name": "Wayne Xin Zhao"}, {"name": "Zheng Liu"}, {"name": "Chaozhuo Li"}, {"name": "Xing Xie"}], "author_detail": {"name": "Xing Xie"}, "author": "Xing Xie", "arxiv_doi": "10.1145/3477495.3531931", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1145/3477495.3531931", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/2205.10775v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2205.10775v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "12 pages", "arxiv_primary_category": {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}