{"id": "http://arxiv.org/abs/2205.08012v2", "guidislink": true, "link": "http://arxiv.org/abs/2205.08012v2", "updated": "2022-09-23T16:45:34Z", "updated_parsed": [2022, 9, 23, 16, 45, 34, 4, 266, 0], "published": "2022-05-16T22:55:45Z", "published_parsed": [2022, 5, 16, 22, 55, 45, 0, 136, 0], "title": "CascadER: Cross-Modal Cascading for Knowledge Graph Link Prediction", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=cascader++cross+modal+cascading+for+knowledge+graph+link+prediction&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "CascadER: Cross-Modal Cascading for Knowledge Graph Link Prediction"}, "summary": "Knowledge graph (KG) link prediction is a fundamental task in artificial\nintelligence, with applications in natural language processing, information\nretrieval, and biomedicine. Recently, promising results have been achieved by\nleveraging cross-modal information in KGs, using ensembles that combine\nknowledge graph embeddings (KGEs) and contextual language models (LMs).\nHowever, existing ensembles are either (1) not consistently effective in terms\nof ranking accuracy gains or (2) impractically inefficient on larger datasets\ndue to the combinatorial explosion problem of pairwise ranking with deep\nlanguage models. In this paper, we propose a novel tiered ranking architecture\nCascadER to maintain the ranking accuracy of full ensembling while improving\nefficiency considerably. CascadER uses LMs to rerank the outputs of more\nefficient base KGEs, relying on an adaptive subset selection scheme aimed at\ninvoking the LMs minimally while maximizing accuracy gain over the KGE.\nExtensive experiments demonstrate that CascadER improves MRR by up to 9 points\nover KGE baselines, setting new state-of-the-art performance on four benchmarks\nwhile improving efficiency by one or more orders of magnitude over competitive\ncross-modal baselines. Our empirical analyses reveal that diversity of models\nacross modalities and preservation of individual models' confidence signals\nhelp explain the effectiveness of CascadER, and suggest promising directions\nfor cross-modal cascaded architectures. Code and pretrained models are\navailable at https://github.com/tsafavi/cascader.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=cascader++cross+modal+cascading+for+knowledge+graph+link+prediction&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Knowledge graph (KG) link prediction is a fundamental task in artificial\nintelligence, with applications in natural language processing, information\nretrieval, and biomedicine. Recently, promising results have been achieved by\nleveraging cross-modal information in KGs, using ensembles that combine\nknowledge graph embeddings (KGEs) and contextual language models (LMs).\nHowever, existing ensembles are either (1) not consistently effective in terms\nof ranking accuracy gains or (2) impractically inefficient on larger datasets\ndue to the combinatorial explosion problem of pairwise ranking with deep\nlanguage models. In this paper, we propose a novel tiered ranking architecture\nCascadER to maintain the ranking accuracy of full ensembling while improving\nefficiency considerably. CascadER uses LMs to rerank the outputs of more\nefficient base KGEs, relying on an adaptive subset selection scheme aimed at\ninvoking the LMs minimally while maximizing accuracy gain over the KGE.\nExtensive experiments demonstrate that CascadER improves MRR by up to 9 points\nover KGE baselines, setting new state-of-the-art performance on four benchmarks\nwhile improving efficiency by one or more orders of magnitude over competitive\ncross-modal baselines. Our empirical analyses reveal that diversity of models\nacross modalities and preservation of individual models' confidence signals\nhelp explain the effectiveness of CascadER, and suggest promising directions\nfor cross-modal cascaded architectures. Code and pretrained models are\navailable at https://github.com/tsafavi/cascader."}, "authors": [{"name": "Tara Safavi"}, {"name": "Doug Downey"}, {"name": "Tom Hope"}], "author_detail": {"name": "Tom Hope"}, "author": "Tom Hope", "arxiv_comment": "AKBC 2022", "links": [{"href": "http://arxiv.org/abs/2205.08012v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2205.08012v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}