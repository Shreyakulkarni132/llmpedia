{"id": "http://arxiv.org/abs/2306.17563v1", "guidislink": true, "link": "http://arxiv.org/abs/2306.17563v1", "updated": "2023-06-30T11:32:25Z", "updated_parsed": [2023, 6, 30, 11, 32, 25, 4, 181, 0], "published": "2023-06-30T11:32:25Z", "published_parsed": [2023, 6, 30, 11, 32, 25, 4, 181, 0], "title": "Large Language Models are Effective Text Rankers with Pairwise Ranking\n  Prompting", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=large+language+models+are+effective+text+rankers+with+pairwise+ranking+prompting&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Large Language Models are Effective Text Rankers with Pairwise Ranking\n  Prompting"}, "summary": "Ranking documents using Large Language Models (LLMs) by directly feeding the\nquery and candidate documents into the prompt is an interesting and practical\nproblem. However, there has been limited success so far, as researchers have\nfound it difficult to outperform fine-tuned baseline rankers on benchmark\ndatasets. We analyze pointwise and listwise ranking prompts used by existing\nmethods and argue that off-the-shelf LLMs do not fully understand these ranking\nformulations, possibly due to the nature of how LLMs are trained. In this\npaper, we propose to significantly reduce the burden on LLMs by using a new\ntechnique called Pairwise Ranking Prompting (PRP). Our results are the first in\nthe literature to achieve state-of-the-art ranking performance on standard\nbenchmarks using moderate-sized open-sourced LLMs. On TREC-DL2020, PRP based on\nthe Flan-UL2 model with 20B parameters outperforms the previous best approach\nin the literature, which is based on the blackbox commercial GPT-4 that has 50x\n(estimated) model size, by over 5% at NDCG@1. On TREC-DL2019, PRP is only\ninferior to the GPT-4 solution on the NDCG@5 and NDCG@10 metrics, while\noutperforming other existing solutions, such as InstructGPT which has 175B\nparameters, by over 10% for nearly all ranking metrics. Furthermore, we propose\nseveral variants of PRP to improve efficiency and show that it is possible to\nachieve competitive results even with linear complexity. We also discuss other\nbenefits of PRP, such as supporting both generation and scoring LLM APIs, as\nwell as being insensitive to input ordering.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=large+language+models+are+effective+text+rankers+with+pairwise+ranking+prompting&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Ranking documents using Large Language Models (LLMs) by directly feeding the\nquery and candidate documents into the prompt is an interesting and practical\nproblem. However, there has been limited success so far, as researchers have\nfound it difficult to outperform fine-tuned baseline rankers on benchmark\ndatasets. We analyze pointwise and listwise ranking prompts used by existing\nmethods and argue that off-the-shelf LLMs do not fully understand these ranking\nformulations, possibly due to the nature of how LLMs are trained. In this\npaper, we propose to significantly reduce the burden on LLMs by using a new\ntechnique called Pairwise Ranking Prompting (PRP). Our results are the first in\nthe literature to achieve state-of-the-art ranking performance on standard\nbenchmarks using moderate-sized open-sourced LLMs. On TREC-DL2020, PRP based on\nthe Flan-UL2 model with 20B parameters outperforms the previous best approach\nin the literature, which is based on the blackbox commercial GPT-4 that has 50x\n(estimated) model size, by over 5% at NDCG@1. On TREC-DL2019, PRP is only\ninferior to the GPT-4 solution on the NDCG@5 and NDCG@10 metrics, while\noutperforming other existing solutions, such as InstructGPT which has 175B\nparameters, by over 10% for nearly all ranking metrics. Furthermore, we propose\nseveral variants of PRP to improve efficiency and show that it is possible to\nachieve competitive results even with linear complexity. We also discuss other\nbenefits of PRP, such as supporting both generation and scoring LLM APIs, as\nwell as being insensitive to input ordering."}, "authors": [{"name": "Zhen Qin"}, {"name": "Rolf Jagerman"}, {"name": "Kai Hui"}, {"name": "Honglei Zhuang"}, {"name": "Junru Wu"}, {"name": "Jiaming Shen"}, {"name": "Tianqi Liu"}, {"name": "Jialu Liu"}, {"name": "Donald Metzler"}, {"name": "Xuanhui Wang"}, {"name": "Michael Bendersky"}], "author_detail": {"name": "Michael Bendersky"}, "author": "Michael Bendersky", "arxiv_comment": "12 pages, 3 figures", "links": [{"href": "http://arxiv.org/abs/2306.17563v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2306.17563v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}