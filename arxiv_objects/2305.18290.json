{"id": "http://arxiv.org/abs/2305.18290v1", "guidislink": true, "link": "http://arxiv.org/abs/2305.18290v1", "updated": "2023-05-29T17:57:46Z", "updated_parsed": [2023, 5, 29, 17, 57, 46, 0, 149, 0], "published": "2023-05-29T17:57:46Z", "published_parsed": [2023, 5, 29, 17, 57, 46, 0, 149, 0], "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward\n  Model", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=direct+preference+optimization++your+language+model+is+secretly+a+reward+model&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Direct Preference Optimization: Your Language Model is Secretly a Reward\n  Model"}, "summary": "While large-scale unsupervised language models (LMs) learn broad world\nknowledge and some reasoning skills, achieving precise control of their\nbehavior is difficult due to the completely unsupervised nature of their\ntraining. Existing methods for gaining such steerability collect human labels\nof the relative quality of model generations and fine-tune the unsupervised LM\nto align with these preferences, often with reinforcement learning from human\nfeedback (RLHF). However, RLHF is a complex and often unstable procedure, first\nfitting a reward model that reflects the human preferences, and then\nfine-tuning the large unsupervised LM using reinforcement learning to maximize\nthis estimated reward without drifting too far from the original model. In this\npaper, we leverage a mapping between reward functions and optimal policies to\nshow that this constrained reward maximization problem can be optimized exactly\nwith a single stage of policy training, essentially solving a classification\nproblem on the human preference data. The resulting algorithm, which we call\nDirect Preference Optimization (DPO), is stable, performant and computationally\nlightweight, eliminating the need for fitting a reward model, sampling from the\nLM during fine-tuning, or performing significant hyperparameter tuning. Our\nexperiments show that DPO can fine-tune LMs to align with human preferences as\nwell as or better than existing methods. Notably, fine-tuning with DPO exceeds\nRLHF's ability to control sentiment of generations and improves response\nquality in summarization and single-turn dialogue while being substantially\nsimpler to implement and train.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=direct+preference+optimization++your+language+model+is+secretly+a+reward+model&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "While large-scale unsupervised language models (LMs) learn broad world\nknowledge and some reasoning skills, achieving precise control of their\nbehavior is difficult due to the completely unsupervised nature of their\ntraining. Existing methods for gaining such steerability collect human labels\nof the relative quality of model generations and fine-tune the unsupervised LM\nto align with these preferences, often with reinforcement learning from human\nfeedback (RLHF). However, RLHF is a complex and often unstable procedure, first\nfitting a reward model that reflects the human preferences, and then\nfine-tuning the large unsupervised LM using reinforcement learning to maximize\nthis estimated reward without drifting too far from the original model. In this\npaper, we leverage a mapping between reward functions and optimal policies to\nshow that this constrained reward maximization problem can be optimized exactly\nwith a single stage of policy training, essentially solving a classification\nproblem on the human preference data. The resulting algorithm, which we call\nDirect Preference Optimization (DPO), is stable, performant and computationally\nlightweight, eliminating the need for fitting a reward model, sampling from the\nLM during fine-tuning, or performing significant hyperparameter tuning. Our\nexperiments show that DPO can fine-tune LMs to align with human preferences as\nwell as or better than existing methods. Notably, fine-tuning with DPO exceeds\nRLHF's ability to control sentiment of generations and improves response\nquality in summarization and single-turn dialogue while being substantially\nsimpler to implement and train."}, "authors": [{"name": "Rafael Rafailov"}, {"name": "Archit Sharma"}, {"name": "Eric Mitchell"}, {"name": "Stefano Ermon"}, {"name": "Christopher D. Manning"}, {"name": "Chelsea Finn"}], "author_detail": {"name": "Chelsea Finn"}, "author": "Chelsea Finn", "links": [{"href": "http://arxiv.org/abs/2305.18290v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2305.18290v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}