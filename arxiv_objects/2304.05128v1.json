{"id": "http://arxiv.org/abs/2304.05128v1", "guidislink": true, "link": "http://arxiv.org/abs/2304.05128v1", "updated": "2023-04-11T10:43:43Z", "updated_parsed": [2023, 4, 11, 10, 43, 43, 1, 101, 0], "published": "2023-04-11T10:43:43Z", "published_parsed": [2023, 4, 11, 10, 43, 43, 1, 101, 0], "title": "Teaching Large Language Models to Self-Debug", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=teaching+large+language+models+to+self+debug&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Teaching Large Language Models to Self-Debug"}, "summary": "Large language models (LLMs) have achieved impressive performance on code\ngeneration. However, for complex programming tasks, generating the correct\nsolution in one go becomes challenging, thus some prior works have designed\nprogram repair approaches to improve code generation performance. In this work,\nwe propose Self-Debugging, which teaches a large language model to debug its\npredicted program via few-shot demonstrations. In particular, we demonstrate\nthat Self-Debugging can teach the large language model to perform rubber duck\ndebugging; i.e., without any feedback on the code correctness or error\nmessages, the model is able to identify its mistakes by explaining the\ngenerated code in natural language. Self-Debugging achieves the\nstate-of-the-art performance on several code generation benchmarks, including\nthe Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python\ntranslation, and MBPP for text-to-Python generation. On the Spider benchmark\nwhere there are no unit tests to verify the correctness of predictions,\nSelf-Debugging with code explanation consistently improves the baseline by\n2-3%, and improves the prediction accuracy on problems of the hardest label by\n9%. On TransCoder and MBPP where unit tests are available, Self-Debugging\nimproves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback\nmessages and reusing failed predictions, Self-Debugging notably improves sample\nefficiency, and can match or outperform baseline models that generate more than\n10x candidate programs.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=teaching+large+language+models+to+self+debug&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Large language models (LLMs) have achieved impressive performance on code\ngeneration. However, for complex programming tasks, generating the correct\nsolution in one go becomes challenging, thus some prior works have designed\nprogram repair approaches to improve code generation performance. In this work,\nwe propose Self-Debugging, which teaches a large language model to debug its\npredicted program via few-shot demonstrations. In particular, we demonstrate\nthat Self-Debugging can teach the large language model to perform rubber duck\ndebugging; i.e., without any feedback on the code correctness or error\nmessages, the model is able to identify its mistakes by explaining the\ngenerated code in natural language. Self-Debugging achieves the\nstate-of-the-art performance on several code generation benchmarks, including\nthe Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python\ntranslation, and MBPP for text-to-Python generation. On the Spider benchmark\nwhere there are no unit tests to verify the correctness of predictions,\nSelf-Debugging with code explanation consistently improves the baseline by\n2-3%, and improves the prediction accuracy on problems of the hardest label by\n9%. On TransCoder and MBPP where unit tests are available, Self-Debugging\nimproves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback\nmessages and reusing failed predictions, Self-Debugging notably improves sample\nefficiency, and can match or outperform baseline models that generate more than\n10x candidate programs."}, "authors": [{"name": "Xinyun Chen"}, {"name": "Maxwell Lin"}, {"name": "Nathanael Sch\u00e4rli"}, {"name": "Denny Zhou"}], "author_detail": {"name": "Denny Zhou"}, "author": "Denny Zhou", "links": [{"href": "http://arxiv.org/abs/2304.05128v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2304.05128v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}