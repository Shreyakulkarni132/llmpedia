{"id": "http://arxiv.org/abs/2302.00618v1", "guidislink": true, "link": "http://arxiv.org/abs/2302.00618v1", "updated": "2023-02-01T17:33:12Z", "updated_parsed": [2023, 2, 1, 17, 33, 12, 2, 32, 0], "published": "2023-02-01T17:33:12Z", "published_parsed": [2023, 2, 1, 17, 33, 12, 2, 32, 0], "title": "Synthetic Prompting: Generating Chain-of-Thought Demonstrations for\n  Large Language Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=synthetic+prompting++generating+chain+of+thought+demonstrations+for+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Synthetic Prompting: Generating Chain-of-Thought Demonstrations for\n  Large Language Models"}, "summary": "Large language models can perform various reasoning tasks by using\nchain-of-thought prompting, which guides them to find answers through\nstep-by-step demonstrations. However, the quality of the prompts depends on the\ndemonstrations given to the models, and creating many of them by hand is\ncostly. We introduce Synthetic prompting, a method that leverages a few\nhandcrafted examples to prompt the model to generate more examples by itself,\nand selects effective demonstrations to elicit better reasoning. Our method\nalternates between a backward and forward process to generate new examples. The\nbackward process generates a question that match a sampled reasoning chain, so\nthat the question is solvable and clear. The forward process produces a more\ndetailed reasoning chain for the question, improving the quality of the\nexample. We evaluate our method on numerical, symbolic, and algorithmic\nreasoning tasks, and show that it outperforms existing prompting techniques.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=synthetic+prompting++generating+chain+of+thought+demonstrations+for+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Large language models can perform various reasoning tasks by using\nchain-of-thought prompting, which guides them to find answers through\nstep-by-step demonstrations. However, the quality of the prompts depends on the\ndemonstrations given to the models, and creating many of them by hand is\ncostly. We introduce Synthetic prompting, a method that leverages a few\nhandcrafted examples to prompt the model to generate more examples by itself,\nand selects effective demonstrations to elicit better reasoning. Our method\nalternates between a backward and forward process to generate new examples. The\nbackward process generates a question that match a sampled reasoning chain, so\nthat the question is solvable and clear. The forward process produces a more\ndetailed reasoning chain for the question, improving the quality of the\nexample. We evaluate our method on numerical, symbolic, and algorithmic\nreasoning tasks, and show that it outperforms existing prompting techniques."}, "authors": [{"name": "Zhihong Shao"}, {"name": "Yeyun Gong"}, {"name": "Yelong Shen"}, {"name": "Minlie Huang"}, {"name": "Nan Duan"}, {"name": "Weizhu Chen"}], "author_detail": {"name": "Weizhu Chen"}, "author": "Weizhu Chen", "arxiv_comment": "Preprint", "links": [{"href": "http://arxiv.org/abs/2302.00618v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2302.00618v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}