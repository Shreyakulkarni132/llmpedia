{"id": "http://arxiv.org/abs/2306.17842v2", "guidislink": true, "link": "http://arxiv.org/abs/2306.17842v2", "updated": "2023-07-03T08:13:19Z", "updated_parsed": [2023, 7, 3, 8, 13, 19, 0, 184, 0], "published": "2023-06-30T17:59:07Z", "published_parsed": [2023, 6, 30, 17, 59, 7, 4, 181, 0], "title": "SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen\n  LLMs", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=spae++semantic+pyramid+autoencoder+for+multimodal+generation+with+frozen+llms&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen\n  LLMs"}, "summary": "In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling\nfrozen LLMs to perform both understanding and generation tasks involving\nnon-linguistic modalities such as images or videos. SPAE converts between raw\npixels and interpretable lexical tokens (or words) extracted from the LLM's\nvocabulary. The resulting tokens capture both the semantic meaning and the\nfine-grained details needed for visual reconstruction, effectively translating\nthe visual content into a language comprehensible to the LLM, and empowering it\nto perform a wide array of multimodal tasks. Our approach is validated through\nin-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set\nof image understanding and generation tasks. Our method marks the first\nsuccessful attempt to enable a frozen LLM to generate image content while\nsurpassing state-of-the-art performance in image understanding tasks, under the\nsame setting, by over 25%.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=spae++semantic+pyramid+autoencoder+for+multimodal+generation+with+frozen+llms&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling\nfrozen LLMs to perform both understanding and generation tasks involving\nnon-linguistic modalities such as images or videos. SPAE converts between raw\npixels and interpretable lexical tokens (or words) extracted from the LLM's\nvocabulary. The resulting tokens capture both the semantic meaning and the\nfine-grained details needed for visual reconstruction, effectively translating\nthe visual content into a language comprehensible to the LLM, and empowering it\nto perform a wide array of multimodal tasks. Our approach is validated through\nin-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set\nof image understanding and generation tasks. Our method marks the first\nsuccessful attempt to enable a frozen LLM to generate image content while\nsurpassing state-of-the-art performance in image understanding tasks, under the\nsame setting, by over 25%."}, "authors": [{"name": "Lijun Yu"}, {"name": "Yong Cheng"}, {"name": "Zhiruo Wang"}, {"name": "Vivek Kumar"}, {"name": "Wolfgang Macherey"}, {"name": "Yanping Huang"}, {"name": "David A. Ross"}, {"name": "Irfan Essa"}, {"name": "Yonatan Bisk"}, {"name": "Ming-Hsuan Yang"}, {"name": "Kevin Murphy"}, {"name": "Alexander G. Hauptmann"}, {"name": "Lu Jiang"}], "author_detail": {"name": "Lu Jiang"}, "author": "Lu Jiang", "links": [{"href": "http://arxiv.org/abs/2306.17842v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2306.17842v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.MM", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}