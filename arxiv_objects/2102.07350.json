{"id": "http://arxiv.org/abs/2102.07350v1", "guidislink": true, "link": "http://arxiv.org/abs/2102.07350v1", "updated": "2021-02-15T05:27:55Z", "updated_parsed": [2021, 2, 15, 5, 27, 55, 0, 46, 0], "published": "2021-02-15T05:27:55Z", "published_parsed": [2021, 2, 15, 5, 27, 55, 0, 46, 0], "title": "Prompt Programming for Large Language Models: Beyond the Few-Shot\n  Paradigm", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=prompt+programming+for+large+language+models++beyond+the+few+shot+paradigm&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Prompt Programming for Large Language Models: Beyond the Few-Shot\n  Paradigm"}, "summary": "Prevailing methods for mapping large generative language models to supervised\ntasks may fail to sufficiently probe models' novel capabilities. Using GPT-3 as\na case study, we show that 0-shot prompts can significantly outperform few-shot\nprompts. We suggest that the function of few-shot examples in these cases is\nbetter described as locating an already learned task rather than meta-learning.\nThis analysis motivates rethinking the role of prompts in controlling and\nevaluating powerful language models. In this work, we discuss methods of prompt\nprogramming, emphasizing the usefulness of considering prompts through the lens\nof natural language. We explore techniques for exploiting the capacity of\nnarratives and cultural anchors to encode nuanced intentions and techniques for\nencouraging deconstruction of a problem into components before producing a\nverdict. Informed by this more encompassing theory of prompt programming, we\nalso introduce the idea of a metaprompt that seeds the model to generate its\nown natural language prompts for a range of tasks. Finally, we discuss how\nthese more general methods of interacting with language models can be\nincorporated into existing and future benchmarks and practical applications.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=prompt+programming+for+large+language+models++beyond+the+few+shot+paradigm&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Prevailing methods for mapping large generative language models to supervised\ntasks may fail to sufficiently probe models' novel capabilities. Using GPT-3 as\na case study, we show that 0-shot prompts can significantly outperform few-shot\nprompts. We suggest that the function of few-shot examples in these cases is\nbetter described as locating an already learned task rather than meta-learning.\nThis analysis motivates rethinking the role of prompts in controlling and\nevaluating powerful language models. In this work, we discuss methods of prompt\nprogramming, emphasizing the usefulness of considering prompts through the lens\nof natural language. We explore techniques for exploiting the capacity of\nnarratives and cultural anchors to encode nuanced intentions and techniques for\nencouraging deconstruction of a problem into components before producing a\nverdict. Informed by this more encompassing theory of prompt programming, we\nalso introduce the idea of a metaprompt that seeds the model to generate its\nown natural language prompts for a range of tasks. Finally, we discuss how\nthese more general methods of interacting with language models can be\nincorporated into existing and future benchmarks and practical applications."}, "authors": [{"name": "Laria Reynolds"}, {"name": "Kyle McDonell"}], "author_detail": {"name": "Kyle McDonell"}, "author": "Kyle McDonell", "links": [{"href": "http://arxiv.org/abs/2102.07350v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2102.07350v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}