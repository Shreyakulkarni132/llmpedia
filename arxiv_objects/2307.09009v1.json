{"id": "http://arxiv.org/abs/2307.09009v1", "guidislink": true, "link": "http://arxiv.org/abs/2307.09009v1", "updated": "2023-07-18T06:56:08Z", "updated_parsed": [2023, 7, 18, 6, 56, 8, 1, 199, 0], "published": "2023-07-18T06:56:08Z", "published_parsed": [2023, 7, 18, 6, 56, 8, 1, 199, 0], "title": "How is ChatGPT's behavior changing over time?", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=how+is+chatgpt+s+behavior+changing+over+time+&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "How is ChatGPT's behavior changing over time?"}, "summary": "GPT-3.5 and GPT-4 are the two most widely used large language model (LLM)\nservices. However, when and how these models are updated over time is opaque.\nHere, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on\nfour diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous\nquestions, 3) generating code and 4) visual reasoning. We find that the\nperformance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time.\nFor example, GPT-4 (March 2023) was very good at identifying prime numbers\n(accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions\n(accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5\n(March 2023) in this task. GPT-4 was less willing to answer sensitive questions\nin June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes\nin code generation in June than in March. Overall, our findings shows that the\nbehavior of the same LLM service can change substantially in a relatively short\namount of time, highlighting the need for continuous monitoring of LLM quality.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=how+is+chatgpt+s+behavior+changing+over+time+&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "GPT-3.5 and GPT-4 are the two most widely used large language model (LLM)\nservices. However, when and how these models are updated over time is opaque.\nHere, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on\nfour diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous\nquestions, 3) generating code and 4) visual reasoning. We find that the\nperformance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time.\nFor example, GPT-4 (March 2023) was very good at identifying prime numbers\n(accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions\n(accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5\n(March 2023) in this task. GPT-4 was less willing to answer sensitive questions\nin June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes\nin code generation in June than in March. Overall, our findings shows that the\nbehavior of the same LLM service can change substantially in a relatively short\namount of time, highlighting the need for continuous monitoring of LLM quality."}, "authors": [{"name": "Lingjiao Chen"}, {"name": "Matei Zaharia"}, {"name": "James Zou"}], "author_detail": {"name": "James Zou"}, "author": "James Zou", "links": [{"href": "http://arxiv.org/abs/2307.09009v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.09009v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}