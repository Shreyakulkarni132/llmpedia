{"id": "http://arxiv.org/abs/2212.11214v1", "guidislink": true, "link": "http://arxiv.org/abs/2212.11214v1", "updated": "2022-12-21T17:41:16Z", "updated_parsed": [2022, 12, 21, 17, 41, 16, 2, 355, 0], "published": "2022-12-21T17:41:16Z", "published_parsed": [2022, 12, 21, 17, 41, 16, 2, 355, 0], "title": "Crowd Score: A Method for the Evaluation of Jokes using Large Language\n  Model AI Voters as Judges", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=crowd+score++a+method+for+the+evaluation+of+jokes+using+large+language+model+ai+voters+as+judges&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Crowd Score: A Method for the Evaluation of Jokes using Large Language\n  Model AI Voters as Judges"}, "summary": "This paper presents the Crowd Score, a novel method to assess the funniness\nof jokes using large language models (LLMs) as AI judges. Our method relies on\ninducing different personalities into the LLM and aggregating the votes of the\nAI judges into a single score to rate jokes. We validate the votes using an\nauditing technique that checks if the explanation for a particular vote is\nreasonable using the LLM. We tested our methodology on 52 jokes in a crowd of\nfour AI voters with different humour types: affiliative, self-enhancing,\naggressive and self-defeating. Our results show that few-shot prompting leads\nto better results than zero-shot for the voting question. Personality induction\nshowed that aggressive and self-defeating voters are significantly more\ninclined to find more jokes funny of a set of aggressive/self-defeating jokes\nthan the affiliative and self-enhancing voters. The Crowd Score follows the\nsame trend as human judges by assigning higher scores to jokes that are also\nconsidered funnier by human judges. We believe that our methodology could be\napplied to other creative domains such as story, poetry, slogans, etc. It could\nboth help the adoption of a flexible and accurate standard approach to compare\ndifferent work in the CC community under a common metric and by minimizing\nhuman participation in assessing creative artefacts, it could accelerate the\nprototyping of creative artefacts and reduce the cost of hiring human\nparticipants to rate creative artefacts.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=crowd+score++a+method+for+the+evaluation+of+jokes+using+large+language+model+ai+voters+as+judges&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "This paper presents the Crowd Score, a novel method to assess the funniness\nof jokes using large language models (LLMs) as AI judges. Our method relies on\ninducing different personalities into the LLM and aggregating the votes of the\nAI judges into a single score to rate jokes. We validate the votes using an\nauditing technique that checks if the explanation for a particular vote is\nreasonable using the LLM. We tested our methodology on 52 jokes in a crowd of\nfour AI voters with different humour types: affiliative, self-enhancing,\naggressive and self-defeating. Our results show that few-shot prompting leads\nto better results than zero-shot for the voting question. Personality induction\nshowed that aggressive and self-defeating voters are significantly more\ninclined to find more jokes funny of a set of aggressive/self-defeating jokes\nthan the affiliative and self-enhancing voters. The Crowd Score follows the\nsame trend as human judges by assigning higher scores to jokes that are also\nconsidered funnier by human judges. We believe that our methodology could be\napplied to other creative domains such as story, poetry, slogans, etc. It could\nboth help the adoption of a flexible and accurate standard approach to compare\ndifferent work in the CC community under a common metric and by minimizing\nhuman participation in assessing creative artefacts, it could accelerate the\nprototyping of creative artefacts and reduce the cost of hiring human\nparticipants to rate creative artefacts."}, "authors": [{"name": "Fabricio Goes"}, {"name": "Zisen Zhou"}, {"name": "Piotr Sawicki"}, {"name": "Marek Grzes"}, {"name": "Daniel G. Brown"}], "author_detail": {"name": "Daniel G. Brown"}, "author": "Daniel G. Brown", "arxiv_comment": "11 pages, 3 figures", "links": [{"href": "http://arxiv.org/abs/2212.11214v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2212.11214v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}