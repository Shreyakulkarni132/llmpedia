{"id": "http://arxiv.org/abs/2212.08286v2", "guidislink": true, "link": "http://arxiv.org/abs/2212.08286v2", "updated": "2023-07-07T17:43:12Z", "updated_parsed": [2023, 7, 7, 17, 43, 12, 4, 188, 0], "published": "2022-12-16T05:15:41Z", "published_parsed": [2022, 12, 16, 5, 15, 41, 4, 350, 0], "title": "ALERT: Adapting Language Models to Reasoning Tasks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=alert++adapting+language+models+to+reasoning+tasks&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "ALERT: Adapting Language Models to Reasoning Tasks"}, "summary": "Current large language models can perform reasonably well on complex tasks\nthat require step-by-step reasoning with few-shot learning. Are these models\napplying reasoning skills they have learnt during pre-training and reason\noutside of their training context, or are they simply memorizing their training\ncorpus at finer granularity and have learnt to better understand their context?\nTo tease apart these possibilities, we introduce ALERT, a benchmark and suite\nof analyses for assessing language models' reasoning ability comparing\npre-trained and finetuned models on complex tasks that require reasoning skills\nto solve. ALERT provides a test bed to asses any language model on fine-grained\nreasoning skills, which spans over 20 datasets and covers 10 different\nreasoning skills. We leverage ALERT to further investigate the role of\nfinetuning. With extensive empirical analysis we find that language models\nlearn more reasoning skills such as textual entailment, abductive reasoning,\nand analogical reasoning during finetuning stage compared to pretraining state.\nWe also find that when language models are finetuned they tend to overfit to\nthe prompt template, which hurts the robustness of models causing\ngeneralization problems.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=alert++adapting+language+models+to+reasoning+tasks&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Current large language models can perform reasonably well on complex tasks\nthat require step-by-step reasoning with few-shot learning. Are these models\napplying reasoning skills they have learnt during pre-training and reason\noutside of their training context, or are they simply memorizing their training\ncorpus at finer granularity and have learnt to better understand their context?\nTo tease apart these possibilities, we introduce ALERT, a benchmark and suite\nof analyses for assessing language models' reasoning ability comparing\npre-trained and finetuned models on complex tasks that require reasoning skills\nto solve. ALERT provides a test bed to asses any language model on fine-grained\nreasoning skills, which spans over 20 datasets and covers 10 different\nreasoning skills. We leverage ALERT to further investigate the role of\nfinetuning. With extensive empirical analysis we find that language models\nlearn more reasoning skills such as textual entailment, abductive reasoning,\nand analogical reasoning during finetuning stage compared to pretraining state.\nWe also find that when language models are finetuned they tend to overfit to\nthe prompt template, which hurts the robustness of models causing\ngeneralization problems."}, "authors": [{"name": "Ping Yu"}, {"name": "Tianlu Wang"}, {"name": "Olga Golovneva"}, {"name": "Badr Alkhamissy"}, {"name": "Gargi Ghosh"}, {"name": "Mona Diab"}, {"name": "Asli Celikyilmaz"}], "author_detail": {"name": "Asli Celikyilmaz"}, "author": "Asli Celikyilmaz", "links": [{"href": "http://arxiv.org/abs/2212.08286v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2212.08286v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}