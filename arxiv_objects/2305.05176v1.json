{"id": "http://arxiv.org/abs/2305.05176v1", "guidislink": true, "link": "http://arxiv.org/abs/2305.05176v1", "updated": "2023-05-09T05:11:02Z", "updated_parsed": [2023, 5, 9, 5, 11, 2, 1, 129, 0], "published": "2023-05-09T05:11:02Z", "published_parsed": [2023, 5, 9, 5, 11, 2, 1, 129, 0], "title": "FrugalGPT: How to Use Large Language Models While Reducing Cost and\n  Improving Performance", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=frugalgpt++how+to+use+large+language+models+while+reducing+cost+and+improving+performance&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "FrugalGPT: How to Use Large Language Models While Reducing Cost and\n  Improving Performance"}, "summary": "There is a rapidly growing number of large language models (LLMs) that users\ncan query for a fee. We review the cost associated with querying popular LLM\nAPIs, e.g. GPT-4, ChatGPT, J1-Jumbo, and find that these models have\nheterogeneous pricing structures, with fees that can differ by two orders of\nmagnitude. In particular, using LLMs on large collections of queries and text\ncan be expensive. Motivated by this, we outline and discuss three types of\nstrategies that users can exploit to reduce the inference cost associated with\nusing LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade. As\nan example, we propose FrugalGPT, a simple yet flexible instantiation of LLM\ncascade which learns which combinations of LLMs to use for different queries in\norder to reduce cost and improve accuracy. Our experiments show that FrugalGPT\ncan match the performance of the best individual LLM (e.g. GPT-4) with up to\n98% cost reduction or improve the accuracy over GPT-4 by 4% with the same cost.\nThe ideas and findings presented here lay a foundation for using LLMs\nsustainably and efficiently.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=frugalgpt++how+to+use+large+language+models+while+reducing+cost+and+improving+performance&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "There is a rapidly growing number of large language models (LLMs) that users\ncan query for a fee. We review the cost associated with querying popular LLM\nAPIs, e.g. GPT-4, ChatGPT, J1-Jumbo, and find that these models have\nheterogeneous pricing structures, with fees that can differ by two orders of\nmagnitude. In particular, using LLMs on large collections of queries and text\ncan be expensive. Motivated by this, we outline and discuss three types of\nstrategies that users can exploit to reduce the inference cost associated with\nusing LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade. As\nan example, we propose FrugalGPT, a simple yet flexible instantiation of LLM\ncascade which learns which combinations of LLMs to use for different queries in\norder to reduce cost and improve accuracy. Our experiments show that FrugalGPT\ncan match the performance of the best individual LLM (e.g. GPT-4) with up to\n98% cost reduction or improve the accuracy over GPT-4 by 4% with the same cost.\nThe ideas and findings presented here lay a foundation for using LLMs\nsustainably and efficiently."}, "authors": [{"name": "Lingjiao Chen"}, {"name": "Matei Zaharia"}, {"name": "James Zou"}], "author_detail": {"name": "James Zou"}, "author": "James Zou", "links": [{"href": "http://arxiv.org/abs/2305.05176v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2305.05176v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.SE", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}