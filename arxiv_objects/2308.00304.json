{"id": "http://arxiv.org/abs/2308.00304v1", "guidislink": true, "link": "http://arxiv.org/abs/2308.00304v1", "updated": "2023-08-01T05:54:12Z", "updated_parsed": [2023, 8, 1, 5, 54, 12, 1, 213, 0], "published": "2023-08-01T05:54:12Z", "published_parsed": [2023, 8, 1, 5, 54, 12, 1, 213, 0], "title": "Skills-in-Context Prompting: Unlocking Compositionality in Large\n  Language Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=skills+in+context+prompting++unlocking+compositionality+in+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Skills-in-Context Prompting: Unlocking Compositionality in Large\n  Language Models"}, "summary": "We consider the problem of eliciting compositional generalization\ncapabilities in large language models (LLMs) with a novel type of prompting\nstrategy. Compositional generalization empowers the LLMs to solve problems that\nare harder than the ones they have seen (i.e., easy-to-hard generalization),\nwhich is a critical reasoning capability of human-like intelligence. However,\neven the current state-of-the-art LLMs still struggle with this form of\nreasoning. To bridge this gap, we propose skills-in-context (SKiC) prompting,\nwhich instructs LLMs how to compose basic skills to resolve more complex\nproblems. We find that it is crucial to demonstrate both the skills and the\ncompositional examples within the same prompting context. With as few as two\nexamplars, our SKiC prompting initiates strong synergies between skills and\ntheir composition capabilities. Notably, it empowers LLMs to solve unseen\nproblems that require innovative skill compositions, achieving near-perfect\ngeneralization on a broad range of challenging compositionality tasks.\nIntriguingly, SKiC prompting unlocks the latent potential of LLMs, enabling\nthem to leverage pre-existing internal skills acquired during earlier\npretraining and alignment stages, even when these skills are not explicitly\npresented in the prompting context. This results in the capability of LLMs to\nsolve unseen complex problems by activating and composing these internal\ncompetencies.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=skills+in+context+prompting++unlocking+compositionality+in+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "We consider the problem of eliciting compositional generalization\ncapabilities in large language models (LLMs) with a novel type of prompting\nstrategy. Compositional generalization empowers the LLMs to solve problems that\nare harder than the ones they have seen (i.e., easy-to-hard generalization),\nwhich is a critical reasoning capability of human-like intelligence. However,\neven the current state-of-the-art LLMs still struggle with this form of\nreasoning. To bridge this gap, we propose skills-in-context (SKiC) prompting,\nwhich instructs LLMs how to compose basic skills to resolve more complex\nproblems. We find that it is crucial to demonstrate both the skills and the\ncompositional examples within the same prompting context. With as few as two\nexamplars, our SKiC prompting initiates strong synergies between skills and\ntheir composition capabilities. Notably, it empowers LLMs to solve unseen\nproblems that require innovative skill compositions, achieving near-perfect\ngeneralization on a broad range of challenging compositionality tasks.\nIntriguingly, SKiC prompting unlocks the latent potential of LLMs, enabling\nthem to leverage pre-existing internal skills acquired during earlier\npretraining and alignment stages, even when these skills are not explicitly\npresented in the prompting context. This results in the capability of LLMs to\nsolve unseen complex problems by activating and composing these internal\ncompetencies."}, "authors": [{"name": "Jiaao Chen"}, {"name": "Xiaoman Pan"}, {"name": "Dian Yu"}, {"name": "Kaiqiang Song"}, {"name": "Xiaoyang Wang"}, {"name": "Dong Yu"}, {"name": "Jianshu Chen"}], "author_detail": {"name": "Jianshu Chen"}, "author": "Jianshu Chen", "links": [{"href": "http://arxiv.org/abs/2308.00304v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2308.00304v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}