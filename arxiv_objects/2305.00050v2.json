{"id": "http://arxiv.org/abs/2305.00050v2", "guidislink": true, "link": "http://arxiv.org/abs/2305.00050v2", "updated": "2023-05-08T17:54:45Z", "updated_parsed": [2023, 5, 8, 17, 54, 45, 0, 128, 0], "published": "2023-04-28T19:00:43Z", "published_parsed": [2023, 4, 28, 19, 0, 43, 4, 118, 0], "title": "Causal Reasoning and Large Language Models: Opening a New Frontier for\n  Causality", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=causal+reasoning+and+large+language+models++opening+a+new+frontier+for+causality&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Causal Reasoning and Large Language Models: Opening a New Frontier for\n  Causality"}, "summary": "The causal capabilities of large language models (LLMs) is a matter of\nsignificant debate, with critical implications for the use of LLMs in\nsocietally impactful domains such as medicine, science, law, and policy. We\nfurther our understanding of LLMs and their causal implications, considering\nthe distinctions between different types of causal reasoning tasks, as well as\nthe entangled threats of construct and measurement validity. LLM-based methods\nestablish new state-of-the-art accuracies on multiple causal benchmarks.\nAlgorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise\ncausal discovery task (97%, 13 points gain), counterfactual reasoning task\n(92%, 20 points gain), and actual causality (86% accuracy in determining\nnecessary and sufficient causes in vignettes). At the same time, LLMs exhibit\nunpredictable failure modes and we provide some techniques to interpret their\nrobustness.\n  Crucially, LLMs perform these causal tasks while relying on sources of\nknowledge and methods distinct from and complementary to non-LLM based\napproaches. Specifically, LLMs bring capabilities so far understood to be\nrestricted to humans, such as using collected knowledge to generate causal\ngraphs or identifying background causal context from natural language. We\nenvision LLMs to be used alongside existing causal methods, as a proxy for\nhuman domain knowledge and to reduce human effort in setting up a causal\nanalysis, one of the biggest impediments to the widespread adoption of causal\nmethods. We also see existing causal methods as promising tools for LLMs to\nformalize, validate, and communicate their reasoning especially in high-stakes\nscenarios.\n  In capturing common sense and domain knowledge about causal mechanisms and\nsupporting translation between natural language and formal methods, LLMs open\nnew frontiers for advancing the research, practice, and adoption of causality.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=causal+reasoning+and+large+language+models++opening+a+new+frontier+for+causality&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "The causal capabilities of large language models (LLMs) is a matter of\nsignificant debate, with critical implications for the use of LLMs in\nsocietally impactful domains such as medicine, science, law, and policy. We\nfurther our understanding of LLMs and their causal implications, considering\nthe distinctions between different types of causal reasoning tasks, as well as\nthe entangled threats of construct and measurement validity. LLM-based methods\nestablish new state-of-the-art accuracies on multiple causal benchmarks.\nAlgorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise\ncausal discovery task (97%, 13 points gain), counterfactual reasoning task\n(92%, 20 points gain), and actual causality (86% accuracy in determining\nnecessary and sufficient causes in vignettes). At the same time, LLMs exhibit\nunpredictable failure modes and we provide some techniques to interpret their\nrobustness.\n  Crucially, LLMs perform these causal tasks while relying on sources of\nknowledge and methods distinct from and complementary to non-LLM based\napproaches. Specifically, LLMs bring capabilities so far understood to be\nrestricted to humans, such as using collected knowledge to generate causal\ngraphs or identifying background causal context from natural language. We\nenvision LLMs to be used alongside existing causal methods, as a proxy for\nhuman domain knowledge and to reduce human effort in setting up a causal\nanalysis, one of the biggest impediments to the widespread adoption of causal\nmethods. We also see existing causal methods as promising tools for LLMs to\nformalize, validate, and communicate their reasoning especially in high-stakes\nscenarios.\n  In capturing common sense and domain knowledge about causal mechanisms and\nsupporting translation between natural language and formal methods, LLMs open\nnew frontiers for advancing the research, practice, and adoption of causality."}, "authors": [{"name": "Emre K\u0131c\u0131man"}, {"name": "Robert Ness"}, {"name": "Amit Sharma"}, {"name": "Chenhao Tan"}], "author_detail": {"name": "Chenhao Tan"}, "author": "Chenhao Tan", "arxiv_comment": "42 pages, 5 figures, working paper (minor changes in v2 to fix typos)", "links": [{"href": "http://arxiv.org/abs/2305.00050v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2305.00050v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CY", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.HC", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}