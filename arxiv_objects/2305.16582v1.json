{"id": "http://arxiv.org/abs/2305.16582v1", "guidislink": true, "link": "http://arxiv.org/abs/2305.16582v1", "updated": "2023-05-26T02:15:09Z", "updated_parsed": [2023, 5, 26, 2, 15, 9, 4, 146, 0], "published": "2023-05-26T02:15:09Z", "published_parsed": [2023, 5, 26, 2, 15, 9, 4, 146, 0], "title": "Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large\n  Language Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=beyond+chain+of+thought++effective+graph+of+thought+reasoning+in+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large\n  Language Models"}, "summary": "With the widespread use of large language models (LLMs) in NLP tasks,\nresearchers have discovered the potential of Chain-of-thought (CoT) to assist\nLLMs in accomplishing complex reasoning tasks by generating intermediate steps.\nHowever, human thought processes are often non-linear, rather than simply\nsequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT)\nreasoning, which models human thought processes not only as a chain but also as\na graph. By representing thought units as nodes and connections between them as\nedges, our approach captures the non-sequential nature of human thinking and\nallows for a more realistic modeling of thought processes. Similar to\nMultimodal-CoT, we modeled GoT reasoning as a two-stage framework, generating\nrationales first and then producing the final answer. Specifically, we employ\nan additional graph-of-thoughts encoder for GoT representation learning and\nfuse the GoT representation with the original input representation through a\ngated fusion mechanism. We implement a GoT reasoning model on the T5\npre-trained model and evaluate its performance on a text-only reasoning task\n(GSM8K) and a multimodal reasoning task (ScienceQA). Our model achieves\nsignificant improvement over the strong CoT baseline with 3.41% and 5.08% on\nthe GSM8K test set with T5-base and T5-large architectures, respectively.\nAdditionally, our model boosts accuracy from 84.91% to 91.54% using the T5-base\nmodel and from 91.68% to 92.77% using the T5-large model over the\nstate-of-the-art Multimodal-CoT on the ScienceQA test set. Experiments have\nshown that GoT achieves comparable results to Multimodal-CoT(large) with over\n700M parameters, despite having fewer than 250M backbone model parameters,\ndemonstrating the effectiveness of GoT.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=beyond+chain+of+thought++effective+graph+of+thought+reasoning+in+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "With the widespread use of large language models (LLMs) in NLP tasks,\nresearchers have discovered the potential of Chain-of-thought (CoT) to assist\nLLMs in accomplishing complex reasoning tasks by generating intermediate steps.\nHowever, human thought processes are often non-linear, rather than simply\nsequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT)\nreasoning, which models human thought processes not only as a chain but also as\na graph. By representing thought units as nodes and connections between them as\nedges, our approach captures the non-sequential nature of human thinking and\nallows for a more realistic modeling of thought processes. Similar to\nMultimodal-CoT, we modeled GoT reasoning as a two-stage framework, generating\nrationales first and then producing the final answer. Specifically, we employ\nan additional graph-of-thoughts encoder for GoT representation learning and\nfuse the GoT representation with the original input representation through a\ngated fusion mechanism. We implement a GoT reasoning model on the T5\npre-trained model and evaluate its performance on a text-only reasoning task\n(GSM8K) and a multimodal reasoning task (ScienceQA). Our model achieves\nsignificant improvement over the strong CoT baseline with 3.41% and 5.08% on\nthe GSM8K test set with T5-base and T5-large architectures, respectively.\nAdditionally, our model boosts accuracy from 84.91% to 91.54% using the T5-base\nmodel and from 91.68% to 92.77% using the T5-large model over the\nstate-of-the-art Multimodal-CoT on the ScienceQA test set. Experiments have\nshown that GoT achieves comparable results to Multimodal-CoT(large) with over\n700M parameters, despite having fewer than 250M backbone model parameters,\ndemonstrating the effectiveness of GoT."}, "authors": [{"name": "Yao Yao"}, {"name": "Zuchao Li"}, {"name": "Hai Zhao"}], "author_detail": {"name": "Hai Zhao"}, "author": "Hai Zhao", "links": [{"href": "http://arxiv.org/abs/2305.16582v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2305.16582v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}