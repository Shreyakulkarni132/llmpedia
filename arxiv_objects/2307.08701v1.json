{"id": "http://arxiv.org/abs/2307.08701v1", "guidislink": true, "link": "http://arxiv.org/abs/2307.08701v1", "updated": "2023-07-17T17:59:40Z", "updated_parsed": [2023, 7, 17, 17, 59, 40, 0, 198, 0], "published": "2023-07-17T17:59:40Z", "published_parsed": [2023, 7, 17, 17, 59, 40, 0, 198, 0], "title": "AlpaGasus: Training A Better Alpaca with Fewer Data", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=alpagasus++training+a+better+alpaca+with+fewer+data&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "AlpaGasus: Training A Better Alpaca with Fewer Data"}, "summary": "Large language models~(LLMs) obtain instruction-following capability through\ninstruction-finetuning (IFT) on supervised instruction/response data. However,\nwidely used IFT datasets (e.g., Alpaca's 52k data) surprisingly contain many\nlow-quality instances with incorrect or irrelevant responses, which are\nmisleading and detrimental to IFT. In this paper, we propose a simple and\neffective data selection strategy that automatically identifies and removes\nlow-quality data using a strong LLM (e.g., ChatGPT). To this end, we introduce\nAlpaGasus, which is finetuned on only 9k high-quality data filtered from the\n52k Alpaca data. AlpaGasus significantly outperforms the original Alpaca as\nevaluated by GPT-4 on multiple test sets and its 13B variant matches $>90\\%$\nperformance of its teacher LLM (i.e., Text-Davinci-003) on test tasks. It also\nprovides 5.7x faster training, reducing the training time for a 7B variant from\n80 minutes (for Alpaca) to 14 minutes \\footnote{We apply IFT for the same\nnumber of epochs as Alpaca(7B) but on fewer data, using 4$\\times$NVIDIA A100\n(80GB) GPUs and following the original Alpaca setting and hyperparameters.}.\nOverall, AlpaGasus demonstrates a novel data-centric IFT paradigm that can be\ngenerally applied to instruction-tuning data, leading to faster training and\nbetter instruction-following models. Our project page is available at:\n\\url{https://lichang-chen.github.io/AlpaGasus/}.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=alpagasus++training+a+better+alpaca+with+fewer+data&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Large language models~(LLMs) obtain instruction-following capability through\ninstruction-finetuning (IFT) on supervised instruction/response data. However,\nwidely used IFT datasets (e.g., Alpaca's 52k data) surprisingly contain many\nlow-quality instances with incorrect or irrelevant responses, which are\nmisleading and detrimental to IFT. In this paper, we propose a simple and\neffective data selection strategy that automatically identifies and removes\nlow-quality data using a strong LLM (e.g., ChatGPT). To this end, we introduce\nAlpaGasus, which is finetuned on only 9k high-quality data filtered from the\n52k Alpaca data. AlpaGasus significantly outperforms the original Alpaca as\nevaluated by GPT-4 on multiple test sets and its 13B variant matches $>90\\%$\nperformance of its teacher LLM (i.e., Text-Davinci-003) on test tasks. It also\nprovides 5.7x faster training, reducing the training time for a 7B variant from\n80 minutes (for Alpaca) to 14 minutes \\footnote{We apply IFT for the same\nnumber of epochs as Alpaca(7B) but on fewer data, using 4$\\times$NVIDIA A100\n(80GB) GPUs and following the original Alpaca setting and hyperparameters.}.\nOverall, AlpaGasus demonstrates a novel data-centric IFT paradigm that can be\ngenerally applied to instruction-tuning data, leading to faster training and\nbetter instruction-following models. Our project page is available at:\n\\url{https://lichang-chen.github.io/AlpaGasus/}."}, "authors": [{"name": "Lichang Chen"}, {"name": "Shiyang Li"}, {"name": "Jun Yan"}, {"name": "Hai Wang"}, {"name": "Kalpa Gunaratna"}, {"name": "Vikas Yadav"}, {"name": "Zheng Tang"}, {"name": "Vijay Srinivasan"}, {"name": "Tianyi Zhou"}, {"name": "Heng Huang"}, {"name": "Hongxia Jin"}], "author_detail": {"name": "Hongxia Jin"}, "author": "Hongxia Jin", "arxiv_comment": "22 pages; 22 figures", "links": [{"href": "http://arxiv.org/abs/2307.08701v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.08701v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}