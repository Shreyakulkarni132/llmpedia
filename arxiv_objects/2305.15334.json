{"id": "http://arxiv.org/abs/2305.15334v1", "guidislink": true, "link": "http://arxiv.org/abs/2305.15334v1", "updated": "2023-05-24T16:48:11Z", "updated_parsed": [2023, 5, 24, 16, 48, 11, 2, 144, 0], "published": "2023-05-24T16:48:11Z", "published_parsed": [2023, 5, 24, 16, 48, 11, 2, 144, 0], "title": "Gorilla: Large Language Model Connected with Massive APIs", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=gorilla++large+language+model+connected+with+massive+apis&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Gorilla: Large Language Model Connected with Massive APIs"}, "summary": "Large Language Models (LLMs) have seen an impressive wave of advances\nrecently, with models now excelling in a variety of tasks, such as mathematical\nreasoning and program synthesis. However, their potential to effectively use\ntools via API calls remains unfulfilled. This is a challenging task even for\ntoday's state-of-the-art LLMs such as GPT-4, largely due to their inability to\ngenerate accurate input arguments and their tendency to hallucinate the wrong\nusage of an API call. We release Gorilla, a finetuned LLaMA-based model that\nsurpasses the performance of GPT-4 on writing API calls. When combined with a\ndocument retriever, Gorilla demonstrates a strong capability to adapt to\ntest-time document changes, enabling flexible user updates or version changes.\nIt also substantially mitigates the issue of hallucination, commonly\nencountered when prompting LLMs directly. To evaluate the model's ability, we\nintroduce APIBench, a comprehensive dataset consisting of HuggingFace,\nTorchHub, and TensorHub APIs. The successful integration of the retrieval\nsystem with Gorilla demonstrates the potential for LLMs to use tools more\naccurately, keep up with frequently updated documentation, and consequently\nincrease the reliability and applicability of their outputs. Gorilla's code,\nmodel, data, and demo are available at https://gorilla.cs.berkeley.edu", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=gorilla++large+language+model+connected+with+massive+apis&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Large Language Models (LLMs) have seen an impressive wave of advances\nrecently, with models now excelling in a variety of tasks, such as mathematical\nreasoning and program synthesis. However, their potential to effectively use\ntools via API calls remains unfulfilled. This is a challenging task even for\ntoday's state-of-the-art LLMs such as GPT-4, largely due to their inability to\ngenerate accurate input arguments and their tendency to hallucinate the wrong\nusage of an API call. We release Gorilla, a finetuned LLaMA-based model that\nsurpasses the performance of GPT-4 on writing API calls. When combined with a\ndocument retriever, Gorilla demonstrates a strong capability to adapt to\ntest-time document changes, enabling flexible user updates or version changes.\nIt also substantially mitigates the issue of hallucination, commonly\nencountered when prompting LLMs directly. To evaluate the model's ability, we\nintroduce APIBench, a comprehensive dataset consisting of HuggingFace,\nTorchHub, and TensorHub APIs. The successful integration of the retrieval\nsystem with Gorilla demonstrates the potential for LLMs to use tools more\naccurately, keep up with frequently updated documentation, and consequently\nincrease the reliability and applicability of their outputs. Gorilla's code,\nmodel, data, and demo are available at https://gorilla.cs.berkeley.edu"}, "authors": [{"name": "Shishir G. Patil"}, {"name": "Tianjun Zhang"}, {"name": "Xin Wang"}, {"name": "Joseph E. Gonzalez"}], "author_detail": {"name": "Joseph E. Gonzalez"}, "author": "Joseph E. Gonzalez", "links": [{"href": "http://arxiv.org/abs/2305.15334v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2305.15334v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}