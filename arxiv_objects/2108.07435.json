{"id": "http://arxiv.org/abs/2108.07435v2", "guidislink": true, "link": "http://arxiv.org/abs/2108.07435v2", "updated": "2021-12-07T16:16:24Z", "updated_parsed": [2021, 12, 7, 16, 16, 24, 1, 341, 0], "published": "2021-08-17T04:13:11Z", "published_parsed": [2021, 8, 17, 4, 13, 11, 1, 229, 0], "title": "Modeling Protein Using Large-scale Pretrain Language Model", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=modeling+protein+using+large+scale+pretrain+language+model&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Modeling Protein Using Large-scale Pretrain Language Model"}, "summary": "Protein is linked to almost every life process. Therefore, analyzing the\nbiological structure and property of protein sequences is critical to the\nexploration of life, as well as disease detection and drug discovery.\nTraditional protein analysis methods tend to be labor-intensive and\ntime-consuming. The emergence of deep learning models makes modeling data\npatterns in large quantities of data possible. Interdisciplinary researchers\nhave begun to leverage deep learning methods to model large biological\ndatasets, e.g. using long short-term memory and convolutional neural network\nfor protein sequence classification. After millions of years of evolution,\nevolutionary information is encoded in protein sequences. Inspired by the\nsimilarity between natural language and protein sequences, we use large-scale\nlanguage models to model evolutionary-scale protein sequences, encoding protein\nbiology information in representation. Significant improvements are observed in\nboth token-level and sequence-level tasks, demonstrating that our large-scale\nmodel can accurately capture evolution information from pretraining on\nevolutionary-scale individual sequences. Our code and model are available at\nhttps://github.com/THUDM/ProteinLM.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=modeling+protein+using+large+scale+pretrain+language+model&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Protein is linked to almost every life process. Therefore, analyzing the\nbiological structure and property of protein sequences is critical to the\nexploration of life, as well as disease detection and drug discovery.\nTraditional protein analysis methods tend to be labor-intensive and\ntime-consuming. The emergence of deep learning models makes modeling data\npatterns in large quantities of data possible. Interdisciplinary researchers\nhave begun to leverage deep learning methods to model large biological\ndatasets, e.g. using long short-term memory and convolutional neural network\nfor protein sequence classification. After millions of years of evolution,\nevolutionary information is encoded in protein sequences. Inspired by the\nsimilarity between natural language and protein sequences, we use large-scale\nlanguage models to model evolutionary-scale protein sequences, encoding protein\nbiology information in representation. Significant improvements are observed in\nboth token-level and sequence-level tasks, demonstrating that our large-scale\nmodel can accurately capture evolution information from pretraining on\nevolutionary-scale individual sequences. Our code and model are available at\nhttps://github.com/THUDM/ProteinLM."}, "authors": [{"name": "Yijia Xiao"}, {"name": "Jiezhong Qiu"}, {"name": "Ziang Li"}, {"name": "Chang-Yu Hsieh"}, {"name": "Jie Tang"}], "author_detail": {"name": "Jie Tang"}, "author": "Jie Tang", "arxiv_comment": "Accepted paper in Pretrain@KDD 2021 (The International Workshop on\n  Pretraining: Algorithms, Architectures, and Applications)", "links": [{"href": "http://arxiv.org/abs/2108.07435v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2108.07435v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "q-bio.BM", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}