{"id": "http://arxiv.org/abs/2206.01335v2", "guidislink": true, "link": "http://arxiv.org/abs/2206.01335v2", "updated": "2022-06-12T09:57:45Z", "updated_parsed": [2022, 6, 12, 9, 57, 45, 6, 163, 0], "published": "2022-06-02T23:15:42Z", "published_parsed": [2022, 6, 2, 23, 15, 42, 3, 153, 0], "title": "Code Generation Tools (Almost) for Free? A Study of Few-Shot,\n  Pre-Trained Language Models on Code", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=code+generation+tools++almost++for+free++a+study+of+few+shot++pre+trained+language+models+on+code&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Code Generation Tools (Almost) for Free? A Study of Few-Shot,\n  Pre-Trained Language Models on Code"}, "summary": "Few-shot learning with large-scale, pre-trained language models is a powerful\nway to answer questions about code, e.g., how to complete a given code example,\nor even generate code snippets from scratch. The success of these models raises\nthe question whether they could serve as a basis for building a wide range code\ngeneration tools. Traditionally, such tools are built manually and separately\nfor each task. Instead, few-shot learning may allow to obtain different tools\nfrom a single pre-trained language model by simply providing a few examples or\na natural language description of the expected tool behavior. This paper\nstudies to what extent a state-of-the-art, pre-trained language model of code,\nCodex, may serve this purpose. We consider three code manipulation and code\ngeneration tasks targeted by a range of traditional tools: (i) code mutation;\n(ii) test oracle generation from natural language documentation; and (iii) test\ncase generation. For each task, we compare few-shot learning to a manually\nbuilt tool. Our results show that the model-based tools complement (code\nmutation), are on par (test oracle generation), or even outperform their\nrespective traditionally built tool (test case generation), while imposing far\nless effort to develop them. By comparing the effectiveness of different\nvariants of the model-based tools, we provide insights on how to design an\nappropriate input (\"prompt\") to the model and what influence the size of the\nmodel has. For example, we find that providing a small natural language\ndescription of the code generation task is an easy way to improve predictions.\nOverall, we conclude that few-shot language models are surprisingly effective,\nyet there is still more work to be done, such as exploring more diverse ways of\nprompting and tackling even more involved tasks.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=code+generation+tools++almost++for+free++a+study+of+few+shot++pre+trained+language+models+on+code&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Few-shot learning with large-scale, pre-trained language models is a powerful\nway to answer questions about code, e.g., how to complete a given code example,\nor even generate code snippets from scratch. The success of these models raises\nthe question whether they could serve as a basis for building a wide range code\ngeneration tools. Traditionally, such tools are built manually and separately\nfor each task. Instead, few-shot learning may allow to obtain different tools\nfrom a single pre-trained language model by simply providing a few examples or\na natural language description of the expected tool behavior. This paper\nstudies to what extent a state-of-the-art, pre-trained language model of code,\nCodex, may serve this purpose. We consider three code manipulation and code\ngeneration tasks targeted by a range of traditional tools: (i) code mutation;\n(ii) test oracle generation from natural language documentation; and (iii) test\ncase generation. For each task, we compare few-shot learning to a manually\nbuilt tool. Our results show that the model-based tools complement (code\nmutation), are on par (test oracle generation), or even outperform their\nrespective traditionally built tool (test case generation), while imposing far\nless effort to develop them. By comparing the effectiveness of different\nvariants of the model-based tools, we provide insights on how to design an\nappropriate input (\"prompt\") to the model and what influence the size of the\nmodel has. For example, we find that providing a small natural language\ndescription of the code generation task is an easy way to improve predictions.\nOverall, we conclude that few-shot language models are surprisingly effective,\nyet there is still more work to be done, such as exploring more diverse ways of\nprompting and tackling even more involved tasks."}, "authors": [{"name": "Patrick Barei\u00df"}, {"name": "Beatriz Souza"}, {"name": "Marcelo d'Amorim"}, {"name": "Michael Pradel"}], "author_detail": {"name": "Michael Pradel"}, "author": "Michael Pradel", "arxiv_comment": "12 pages, 5 figures", "links": [{"href": "http://arxiv.org/abs/2206.01335v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2206.01335v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SE", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SE", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}