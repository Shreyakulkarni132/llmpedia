{"id": "http://arxiv.org/abs/2306.01693v1", "guidislink": true, "link": "http://arxiv.org/abs/2306.01693v1", "updated": "2023-06-02T17:11:37Z", "updated_parsed": [2023, 6, 2, 17, 11, 37, 4, 153, 0], "published": "2023-06-02T17:11:37Z", "published_parsed": [2023, 6, 2, 17, 11, 37, 4, 153, 0], "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model\n  Training", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=fine+grained+human+feedback+gives+better+rewards+for+language+model+training&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Fine-Grained Human Feedback Gives Better Rewards for Language Model\n  Training"}, "summary": "Language models (LMs) often exhibit undesirable text generation behaviors,\nincluding generating false, toxic, or irrelevant outputs. Reinforcement\nlearning from human feedback (RLHF) - where human preference judgments on LM\noutputs are transformed into a learning signal - has recently shown promise in\naddressing these issues. However, such holistic feedback conveys limited\ninformation on long text outputs; it does not indicate which aspects of the\noutputs influenced user preference; e.g., which parts contain what type(s) of\nerrors. In this paper, we use fine-grained human feedback (e.g., which sentence\nis false, which sub-sentence is irrelevant) as an explicit training signal. We\nintroduce Fine-Grained RLHF, a framework that enables training and learning\nfrom reward functions that are fine-grained in two respects: (1) density,\nproviding a reward after every segment (e.g., a sentence) is generated; and (2)\nincorporating multiple reward models associated with different feedback types\n(e.g., factual incorrectness, irrelevance, and information incompleteness). We\nconduct experiments on detoxification and long-form question answering to\nillustrate how learning with such reward functions leads to improved\nperformance, supported by both automatic and human evaluation. Additionally, we\nshow that LM behaviors can be customized using different combinations of\nfine-grained reward models. We release all data, collected human feedback, and\ncodes at https://FineGrainedRLHF.github.io.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=fine+grained+human+feedback+gives+better+rewards+for+language+model+training&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Language models (LMs) often exhibit undesirable text generation behaviors,\nincluding generating false, toxic, or irrelevant outputs. Reinforcement\nlearning from human feedback (RLHF) - where human preference judgments on LM\noutputs are transformed into a learning signal - has recently shown promise in\naddressing these issues. However, such holistic feedback conveys limited\ninformation on long text outputs; it does not indicate which aspects of the\noutputs influenced user preference; e.g., which parts contain what type(s) of\nerrors. In this paper, we use fine-grained human feedback (e.g., which sentence\nis false, which sub-sentence is irrelevant) as an explicit training signal. We\nintroduce Fine-Grained RLHF, a framework that enables training and learning\nfrom reward functions that are fine-grained in two respects: (1) density,\nproviding a reward after every segment (e.g., a sentence) is generated; and (2)\nincorporating multiple reward models associated with different feedback types\n(e.g., factual incorrectness, irrelevance, and information incompleteness). We\nconduct experiments on detoxification and long-form question answering to\nillustrate how learning with such reward functions leads to improved\nperformance, supported by both automatic and human evaluation. Additionally, we\nshow that LM behaviors can be customized using different combinations of\nfine-grained reward models. We release all data, collected human feedback, and\ncodes at https://FineGrainedRLHF.github.io."}, "authors": [{"name": "Zeqiu Wu"}, {"name": "Yushi Hu"}, {"name": "Weijia Shi"}, {"name": "Nouha Dziri"}, {"name": "Alane Suhr"}, {"name": "Prithviraj Ammanabrolu"}, {"name": "Noah A. Smith"}, {"name": "Mari Ostendorf"}, {"name": "Hannaneh Hajishirzi"}], "author_detail": {"name": "Hannaneh Hajishirzi"}, "author": "Hannaneh Hajishirzi", "links": [{"href": "http://arxiv.org/abs/2306.01693v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2306.01693v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}