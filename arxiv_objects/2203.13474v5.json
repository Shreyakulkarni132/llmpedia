{"id": "http://arxiv.org/abs/2203.13474v5", "guidislink": true, "link": "http://arxiv.org/abs/2203.13474v5", "updated": "2023-02-27T21:26:48Z", "updated_parsed": [2023, 2, 27, 21, 26, 48, 0, 58, 0], "published": "2022-03-25T06:55:15Z", "published_parsed": [2022, 3, 25, 6, 55, 15, 4, 84, 0], "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program\n  Synthesis", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=codegen++an+open+large+language+model+for+code+with+multi+turn+program+synthesis&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program\n  Synthesis"}, "summary": "Program synthesis strives to generate a computer program as a solution to a\ngiven problem specification, expressed with input-output examples or natural\nlanguage descriptions. The prevalence of large language models advances the\nstate-of-the-art for program synthesis, though limited training resources and\ndata impede open access to such models. To democratize this, we train and\nrelease a family of large language models up to 16.1B parameters, called\nCODEGEN, on natural language and programming language data, and open source the\ntraining library JAXFORMER. We show the utility of the trained model by\ndemonstrating that it is competitive with the previous state-of-the-art on\nzero-shot Python code generation on HumanEval. We further investigate the\nmulti-step paradigm for program synthesis, where a single program is factorized\ninto multiple prompts specifying subproblems. To this end, we construct an open\nbenchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse\nproblem sets that are factorized into multi-turn prompts. Our analysis on MTPB\nshows that the same intent provided to CODEGEN in multi-turn fashion\nsignificantly improves program synthesis over that provided as a single turn.\nWe make the training library JAXFORMER and model checkpoints available as open\nsource contribution: https://github.com/salesforce/CodeGen.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=codegen++an+open+large+language+model+for+code+with+multi+turn+program+synthesis&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Program synthesis strives to generate a computer program as a solution to a\ngiven problem specification, expressed with input-output examples or natural\nlanguage descriptions. The prevalence of large language models advances the\nstate-of-the-art for program synthesis, though limited training resources and\ndata impede open access to such models. To democratize this, we train and\nrelease a family of large language models up to 16.1B parameters, called\nCODEGEN, on natural language and programming language data, and open source the\ntraining library JAXFORMER. We show the utility of the trained model by\ndemonstrating that it is competitive with the previous state-of-the-art on\nzero-shot Python code generation on HumanEval. We further investigate the\nmulti-step paradigm for program synthesis, where a single program is factorized\ninto multiple prompts specifying subproblems. To this end, we construct an open\nbenchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse\nproblem sets that are factorized into multi-turn prompts. Our analysis on MTPB\nshows that the same intent provided to CODEGEN in multi-turn fashion\nsignificantly improves program synthesis over that provided as a single turn.\nWe make the training library JAXFORMER and model checkpoints available as open\nsource contribution: https://github.com/salesforce/CodeGen."}, "authors": [{"name": "Erik Nijkamp"}, {"name": "Bo Pang"}, {"name": "Hiroaki Hayashi"}, {"name": "Lifu Tu"}, {"name": "Huan Wang"}, {"name": "Yingbo Zhou"}, {"name": "Silvio Savarese"}, {"name": "Caiming Xiong"}], "author_detail": {"name": "Caiming Xiong"}, "author": "Caiming Xiong", "links": [{"href": "http://arxiv.org/abs/2203.13474v5", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2203.13474v5", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.PL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}