{"id": "http://arxiv.org/abs/2307.03172v2", "guidislink": true, "link": "http://arxiv.org/abs/2307.03172v2", "updated": "2023-07-31T17:48:48Z", "updated_parsed": [2023, 7, 31, 17, 48, 48, 0, 212, 0], "published": "2023-07-06T17:54:11Z", "published_parsed": [2023, 7, 6, 17, 54, 11, 3, 187, 0], "title": "Lost in the Middle: How Language Models Use Long Contexts", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=lost+in+the+middle++how+language+models+use+long+contexts&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Lost in the Middle: How Language Models Use Long Contexts"}, "summary": "While recent language models have the ability to take long contexts as input,\nrelatively little is known about how well they use longer context. We analyze\nlanguage model performance on two tasks that require identifying relevant\ninformation within their input contexts: multi-document question answering and\nkey-value retrieval. We find that performance is often highest when relevant\ninformation occurs at the beginning or end of the input context, and\nsignificantly degrades when models must access relevant information in the\nmiddle of long contexts. Furthermore, performance substantially decreases as\nthe input context grows longer, even for explicitly long-context models. Our\nanalysis provides a better understanding of how language models use their input\ncontext and provides new evaluation protocols for future long-context models.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=lost+in+the+middle++how+language+models+use+long+contexts&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "While recent language models have the ability to take long contexts as input,\nrelatively little is known about how well they use longer context. We analyze\nlanguage model performance on two tasks that require identifying relevant\ninformation within their input contexts: multi-document question answering and\nkey-value retrieval. We find that performance is often highest when relevant\ninformation occurs at the beginning or end of the input context, and\nsignificantly degrades when models must access relevant information in the\nmiddle of long contexts. Furthermore, performance substantially decreases as\nthe input context grows longer, even for explicitly long-context models. Our\nanalysis provides a better understanding of how language models use their input\ncontext and provides new evaluation protocols for future long-context models."}, "authors": [{"name": "Nelson F. Liu"}, {"name": "Kevin Lin"}, {"name": "John Hewitt"}, {"name": "Ashwin Paranjape"}, {"name": "Michele Bevilacqua"}, {"name": "Fabio Petroni"}, {"name": "Percy Liang"}], "author_detail": {"name": "Percy Liang"}, "author": "Percy Liang", "arxiv_comment": "19 pages, 18 figures", "links": [{"href": "http://arxiv.org/abs/2307.03172v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.03172v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}