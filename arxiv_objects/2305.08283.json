{"id": "http://arxiv.org/abs/2305.08283v3", "guidislink": true, "link": "http://arxiv.org/abs/2305.08283v3", "updated": "2023-07-06T00:40:53Z", "updated_parsed": [2023, 7, 6, 0, 40, 53, 3, 187, 0], "published": "2023-05-15T00:06:30Z", "published_parsed": [2023, 5, 15, 0, 6, 30, 0, 135, 0], "title": "From Pretraining Data to Language Models to Downstream Tasks: Tracking\n  the Trails of Political Biases Leading to Unfair NLP Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=from+pretraining+data+to+language+models+to+downstream+tasks++tracking+the+trails+of+political+biases+leading+to+unfair+nlp+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "From Pretraining Data to Language Models to Downstream Tasks: Tracking\n  the Trails of Political Biases Leading to Unfair NLP Models"}, "summary": "Language models (LMs) are pretrained on diverse data sources, including news,\ndiscussion forums, books, and online encyclopedias. A significant portion of\nthis data includes opinions and perspectives which, on one hand, celebrate\ndemocracy and diversity of ideas, and on the other hand are inherently socially\nbiased. Our work develops new methods to (1) measure political biases in LMs\ntrained on such corpora, along social and economic axes, and (2) measure the\nfairness of downstream NLP models trained on top of politically biased LMs. We\nfocus on hate speech and misinformation detection, aiming to empirically\nquantify the effects of political (social, economic) biases in pretraining data\non the fairness of high-stakes social-oriented tasks. Our findings reveal that\npretrained LMs do have political leanings that reinforce the polarization\npresent in pretraining corpora, propagating social biases into hate speech\npredictions and misinformation detectors. We discuss the implications of our\nfindings for NLP research and propose future directions to mitigate unfairness.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=from+pretraining+data+to+language+models+to+downstream+tasks++tracking+the+trails+of+political+biases+leading+to+unfair+nlp+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Language models (LMs) are pretrained on diverse data sources, including news,\ndiscussion forums, books, and online encyclopedias. A significant portion of\nthis data includes opinions and perspectives which, on one hand, celebrate\ndemocracy and diversity of ideas, and on the other hand are inherently socially\nbiased. Our work develops new methods to (1) measure political biases in LMs\ntrained on such corpora, along social and economic axes, and (2) measure the\nfairness of downstream NLP models trained on top of politically biased LMs. We\nfocus on hate speech and misinformation detection, aiming to empirically\nquantify the effects of political (social, economic) biases in pretraining data\non the fairness of high-stakes social-oriented tasks. Our findings reveal that\npretrained LMs do have political leanings that reinforce the polarization\npresent in pretraining corpora, propagating social biases into hate speech\npredictions and misinformation detectors. We discuss the implications of our\nfindings for NLP research and propose future directions to mitigate unfairness."}, "authors": [{"name": "Shangbin Feng"}, {"name": "Chan Young Park"}, {"name": "Yuhan Liu"}, {"name": "Yulia Tsvetkov"}], "author_detail": {"name": "Yulia Tsvetkov"}, "author": "Yulia Tsvetkov", "arxiv_comment": "ACL 2023", "links": [{"href": "http://arxiv.org/abs/2305.08283v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2305.08283v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}