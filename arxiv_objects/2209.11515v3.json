{"id": "http://arxiv.org/abs/2209.11515v3", "guidislink": true, "link": "http://arxiv.org/abs/2209.11515v3", "updated": "2023-07-25T03:47:36Z", "updated_parsed": [2023, 7, 25, 3, 47, 36, 1, 206, 0], "published": "2022-09-23T10:50:47Z", "published_parsed": [2022, 9, 23, 10, 50, 47, 4, 266, 0], "title": "Large Language Models are Few-shot Testers: Exploring LLM-based General\n  Bug Reproduction", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=large+language+models+are+few+shot+testers++exploring+llm+based+general+bug+reproduction&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Large Language Models are Few-shot Testers: Exploring LLM-based General\n  Bug Reproduction"}, "summary": "Many automated test generation techniques have been developed to aid\ndevelopers with writing tests. To facilitate full automation, most existing\ntechniques aim to either increase coverage, or generate exploratory inputs.\nHowever, existing test generation techniques largely fall short of achieving\nmore semantic objectives, such as generating tests to reproduce a given bug\nreport. Reproducing bugs is nonetheless important, as our empirical study shows\nthat the number of tests added in open source repositories due to issues was\nabout 28% of the corresponding project test suite size. Meanwhile, due to the\ndifficulties of transforming the expected program semantics in bug reports into\ntest oracles, existing failure reproduction techniques tend to deal exclusively\nwith program crashes, a small subset of all bug reports. To automate test\ngeneration from general bug reports, we propose LIBRO, a framework that uses\nLarge Language Models (LLMs), which have been shown to be capable of performing\ncode-related tasks. Since LLMs themselves cannot execute the target buggy code,\nwe focus on post-processing steps that help us discern when LLMs are effective,\nand rank the produced tests according to their validity. Our evaluation of\nLIBRO shows that, on the widely studied Defects4J benchmark, LIBRO can generate\nfailure reproducing test cases for 33% of all studied cases (251 out of 750),\nwhile suggesting a bug reproducing test in first place for 149 bugs. To\nmitigate data contamination, we also evaluate LIBRO against 31 bug reports\nsubmitted after the collection of the LLM training data terminated: LIBRO\nproduces bug reproducing tests for 32% of the studied bug reports. Overall, our\nresults show LIBRO has the potential to significantly enhance developer\nefficiency by automatically generating tests from bug reports.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=large+language+models+are+few+shot+testers++exploring+llm+based+general+bug+reproduction&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Many automated test generation techniques have been developed to aid\ndevelopers with writing tests. To facilitate full automation, most existing\ntechniques aim to either increase coverage, or generate exploratory inputs.\nHowever, existing test generation techniques largely fall short of achieving\nmore semantic objectives, such as generating tests to reproduce a given bug\nreport. Reproducing bugs is nonetheless important, as our empirical study shows\nthat the number of tests added in open source repositories due to issues was\nabout 28% of the corresponding project test suite size. Meanwhile, due to the\ndifficulties of transforming the expected program semantics in bug reports into\ntest oracles, existing failure reproduction techniques tend to deal exclusively\nwith program crashes, a small subset of all bug reports. To automate test\ngeneration from general bug reports, we propose LIBRO, a framework that uses\nLarge Language Models (LLMs), which have been shown to be capable of performing\ncode-related tasks. Since LLMs themselves cannot execute the target buggy code,\nwe focus on post-processing steps that help us discern when LLMs are effective,\nand rank the produced tests according to their validity. Our evaluation of\nLIBRO shows that, on the widely studied Defects4J benchmark, LIBRO can generate\nfailure reproducing test cases for 33% of all studied cases (251 out of 750),\nwhile suggesting a bug reproducing test in first place for 149 bugs. To\nmitigate data contamination, we also evaluate LIBRO against 31 bug reports\nsubmitted after the collection of the LLM training data terminated: LIBRO\nproduces bug reproducing tests for 32% of the studied bug reports. Overall, our\nresults show LIBRO has the potential to significantly enhance developer\nefficiency by automatically generating tests from bug reports."}, "authors": [{"name": "Sungmin Kang"}, {"name": "Juyeon Yoon"}, {"name": "Shin Yoo"}], "author_detail": {"name": "Shin Yoo"}, "author": "Shin Yoo", "arxiv_comment": "Accepted to IEEE/ACM International Conference on Software Engineering\n  2023 (ICSE 2023)", "links": [{"href": "http://arxiv.org/abs/2209.11515v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2209.11515v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SE", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SE", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}