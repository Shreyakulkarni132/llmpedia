{"id": "http://arxiv.org/abs/2302.05206v1", "guidislink": true, "link": "http://arxiv.org/abs/2302.05206v1", "updated": "2023-02-10T12:16:38Z", "updated_parsed": [2023, 2, 10, 12, 16, 38, 4, 41, 0], "published": "2023-02-10T12:16:38Z", "published_parsed": [2023, 2, 10, 12, 16, 38, 4, 41, 0], "title": "The Wisdom of Hindsight Makes Language Models Better Instruction\n  Followers", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=the+wisdom+of+hindsight+makes+language+models+better+instruction+followers&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "The Wisdom of Hindsight Makes Language Models Better Instruction\n  Followers"}, "summary": "Reinforcement learning has seen wide success in finetuning large language\nmodels to better align with instructions via human feedback. The so-called\nalgorithm, Reinforcement Learning with Human Feedback (RLHF) demonstrates\nimpressive performance on the GPT series models. However, the underlying\nReinforcement Learning (RL) algorithm is complex and requires an additional\ntraining pipeline for reward and value networks. In this paper, we consider an\nalternative approach: converting feedback to instruction by relabeling the\noriginal one and training the model for better alignment in a supervised\nmanner. Such an algorithm doesn't require any additional parameters except for\nthe original language model and maximally reuses the pretraining pipeline. To\nachieve this, we formulate instruction alignment problem for language models as\na goal-reaching problem in decision making. We propose Hindsight Instruction\nRelabeling (HIR), a novel algorithm for aligning language models with\ninstructions. The resulting two-stage algorithm shed light to a family of\nreward-free approaches that utilize the hindsightly relabeled instructions\nbased on feedback. We evaluate the performance of HIR extensively on 12\nchallenging BigBench reasoning tasks and show that HIR outperforms the baseline\nalgorithms and is comparable to or even surpasses supervised finetuning.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=the+wisdom+of+hindsight+makes+language+models+better+instruction+followers&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Reinforcement learning has seen wide success in finetuning large language\nmodels to better align with instructions via human feedback. The so-called\nalgorithm, Reinforcement Learning with Human Feedback (RLHF) demonstrates\nimpressive performance on the GPT series models. However, the underlying\nReinforcement Learning (RL) algorithm is complex and requires an additional\ntraining pipeline for reward and value networks. In this paper, we consider an\nalternative approach: converting feedback to instruction by relabeling the\noriginal one and training the model for better alignment in a supervised\nmanner. Such an algorithm doesn't require any additional parameters except for\nthe original language model and maximally reuses the pretraining pipeline. To\nachieve this, we formulate instruction alignment problem for language models as\na goal-reaching problem in decision making. We propose Hindsight Instruction\nRelabeling (HIR), a novel algorithm for aligning language models with\ninstructions. The resulting two-stage algorithm shed light to a family of\nreward-free approaches that utilize the hindsightly relabeled instructions\nbased on feedback. We evaluate the performance of HIR extensively on 12\nchallenging BigBench reasoning tasks and show that HIR outperforms the baseline\nalgorithms and is comparable to or even surpasses supervised finetuning."}, "authors": [{"name": "Tianjun Zhang"}, {"name": "Fangchen Liu"}, {"name": "Justin Wong"}, {"name": "Pieter Abbeel"}, {"name": "Joseph E. Gonzalez"}], "author_detail": {"name": "Joseph E. Gonzalez"}, "author": "Joseph E. Gonzalez", "links": [{"href": "http://arxiv.org/abs/2302.05206v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2302.05206v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}