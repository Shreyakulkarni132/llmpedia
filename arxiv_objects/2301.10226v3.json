{"id": "http://arxiv.org/abs/2301.10226v3", "guidislink": true, "link": "http://arxiv.org/abs/2301.10226v3", "updated": "2023-06-06T17:50:01Z", "updated_parsed": [2023, 6, 6, 17, 50, 1, 1, 157, 0], "published": "2023-01-24T18:52:59Z", "published_parsed": [2023, 1, 24, 18, 52, 59, 1, 24, 0], "title": "A Watermark for Large Language Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=a+watermark+for+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "A Watermark for Large Language Models"}, "summary": "Potential harms of large language models can be mitigated by watermarking\nmodel output, i.e., embedding signals into generated text that are invisible to\nhumans but algorithmically detectable from a short span of tokens. We propose a\nwatermarking framework for proprietary language models. The watermark can be\nembedded with negligible impact on text quality, and can be detected using an\nefficient open-source algorithm without access to the language model API or\nparameters. The watermark works by selecting a randomized set of \"green\" tokens\nbefore a word is generated, and then softly promoting use of green tokens\nduring sampling. We propose a statistical test for detecting the watermark with\ninterpretable p-values, and derive an information-theoretic framework for\nanalyzing the sensitivity of the watermark. We test the watermark using a\nmulti-billion parameter model from the Open Pretrained Transformer (OPT)\nfamily, and discuss robustness and security.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=a+watermark+for+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Potential harms of large language models can be mitigated by watermarking\nmodel output, i.e., embedding signals into generated text that are invisible to\nhumans but algorithmically detectable from a short span of tokens. We propose a\nwatermarking framework for proprietary language models. The watermark can be\nembedded with negligible impact on text quality, and can be detected using an\nefficient open-source algorithm without access to the language model API or\nparameters. The watermark works by selecting a randomized set of \"green\" tokens\nbefore a word is generated, and then softly promoting use of green tokens\nduring sampling. We propose a statistical test for detecting the watermark with\ninterpretable p-values, and derive an information-theoretic framework for\nanalyzing the sensitivity of the watermark. We test the watermark using a\nmulti-billion parameter model from the Open Pretrained Transformer (OPT)\nfamily, and discuss robustness and security."}, "authors": [{"name": "John Kirchenbauer"}, {"name": "Jonas Geiping"}, {"name": "Yuxin Wen"}, {"name": "Jonathan Katz"}, {"name": "Ian Miers"}, {"name": "Tom Goldstein"}], "author_detail": {"name": "Tom Goldstein"}, "author": "Tom Goldstein", "arxiv_comment": "13 pages in the main body. Published at ICML 2023. Code is available\n  at github.com/jwkirchenbauer/lm-watermarking", "links": [{"href": "http://arxiv.org/abs/2301.10226v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2301.10226v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CR", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}