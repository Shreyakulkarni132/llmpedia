{"id": "http://arxiv.org/abs/2207.07051v1", "guidislink": true, "link": "http://arxiv.org/abs/2207.07051v1", "updated": "2022-07-14T16:51:09Z", "updated_parsed": [2022, 7, 14, 16, 51, 9, 3, 195, 0], "published": "2022-07-14T16:51:09Z", "published_parsed": [2022, 7, 14, 16, 51, 9, 3, 195, 0], "title": "Language models show human-like content effects on reasoning", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=language+models+show+human+like+content+effects+on+reasoning&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Language models show human-like content effects on reasoning"}, "summary": "Abstract reasoning is a key ability for an intelligent system. Large language\nmodels achieve above-chance performance on abstract reasoning tasks, but\nexhibit many imperfections. However, human abstract reasoning is also\nimperfect, and depends on our knowledge and beliefs about the content of the\nreasoning problem. For example, humans reason much more reliably about logical\nrules that are grounded in everyday situations than arbitrary rules about\nabstract attributes. The training experiences of language models similarly\nendow them with prior expectations that reflect human knowledge and beliefs. We\ntherefore hypothesized that language models would show human-like content\neffects on abstract reasoning problems. We explored this hypothesis across\nthree logical reasoning tasks: natural language inference, judging the logical\nvalidity of syllogisms, and the Wason selection task (Wason, 1968). We find\nthat state of the art large language models (with 7 or 70 billion parameters;\nHoffman et al., 2022) reflect many of the same patterns observed in humans\nacross these tasks -- like humans, models reason more effectively about\nbelievable situations than unrealistic or abstract ones. Our findings have\nimplications for understanding both these cognitive effects, and the factors\nthat contribute to language model performance.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=language+models+show+human+like+content+effects+on+reasoning&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Abstract reasoning is a key ability for an intelligent system. Large language\nmodels achieve above-chance performance on abstract reasoning tasks, but\nexhibit many imperfections. However, human abstract reasoning is also\nimperfect, and depends on our knowledge and beliefs about the content of the\nreasoning problem. For example, humans reason much more reliably about logical\nrules that are grounded in everyday situations than arbitrary rules about\nabstract attributes. The training experiences of language models similarly\nendow them with prior expectations that reflect human knowledge and beliefs. We\ntherefore hypothesized that language models would show human-like content\neffects on abstract reasoning problems. We explored this hypothesis across\nthree logical reasoning tasks: natural language inference, judging the logical\nvalidity of syllogisms, and the Wason selection task (Wason, 1968). We find\nthat state of the art large language models (with 7 or 70 billion parameters;\nHoffman et al., 2022) reflect many of the same patterns observed in humans\nacross these tasks -- like humans, models reason more effectively about\nbelievable situations than unrealistic or abstract ones. Our findings have\nimplications for understanding both these cognitive effects, and the factors\nthat contribute to language model performance."}, "authors": [{"name": "Ishita Dasgupta"}, {"name": "Andrew K. Lampinen"}, {"name": "Stephanie C. Y. Chan"}, {"name": "Antonia Creswell"}, {"name": "Dharshan Kumaran"}, {"name": "James L. McClelland"}, {"name": "Felix Hill"}], "author_detail": {"name": "Felix Hill"}, "author": "Felix Hill", "links": [{"href": "http://arxiv.org/abs/2207.07051v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2207.07051v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}