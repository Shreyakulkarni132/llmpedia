{"id": "http://arxiv.org/abs/2205.03767v3", "guidislink": true, "link": "http://arxiv.org/abs/2205.03767v3", "updated": "2022-05-11T02:25:35Z", "updated_parsed": [2022, 5, 11, 2, 25, 35, 2, 131, 0], "published": "2022-05-08T03:02:53Z", "published_parsed": [2022, 5, 8, 3, 2, 53, 6, 128, 0], "title": "Context-Aware Abbreviation Expansion Using Large Language Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=context+aware+abbreviation+expansion+using+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Context-Aware Abbreviation Expansion Using Large Language Models"}, "summary": "Motivated by the need for accelerating text entry in augmentative and\nalternative communication (AAC) for people with severe motor impairments, we\npropose a paradigm in which phrases are abbreviated aggressively as primarily\nword-initial letters. Our approach is to expand the abbreviations into\nfull-phrase options by leveraging conversation context with the power of\npretrained large language models (LLMs). Through zero-shot, few-shot, and\nfine-tuning experiments on four public conversation datasets, we show that for\nreplies to the initial turn of a dialog, an LLM with 64B parameters is able to\nexactly expand over 70% of phrases with abbreviation length up to 10, leading\nto an effective keystroke saving rate of up to about 77% on these exact\nexpansions. Including a small amount of context in the form of a single\nconversation turn more than doubles abbreviation expansion accuracies compared\nto having no context, an effect that is more pronounced for longer phrases.\nAdditionally, the robustness of models against typo noise can be enhanced\nthrough fine-tuning on noisy data.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=context+aware+abbreviation+expansion+using+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Motivated by the need for accelerating text entry in augmentative and\nalternative communication (AAC) for people with severe motor impairments, we\npropose a paradigm in which phrases are abbreviated aggressively as primarily\nword-initial letters. Our approach is to expand the abbreviations into\nfull-phrase options by leveraging conversation context with the power of\npretrained large language models (LLMs). Through zero-shot, few-shot, and\nfine-tuning experiments on four public conversation datasets, we show that for\nreplies to the initial turn of a dialog, an LLM with 64B parameters is able to\nexactly expand over 70% of phrases with abbreviation length up to 10, leading\nto an effective keystroke saving rate of up to about 77% on these exact\nexpansions. Including a small amount of context in the form of a single\nconversation turn more than doubles abbreviation expansion accuracies compared\nto having no context, an effect that is more pronounced for longer phrases.\nAdditionally, the robustness of models against typo noise can be enhanced\nthrough fine-tuning on noisy data."}, "authors": [{"name": "Shanqing Cai"}, {"name": "Subhashini Venugopalan"}, {"name": "Katrin Tomanek"}, {"name": "Ajit Narayanan"}, {"name": "Meredith Ringel Morris"}, {"name": "Michael P. Brenner"}], "author_detail": {"name": "Michael P. Brenner"}, "author": "Michael P. Brenner", "arxiv_comment": "15 pages, 7 figures, 8 tables. Accepted as a long paper at NAACL 2022", "links": [{"href": "http://arxiv.org/abs/2205.03767v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2205.03767v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}