{"id": "http://arxiv.org/abs/2307.00112v2", "guidislink": true, "link": "http://arxiv.org/abs/2307.00112v2", "updated": "2023-07-27T23:19:12Z", "updated_parsed": [2023, 7, 27, 23, 19, 12, 3, 208, 0], "published": "2023-06-30T19:53:23Z", "published_parsed": [2023, 6, 30, 19, 53, 23, 4, 181, 0], "title": "Performance of ChatGPT on USMLE: Unlocking the Potential of Large\n  Language Models for AI-Assisted Medical Education", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=performance+of+chatgpt+on+usmle++unlocking+the+potential+of+large+language+models+for+ai+assisted+medical+education&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Performance of ChatGPT on USMLE: Unlocking the Potential of Large\n  Language Models for AI-Assisted Medical Education"}, "summary": "Artificial intelligence is gaining traction in more ways than ever before.\nThe popularity of language models and AI-based businesses has soared since\nChatGPT was made available to the general public via OpenAI. It is becoming\nincreasingly common for people to use ChatGPT both professionally and\npersonally. Considering the widespread use of ChatGPT and the reliance people\nplace on it, this study determined how reliable ChatGPT can be for answering\ncomplex medical and clinical questions. Harvard University gross anatomy along\nwith the United States Medical Licensing Examination (USMLE) questionnaire were\nused to accomplish the objective. The paper evaluated the obtained results\nusing a 2-way ANOVA and posthoc analysis. Both showed systematic covariation\nbetween format and prompt. Furthermore, the physician adjudicators\nindependently rated the outcome's accuracy, concordance, and insight. As a\nresult of the analysis, ChatGPT-generated answers were found to be more\ncontext-oriented and represented a better model for deductive reasoning than\nregular Google search results. Furthermore, ChatGPT obtained 58.8% on logical\nquestions and 60% on ethical questions. This means that the ChatGPT is\napproaching the passing range for logical questions and has crossed the\nthreshold for ethical questions. The paper believes ChatGPT and other language\nlearning models can be invaluable tools for e-learners; however, the study\nsuggests that there is still room to improve their accuracy. In order to\nimprove ChatGPT's performance in the future, further research is needed to\nbetter understand how it can answer different types of questions.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=performance+of+chatgpt+on+usmle++unlocking+the+potential+of+large+language+models+for+ai+assisted+medical+education&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Artificial intelligence is gaining traction in more ways than ever before.\nThe popularity of language models and AI-based businesses has soared since\nChatGPT was made available to the general public via OpenAI. It is becoming\nincreasingly common for people to use ChatGPT both professionally and\npersonally. Considering the widespread use of ChatGPT and the reliance people\nplace on it, this study determined how reliable ChatGPT can be for answering\ncomplex medical and clinical questions. Harvard University gross anatomy along\nwith the United States Medical Licensing Examination (USMLE) questionnaire were\nused to accomplish the objective. The paper evaluated the obtained results\nusing a 2-way ANOVA and posthoc analysis. Both showed systematic covariation\nbetween format and prompt. Furthermore, the physician adjudicators\nindependently rated the outcome's accuracy, concordance, and insight. As a\nresult of the analysis, ChatGPT-generated answers were found to be more\ncontext-oriented and represented a better model for deductive reasoning than\nregular Google search results. Furthermore, ChatGPT obtained 58.8% on logical\nquestions and 60% on ethical questions. This means that the ChatGPT is\napproaching the passing range for logical questions and has crossed the\nthreshold for ethical questions. The paper believes ChatGPT and other language\nlearning models can be invaluable tools for e-learners; however, the study\nsuggests that there is still room to improve their accuracy. In order to\nimprove ChatGPT's performance in the future, further research is needed to\nbetter understand how it can answer different types of questions."}, "authors": [{"name": "Prabin Sharma"}, {"name": "Kisan Thapa"}, {"name": "Dikshya Thapa"}, {"name": "Prastab Dhakal"}, {"name": "Mala Deep Upadhaya"}, {"name": "Santosh Adhikari"}, {"name": "Salik Ram Khanal"}], "author_detail": {"name": "Salik Ram Khanal"}, "author": "Salik Ram Khanal", "arxiv_comment": "12 pages, 4 Figues, 4 tables", "links": [{"href": "http://arxiv.org/abs/2307.00112v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.00112v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CY", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CY", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}