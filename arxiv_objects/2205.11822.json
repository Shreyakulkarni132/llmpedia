{"id": "http://arxiv.org/abs/2205.11822v2", "guidislink": true, "link": "http://arxiv.org/abs/2205.11822v2", "updated": "2022-10-24T18:40:42Z", "updated_parsed": [2022, 10, 24, 18, 40, 42, 0, 297, 0], "published": "2022-05-24T06:36:42Z", "published_parsed": [2022, 5, 24, 6, 36, 42, 1, 144, 0], "title": "Maieutic Prompting: Logically Consistent Reasoning with Recursive\n  Explanations", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=maieutic+prompting++logically+consistent+reasoning+with+recursive+explanations&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Maieutic Prompting: Logically Consistent Reasoning with Recursive\n  Explanations"}, "summary": "Despite their impressive capabilities, large pre-trained language models\n(LMs) struggle with consistent reasoning; recently, prompting LMs to generate\nexplanations that self-guide the inference has emerged as a promising direction\nto amend this. However, these approaches are fundamentally bounded by the\ncorrectness of explanations, which themselves are often noisy and inconsistent.\nIn this work, we develop Maieutic Prompting, which infers a correct answer to a\nquestion even from the noisy and inconsistent generations of LM. Maieutic\nPrompting induces a tree of explanations abductively (e.g. X is true, because\n...) and recursively, then frames the inference as a satisfiability problem\nover these explanations and their logical relations. We test Maieutic Prompting\nfor true/false QA on three challenging benchmarks that require complex\ncommonsense reasoning. Maieutic Prompting achieves up to 20% better accuracy\nthan state-of-the-art prompting methods, and as a fully unsupervised approach,\nperforms competitively with supervised models. We also show that Maieutic\nPrompting improves robustness in inference while providing interpretable\nrationales.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=maieutic+prompting++logically+consistent+reasoning+with+recursive+explanations&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Despite their impressive capabilities, large pre-trained language models\n(LMs) struggle with consistent reasoning; recently, prompting LMs to generate\nexplanations that self-guide the inference has emerged as a promising direction\nto amend this. However, these approaches are fundamentally bounded by the\ncorrectness of explanations, which themselves are often noisy and inconsistent.\nIn this work, we develop Maieutic Prompting, which infers a correct answer to a\nquestion even from the noisy and inconsistent generations of LM. Maieutic\nPrompting induces a tree of explanations abductively (e.g. X is true, because\n...) and recursively, then frames the inference as a satisfiability problem\nover these explanations and their logical relations. We test Maieutic Prompting\nfor true/false QA on three challenging benchmarks that require complex\ncommonsense reasoning. Maieutic Prompting achieves up to 20% better accuracy\nthan state-of-the-art prompting methods, and as a fully unsupervised approach,\nperforms competitively with supervised models. We also show that Maieutic\nPrompting improves robustness in inference while providing interpretable\nrationales."}, "authors": [{"name": "Jaehun Jung"}, {"name": "Lianhui Qin"}, {"name": "Sean Welleck"}, {"name": "Faeze Brahman"}, {"name": "Chandra Bhagavatula"}, {"name": "Ronan Le Bras"}, {"name": "Yejin Choi"}], "author_detail": {"name": "Yejin Choi"}, "author": "Yejin Choi", "arxiv_comment": "EMNLP 2022", "links": [{"href": "http://arxiv.org/abs/2205.11822v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2205.11822v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}