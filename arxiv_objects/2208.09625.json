{"id": "http://arxiv.org/abs/2208.09625v2", "guidislink": true, "link": "http://arxiv.org/abs/2208.09625v2", "updated": "2022-10-24T00:49:00Z", "updated_parsed": [2022, 10, 24, 0, 49, 0, 0, 297, 0], "published": "2022-08-20T07:32:25Z", "published_parsed": [2022, 8, 20, 7, 32, 25, 5, 232, 0], "title": "SPOT: Knowledge-Enhanced Language Representations for Information\n  Extraction", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=spot++knowledge+enhanced+language+representations+for+information+extraction&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "SPOT: Knowledge-Enhanced Language Representations for Information\n  Extraction"}, "summary": "Knowledge-enhanced pre-trained models for language representation have been\nshown to be more effective in knowledge base construction tasks (i.e.,~relation\nextraction) than language models such as BERT. These knowledge-enhanced\nlanguage models incorporate knowledge into pre-training to generate\nrepresentations of entities or relationships. However, existing methods\ntypically represent each entity with a separate embedding. As a result, these\nmethods struggle to represent out-of-vocabulary entities and a large amount of\nparameters, on top of their underlying token models (i.e.,~the transformer),\nmust be used and the number of entities that can be handled is limited in\npractice due to memory constraints. Moreover, existing models still struggle to\nrepresent entities and relationships simultaneously. To address these problems,\nwe propose a new pre-trained model that learns representations of both entities\nand relationships from token spans and span pairs in the text respectively. By\nencoding spans efficiently with span modules, our model can represent both\nentities and their relationships but requires fewer parameters than existing\nmodels. We pre-trained our model with the knowledge graph extracted from\nWikipedia and test it on a broad range of supervised and unsupervised\ninformation extraction tasks. Results show that our model learns better\nrepresentations for both entities and relationships than baselines, while in\nsupervised settings, fine-tuning our model outperforms RoBERTa consistently and\nachieves competitive results on information extraction tasks.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=spot++knowledge+enhanced+language+representations+for+information+extraction&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Knowledge-enhanced pre-trained models for language representation have been\nshown to be more effective in knowledge base construction tasks (i.e.,~relation\nextraction) than language models such as BERT. These knowledge-enhanced\nlanguage models incorporate knowledge into pre-training to generate\nrepresentations of entities or relationships. However, existing methods\ntypically represent each entity with a separate embedding. As a result, these\nmethods struggle to represent out-of-vocabulary entities and a large amount of\nparameters, on top of their underlying token models (i.e.,~the transformer),\nmust be used and the number of entities that can be handled is limited in\npractice due to memory constraints. Moreover, existing models still struggle to\nrepresent entities and relationships simultaneously. To address these problems,\nwe propose a new pre-trained model that learns representations of both entities\nand relationships from token spans and span pairs in the text respectively. By\nencoding spans efficiently with span modules, our model can represent both\nentities and their relationships but requires fewer parameters than existing\nmodels. We pre-trained our model with the knowledge graph extracted from\nWikipedia and test it on a broad range of supervised and unsupervised\ninformation extraction tasks. Results show that our model learns better\nrepresentations for both entities and relationships than baselines, while in\nsupervised settings, fine-tuning our model outperforms RoBERTa consistently and\nachieves competitive results on information extraction tasks."}, "authors": [{"name": "Jiacheng Li"}, {"name": "Yannis Katsis"}, {"name": "Tyler Baldwin"}, {"name": "Ho-Cheol Kim"}, {"name": "Andrew Bartko"}, {"name": "Julian McAuley"}, {"name": "Chun-Nan Hsu"}], "author_detail": {"name": "Chun-Nan Hsu"}, "author": "Chun-Nan Hsu", "arxiv_comment": "CIKM 2022", "links": [{"href": "http://arxiv.org/abs/2208.09625v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2208.09625v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}