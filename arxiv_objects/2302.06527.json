{"id": "http://arxiv.org/abs/2302.06527v2", "guidislink": true, "link": "http://arxiv.org/abs/2302.06527v2", "updated": "2023-02-20T09:22:50Z", "updated_parsed": [2023, 2, 20, 9, 22, 50, 0, 51, 0], "published": "2023-02-13T17:13:41Z", "published_parsed": [2023, 2, 13, 17, 13, 41, 0, 44, 0], "title": "Adaptive Test Generation Using a Large Language Model", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=adaptive+test+generation+using+a+large+language+model&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Adaptive Test Generation Using a Large Language Model"}, "summary": "Unit tests play a key role in ensuring the correctness of software. However,\nmanually creating unit tests is a laborious task, motivating the need for\nautomation. This paper presents TestPilot, an adaptive test generation\ntechnique that leverages Large Language Models (LLMs). TestPilot uses Codex, an\noff-the-shelf LLM, to automatically generate unit tests for a given program\nwithout requiring additional training or few-shot learning on examples of\nexisting tests. In our approach, Codex is provided with prompts that include\nthe signature and implementation of a function under test, along with usage\nexamples extracted from documentation. If a generated test fails, TestPilot's\nadaptive component attempts to generate a new test that fixes the problem by\nre-prompting the model with the failing test and error message. We created an\nimplementation of TestPilot for JavaScript and evaluated it on 25 npm packages\nwith a total of 1,684 API functions to generate tests for. Our results show\nthat the generated tests achieve up to 93.1% statement coverage (median 68.2%).\nMoreover, on average, 58.5% of the generated tests contain at least one\nassertion that exercises functionality from the package under test. Our\nexperiments with excluding parts of the information included in the prompts\nshow that all components contribute towards the generation of effective test\nsuites. Finally, we find that TestPilot does not generate memorized tests:\n92.7% of our generated tests have $\\leq$ 50% similarity with existing tests (as\nmeasured by normalized edit distance), with none of them being exact copies.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=adaptive+test+generation+using+a+large+language+model&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Unit tests play a key role in ensuring the correctness of software. However,\nmanually creating unit tests is a laborious task, motivating the need for\nautomation. This paper presents TestPilot, an adaptive test generation\ntechnique that leverages Large Language Models (LLMs). TestPilot uses Codex, an\noff-the-shelf LLM, to automatically generate unit tests for a given program\nwithout requiring additional training or few-shot learning on examples of\nexisting tests. In our approach, Codex is provided with prompts that include\nthe signature and implementation of a function under test, along with usage\nexamples extracted from documentation. If a generated test fails, TestPilot's\nadaptive component attempts to generate a new test that fixes the problem by\nre-prompting the model with the failing test and error message. We created an\nimplementation of TestPilot for JavaScript and evaluated it on 25 npm packages\nwith a total of 1,684 API functions to generate tests for. Our results show\nthat the generated tests achieve up to 93.1% statement coverage (median 68.2%).\nMoreover, on average, 58.5% of the generated tests contain at least one\nassertion that exercises functionality from the package under test. Our\nexperiments with excluding parts of the information included in the prompts\nshow that all components contribute towards the generation of effective test\nsuites. Finally, we find that TestPilot does not generate memorized tests:\n92.7% of our generated tests have $\\leq$ 50% similarity with existing tests (as\nmeasured by normalized edit distance), with none of them being exact copies."}, "authors": [{"name": "Max Sch\u00e4fer"}, {"name": "Sarah Nadi"}, {"name": "Aryaz Eghbali"}, {"name": "Frank Tip"}], "author_detail": {"name": "Frank Tip"}, "author": "Frank Tip", "links": [{"href": "http://arxiv.org/abs/2302.06527v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2302.06527v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SE", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SE", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}