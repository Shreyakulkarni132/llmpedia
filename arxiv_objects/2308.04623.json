{"id": "http://arxiv.org/abs/2308.04623v1", "guidislink": true, "link": "http://arxiv.org/abs/2308.04623v1", "updated": "2023-08-08T23:29:55Z", "updated_parsed": [2023, 8, 8, 23, 29, 55, 1, 220, 0], "published": "2023-08-08T23:29:55Z", "published_parsed": [2023, 8, 8, 23, 29, 55, 1, 220, 0], "title": "Accelerating LLM Inference with Staged Speculative Decoding", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=accelerating+llm+inference+with+staged+speculative+decoding&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Accelerating LLM Inference with Staged Speculative Decoding"}, "summary": "Recent advances with large language models (LLM) illustrate their diverse\ncapabilities. We propose a novel algorithm, staged speculative decoding, to\naccelerate LLM inference in small-batch, on-device scenarios. We address the\nlow arithmetic intensity of small-batch inference by improving upon previous\nwork in speculative decoding. First, we restructure the speculative batch as a\ntree, which reduces generation costs and increases the expected tokens per\nbatch. Second, we add a second stage of speculative decoding. Taken together,\nwe reduce single-batch decoding latency by 3.16x with a 762M parameter GPT-2-L\nmodel while perfectly preserving output quality.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=accelerating+llm+inference+with+staged+speculative+decoding&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Recent advances with large language models (LLM) illustrate their diverse\ncapabilities. We propose a novel algorithm, staged speculative decoding, to\naccelerate LLM inference in small-batch, on-device scenarios. We address the\nlow arithmetic intensity of small-batch inference by improving upon previous\nwork in speculative decoding. First, we restructure the speculative batch as a\ntree, which reduces generation costs and increases the expected tokens per\nbatch. Second, we add a second stage of speculative decoding. Taken together,\nwe reduce single-batch decoding latency by 3.16x with a 762M parameter GPT-2-L\nmodel while perfectly preserving output quality."}, "authors": [{"name": "Benjamin Spector"}, {"name": "Chris Re"}], "author_detail": {"name": "Chris Re"}, "author": "Chris Re", "arxiv_comment": "Published at ES-FOMO at ICML 2023", "links": [{"href": "http://arxiv.org/abs/2308.04623v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2308.04623v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}