{"id": "http://arxiv.org/abs/2209.05735v3", "guidislink": true, "link": "http://arxiv.org/abs/2209.05735v3", "updated": "2023-03-12T04:46:00Z", "updated_parsed": [2023, 3, 12, 4, 46, 0, 6, 71, 0], "published": "2022-09-13T05:14:08Z", "published_parsed": [2022, 9, 13, 5, 14, 8, 1, 256, 0], "title": "Learning ASR pathways: A sparse multilingual ASR model", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=learning+asr+pathways++a+sparse+multilingual+asr+model&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Learning ASR pathways: A sparse multilingual ASR model"}, "summary": "Neural network pruning compresses automatic speech recognition (ASR) models\neffectively. However, in multilingual ASR, language-agnostic pruning may lead\nto severe performance drops on some languages because language-agnostic pruning\nmasks may not fit all languages and discard important language-specific\nparameters. In this work, we present ASR pathways, a sparse multilingual ASR\nmodel that activates language-specific sub-networks (\"pathways\"), such that the\nparameters for each language are learned explicitly. With the overlapping\nsub-networks, the shared parameters can also enable knowledge transfer for\nlower-resource languages via joint multilingual training. We propose a novel\nalgorithm to learn ASR pathways, and evaluate the proposed method on 4\nlanguages with a streaming RNN-T model. Our proposed ASR pathways outperform\nboth dense models and a language-agnostically pruned model, and provide better\nperformance on low-resource languages compared to the monolingual sparse\nmodels.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=learning+asr+pathways++a+sparse+multilingual+asr+model&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Neural network pruning compresses automatic speech recognition (ASR) models\neffectively. However, in multilingual ASR, language-agnostic pruning may lead\nto severe performance drops on some languages because language-agnostic pruning\nmasks may not fit all languages and discard important language-specific\nparameters. In this work, we present ASR pathways, a sparse multilingual ASR\nmodel that activates language-specific sub-networks (\"pathways\"), such that the\nparameters for each language are learned explicitly. With the overlapping\nsub-networks, the shared parameters can also enable knowledge transfer for\nlower-resource languages via joint multilingual training. We propose a novel\nalgorithm to learn ASR pathways, and evaluate the proposed method on 4\nlanguages with a streaming RNN-T model. Our proposed ASR pathways outperform\nboth dense models and a language-agnostically pruned model, and provide better\nperformance on low-resource languages compared to the monolingual sparse\nmodels."}, "authors": [{"name": "Mu Yang"}, {"name": "Andros Tjandra"}, {"name": "Chunxi Liu"}, {"name": "David Zhang"}, {"name": "Duc Le"}, {"name": "Ozlem Kalinli"}], "author_detail": {"name": "Ozlem Kalinli"}, "author": "Ozlem Kalinli", "arxiv_comment": "Accepted by ICASSP 2023", "links": [{"href": "http://arxiv.org/abs/2209.05735v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2209.05735v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "eess.AS", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "eess.AS", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}