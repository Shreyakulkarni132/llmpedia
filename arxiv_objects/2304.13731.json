{"id": "http://arxiv.org/abs/2304.13731v2", "guidislink": true, "link": "http://arxiv.org/abs/2304.13731v2", "updated": "2023-05-29T12:09:08Z", "updated_parsed": [2023, 5, 29, 12, 9, 8, 0, 149, 0], "published": "2023-04-24T07:45:28Z", "published_parsed": [2023, 4, 24, 7, 45, 28, 0, 114, 0], "title": "Text-to-Audio Generation using Instruction-Tuned LLM and Latent\n  Diffusion Model", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=text+to+audio+generation+using+instruction+tuned+llm+and+latent+diffusion+model&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Text-to-Audio Generation using Instruction-Tuned LLM and Latent\n  Diffusion Model"}, "summary": "The immense scale of the recent large language models (LLM) allows many\ninteresting properties, such as, instruction- and chain-of-thought-based\nfine-tuning, that has significantly improved zero- and few-shot performance in\nmany natural language processing (NLP) tasks. Inspired by such successes, we\nadopt such an instruction-tuned LLM Flan-T5 as the text encoder for\ntext-to-audio (TTA) generation -- a task where the goal is to generate an audio\nfrom its textual description. The prior works on TTA either pre-trained a joint\ntext-audio encoder or used a non-instruction-tuned model, such as, T5.\nConsequently, our latent diffusion model (LDM)-based approach TANGO outperforms\nthe state-of-the-art AudioLDM on most metrics and stays comparable on the rest\non AudioCaps test set, despite training the LDM on a 63 times smaller dataset\nand keeping the text encoder frozen. This improvement might also be attributed\nto the adoption of audio pressure level-based sound mixing for training set\naugmentation, whereas the prior methods take a random mix.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=text+to+audio+generation+using+instruction+tuned+llm+and+latent+diffusion+model&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "The immense scale of the recent large language models (LLM) allows many\ninteresting properties, such as, instruction- and chain-of-thought-based\nfine-tuning, that has significantly improved zero- and few-shot performance in\nmany natural language processing (NLP) tasks. Inspired by such successes, we\nadopt such an instruction-tuned LLM Flan-T5 as the text encoder for\ntext-to-audio (TTA) generation -- a task where the goal is to generate an audio\nfrom its textual description. The prior works on TTA either pre-trained a joint\ntext-audio encoder or used a non-instruction-tuned model, such as, T5.\nConsequently, our latent diffusion model (LDM)-based approach TANGO outperforms\nthe state-of-the-art AudioLDM on most metrics and stays comparable on the rest\non AudioCaps test set, despite training the LDM on a 63 times smaller dataset\nand keeping the text encoder frozen. This improvement might also be attributed\nto the adoption of audio pressure level-based sound mixing for training set\naugmentation, whereas the prior methods take a random mix."}, "authors": [{"name": "Deepanway Ghosal"}, {"name": "Navonil Majumder"}, {"name": "Ambuj Mehrish"}, {"name": "Soujanya Poria"}], "author_detail": {"name": "Soujanya Poria"}, "author": "Soujanya Poria", "arxiv_comment": "https://github.com/declare-lab/tango", "links": [{"href": "http://arxiv.org/abs/2304.13731v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2304.13731v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "eess.AS", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "eess.AS", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.SD", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}