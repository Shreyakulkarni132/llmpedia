{"id": "http://arxiv.org/abs/2306.11695v1", "guidislink": true, "link": "http://arxiv.org/abs/2306.11695v1", "updated": "2023-06-20T17:18:20Z", "updated_parsed": [2023, 6, 20, 17, 18, 20, 1, 171, 0], "published": "2023-06-20T17:18:20Z", "published_parsed": [2023, 6, 20, 17, 18, 20, 1, 171, 0], "title": "A Simple and Effective Pruning Approach for Large Language Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=a+simple+and+effective+pruning+approach+for+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "A Simple and Effective Pruning Approach for Large Language Models"}, "summary": "As their size increases, Large Languages Models (LLMs) are natural candidates\nfor network pruning methods: approaches that drop a subset of network weights\nwhile striving to preserve performance. Existing methods, however, require\neither retraining, which is rarely affordable for billion-scale LLMs, or\nsolving a weight reconstruction problem reliant on second-order information,\nwhich may also be computationally expensive. In this paper, we introduce a\nnovel, straightforward yet effective pruning method, termed Wanda (Pruning by\nWeights and activations), designed to induce sparsity in pretrained LLMs.\nMotivated by the recent observation of emergent large magnitude features in\nLLMs, our approach prune weights with the smallest magnitudes multiplied by the\ncorresponding input activations, on a per-output basis. Notably, Wanda requires\nno retraining or weight update, and the pruned LLM can be used as is. We\nconduct a thorough evaluation of our method on LLaMA across various language\nbenchmarks. Wanda significantly outperforms the established baseline of\nmagnitude pruning and competes favorably against recent methods involving\nintensive weight update. Code is available at\nhttps://github.com/locuslab/wanda.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=a+simple+and+effective+pruning+approach+for+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "As their size increases, Large Languages Models (LLMs) are natural candidates\nfor network pruning methods: approaches that drop a subset of network weights\nwhile striving to preserve performance. Existing methods, however, require\neither retraining, which is rarely affordable for billion-scale LLMs, or\nsolving a weight reconstruction problem reliant on second-order information,\nwhich may also be computationally expensive. In this paper, we introduce a\nnovel, straightforward yet effective pruning method, termed Wanda (Pruning by\nWeights and activations), designed to induce sparsity in pretrained LLMs.\nMotivated by the recent observation of emergent large magnitude features in\nLLMs, our approach prune weights with the smallest magnitudes multiplied by the\ncorresponding input activations, on a per-output basis. Notably, Wanda requires\nno retraining or weight update, and the pruned LLM can be used as is. We\nconduct a thorough evaluation of our method on LLaMA across various language\nbenchmarks. Wanda significantly outperforms the established baseline of\nmagnitude pruning and competes favorably against recent methods involving\nintensive weight update. Code is available at\nhttps://github.com/locuslab/wanda."}, "authors": [{"name": "Mingjie Sun"}, {"name": "Zhuang Liu"}, {"name": "Anna Bair"}, {"name": "J. Zico Kolter"}], "author_detail": {"name": "J. Zico Kolter"}, "author": "J. Zico Kolter", "arxiv_comment": "Technical Report", "links": [{"href": "http://arxiv.org/abs/2306.11695v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2306.11695v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}