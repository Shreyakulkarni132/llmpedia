{"id": "http://arxiv.org/abs/2306.15595v2", "guidislink": true, "link": "http://arxiv.org/abs/2306.15595v2", "updated": "2023-06-28T04:26:05Z", "updated_parsed": [2023, 6, 28, 4, 26, 5, 2, 179, 0], "published": "2023-06-27T16:26:26Z", "published_parsed": [2023, 6, 27, 16, 26, 26, 1, 178, 0], "title": "Extending Context Window of Large Language Models via Positional\n  Interpolation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=extending+context+window+of+large+language+models+via+positional+interpolation&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Extending Context Window of Large Language Models via Positional\n  Interpolation"}, "summary": "We present Position Interpolation (PI) that extends the context window sizes\nof RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal\nfine-tuning (within 1000 steps), while demonstrating strong empirical results\non various tasks that require long context, including passkey retrieval,\nlanguage modeling, and long document summarization from LLaMA 7B to 65B.\nMeanwhile, the extended model by Position Interpolation preserve quality\nrelatively well on tasks within its original context window. To achieve this\ngoal, Position Interpolation linearly down-scales the input position indices to\nmatch the original context window size, rather than extrapolating beyond the\ntrained context length which may lead to catastrophically high attention scores\nthat completely ruin the self-attention mechanism. Our theoretical study shows\nthat the upper bound of interpolation is at least $\\sim 600 \\times$ smaller\nthan that of extrapolation, further demonstrating its stability. Models\nextended via Position Interpolation retain its original architecture and can\nreuse most pre-existing optimization and infrastructure.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=extending+context+window+of+large+language+models+via+positional+interpolation&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "We present Position Interpolation (PI) that extends the context window sizes\nof RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal\nfine-tuning (within 1000 steps), while demonstrating strong empirical results\non various tasks that require long context, including passkey retrieval,\nlanguage modeling, and long document summarization from LLaMA 7B to 65B.\nMeanwhile, the extended model by Position Interpolation preserve quality\nrelatively well on tasks within its original context window. To achieve this\ngoal, Position Interpolation linearly down-scales the input position indices to\nmatch the original context window size, rather than extrapolating beyond the\ntrained context length which may lead to catastrophically high attention scores\nthat completely ruin the self-attention mechanism. Our theoretical study shows\nthat the upper bound of interpolation is at least $\\sim 600 \\times$ smaller\nthan that of extrapolation, further demonstrating its stability. Models\nextended via Position Interpolation retain its original architecture and can\nreuse most pre-existing optimization and infrastructure."}, "authors": [{"name": "Shouyuan Chen"}, {"name": "Sherman Wong"}, {"name": "Liangjian Chen"}, {"name": "Yuandong Tian"}], "author_detail": {"name": "Yuandong Tian"}, "author": "Yuandong Tian", "arxiv_comment": "Fix template issues", "links": [{"href": "http://arxiv.org/abs/2306.15595v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2306.15595v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}