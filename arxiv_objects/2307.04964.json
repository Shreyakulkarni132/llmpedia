{"id": "http://arxiv.org/abs/2307.04964v2", "guidislink": true, "link": "http://arxiv.org/abs/2307.04964v2", "updated": "2023-07-18T08:44:47Z", "updated_parsed": [2023, 7, 18, 8, 44, 47, 1, 199, 0], "published": "2023-07-11T01:55:24Z", "published_parsed": [2023, 7, 11, 1, 55, 24, 1, 192, 0], "title": "Secrets of RLHF in Large Language Models Part I: PPO", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=secrets+of+rlhf+in+large+language+models+part+i++ppo&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Secrets of RLHF in Large Language Models Part I: PPO"}, "summary": "Large language models (LLMs) have formulated a blueprint for the advancement\nof artificial general intelligence. Its primary objective is to function as a\nhuman-centric (helpful, honest, and harmless) assistant. Alignment with humans\nassumes paramount significance, and reinforcement learning with human feedback\n(RLHF) emerges as the pivotal technological paradigm underpinning this pursuit.\nCurrent technical routes usually include \\textbf{reward models} to measure\nhuman preferences, \\textbf{Proximal Policy Optimization} (PPO) to optimize\npolicy model outputs, and \\textbf{process supervision} to improve step-by-step\nreasoning capabilities. However, due to the challenges of reward design,\nenvironment interaction, and agent training, coupled with huge trial and error\ncost of large language models, there is a significant barrier for AI\nresearchers to motivate the development of technical alignment and safe landing\nof LLMs. The stable training of RLHF has still been a puzzle. In the first\nreport, we dissect the framework of RLHF, re-evaluate the inner workings of\nPPO, and explore how the parts comprising PPO algorithms impact policy agent\ntraining. We identify policy constraints being the key factor for the effective\nimplementation of the PPO algorithm. Therefore, we explore the PPO-max, an\nadvanced version of PPO algorithm, to efficiently improve the training\nstability of the policy model. Based on our main results, we perform a\ncomprehensive analysis of RLHF abilities compared with SFT models and ChatGPT.\nThe absence of open-source implementations has posed significant challenges to\nthe investigation of LLMs alignment. Therefore, we are eager to release\ntechnical reports, reward models and PPO codes, aiming to make modest\ncontributions to the advancement of LLMs.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=secrets+of+rlhf+in+large+language+models+part+i++ppo&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Large language models (LLMs) have formulated a blueprint for the advancement\nof artificial general intelligence. Its primary objective is to function as a\nhuman-centric (helpful, honest, and harmless) assistant. Alignment with humans\nassumes paramount significance, and reinforcement learning with human feedback\n(RLHF) emerges as the pivotal technological paradigm underpinning this pursuit.\nCurrent technical routes usually include \\textbf{reward models} to measure\nhuman preferences, \\textbf{Proximal Policy Optimization} (PPO) to optimize\npolicy model outputs, and \\textbf{process supervision} to improve step-by-step\nreasoning capabilities. However, due to the challenges of reward design,\nenvironment interaction, and agent training, coupled with huge trial and error\ncost of large language models, there is a significant barrier for AI\nresearchers to motivate the development of technical alignment and safe landing\nof LLMs. The stable training of RLHF has still been a puzzle. In the first\nreport, we dissect the framework of RLHF, re-evaluate the inner workings of\nPPO, and explore how the parts comprising PPO algorithms impact policy agent\ntraining. We identify policy constraints being the key factor for the effective\nimplementation of the PPO algorithm. Therefore, we explore the PPO-max, an\nadvanced version of PPO algorithm, to efficiently improve the training\nstability of the policy model. Based on our main results, we perform a\ncomprehensive analysis of RLHF abilities compared with SFT models and ChatGPT.\nThe absence of open-source implementations has posed significant challenges to\nthe investigation of LLMs alignment. Therefore, we are eager to release\ntechnical reports, reward models and PPO codes, aiming to make modest\ncontributions to the advancement of LLMs."}, "authors": [{"name": "Rui Zheng"}, {"name": "Shihan Dou"}, {"name": "Songyang Gao"}, {"name": "Yuan Hua"}, {"name": "Wei Shen"}, {"name": "Binghai Wang"}, {"name": "Yan Liu"}, {"name": "Senjie Jin"}, {"name": "Qin Liu"}, {"name": "Yuhao Zhou"}, {"name": "Limao Xiong"}, {"name": "Lu Chen"}, {"name": "Zhiheng Xi"}, {"name": "Nuo Xu"}, {"name": "Wenbin Lai"}, {"name": "Minghao Zhu"}, {"name": "Cheng Chang"}, {"name": "Zhangyue Yin"}, {"name": "Rongxiang Weng"}, {"name": "Wensen Cheng"}, {"name": "Haoran Huang"}, {"name": "Tianxiang Sun"}, {"name": "Hang Yan"}, {"name": "Tao Gui"}, {"name": "Qi Zhang"}, {"name": "Xipeng Qiu"}, {"name": "Xuanjing Huang"}], "author_detail": {"name": "Xuanjing Huang"}, "author": "Xuanjing Huang", "links": [{"href": "http://arxiv.org/abs/2307.04964v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.04964v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}