{"id": "http://arxiv.org/abs/2307.13269v1", "guidislink": true, "link": "http://arxiv.org/abs/2307.13269v1", "updated": "2023-07-25T05:39:21Z", "updated_parsed": [2023, 7, 25, 5, 39, 21, 1, 206, 0], "published": "2023-07-25T05:39:21Z", "published_parsed": [2023, 7, 25, 5, 39, 21, 1, 206, 0], "title": "LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA\n  Composition", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=lorahub++efficient+cross+task+generalization+via+dynamic+lora+composition&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA\n  Composition"}, "summary": "Low-rank adaptations (LoRA) are often employed to fine-tune large language\nmodels (LLMs) for new tasks. This paper investigates LoRA composability for\ncross-task generalization and introduces LoraHub, a strategic framework devised\nfor the purposive assembly of LoRA modules trained on diverse given tasks, with\nthe objective of achieving adaptable performance on unseen tasks. With just a\nfew examples from a novel task, LoraHub enables the fluid combination of\nmultiple LoRA modules, eradicating the need for human expertise. Notably, the\ncomposition requires neither additional model parameters nor gradients. Our\nempirical results, derived from the Big-Bench Hard (BBH) benchmark, suggest\nthat LoraHub can effectively mimic the performance of in-context learning in\nfew-shot scenarios, excluding the necessity of in-context examples alongside\neach inference input. A significant contribution of our research is the\nfostering of a community for LoRA, where users can share their trained LoRA\nmodules, thereby facilitating their application to new tasks. We anticipate\nthis resource will widen access to and spur advancements in general\nintelligence as well as LLMs in production. Code will be available at\nhttps://github.com/sail-sg/lorahub.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=lorahub++efficient+cross+task+generalization+via+dynamic+lora+composition&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Low-rank adaptations (LoRA) are often employed to fine-tune large language\nmodels (LLMs) for new tasks. This paper investigates LoRA composability for\ncross-task generalization and introduces LoraHub, a strategic framework devised\nfor the purposive assembly of LoRA modules trained on diverse given tasks, with\nthe objective of achieving adaptable performance on unseen tasks. With just a\nfew examples from a novel task, LoraHub enables the fluid combination of\nmultiple LoRA modules, eradicating the need for human expertise. Notably, the\ncomposition requires neither additional model parameters nor gradients. Our\nempirical results, derived from the Big-Bench Hard (BBH) benchmark, suggest\nthat LoraHub can effectively mimic the performance of in-context learning in\nfew-shot scenarios, excluding the necessity of in-context examples alongside\neach inference input. A significant contribution of our research is the\nfostering of a community for LoRA, where users can share their trained LoRA\nmodules, thereby facilitating their application to new tasks. We anticipate\nthis resource will widen access to and spur advancements in general\nintelligence as well as LLMs in production. Code will be available at\nhttps://github.com/sail-sg/lorahub."}, "authors": [{"name": "Chengsong Huang"}, {"name": "Qian Liu"}, {"name": "Bill Yuchen Lin"}, {"name": "Tianyu Pang"}, {"name": "Chao Du"}, {"name": "Min Lin"}], "author_detail": {"name": "Min Lin"}, "author": "Min Lin", "arxiv_comment": "Work in progress. The first three authors contributed equally to this\n  work", "links": [{"href": "http://arxiv.org/abs/2307.13269v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.13269v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}