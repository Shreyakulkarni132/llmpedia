{"id": "http://arxiv.org/abs/2306.08162v1", "guidislink": true, "link": "http://arxiv.org/abs/2306.08162v1", "updated": "2023-06-13T22:25:35Z", "updated_parsed": [2023, 6, 13, 22, 25, 35, 1, 164, 0], "published": "2023-06-13T22:25:35Z", "published_parsed": [2023, 6, 13, 22, 25, 35, 1, 164, 0], "title": "INT2.1: Towards Fine-Tunable Quantized Large Language Models with Error\n  Correction through Low-Rank Adaptation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=int2+1++towards+fine+tunable+quantized+large+language+models+with+error+correction+through+low+rank+adaptation&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "INT2.1: Towards Fine-Tunable Quantized Large Language Models with Error\n  Correction through Low-Rank Adaptation"}, "summary": "We introduce a method that dramatically reduces fine-tuning VRAM requirements\nand rectifies quantization errors in quantized Large Language Models. First, we\ndevelop an extremely memory-efficient fine-tuning (EMEF) method for quantized\nmodels using Low-Rank Adaptation (LoRA), and drawing upon it, we construct an\nerror-correcting algorithm designed to minimize errors induced by the\nquantization process. Our method reduces the memory requirements by up to 5.6\ntimes, which enables fine-tuning a 7 billion parameter Large Language Model\n(LLM) on consumer laptops. At the same time, we propose a Low-Rank Error\nCorrection (LREC) method that exploits the added LoRA layers to ameliorate the\ngap between the quantized model and its float point counterpart. Our error\ncorrection framework leads to a fully functional INT2 quantized LLM with the\ncapacity to generate coherent English text. To the best of our knowledge, this\nis the first INT2 Large Language Model that has been able to reach such a\nperformance. The overhead of our method is merely a 1.05 times increase in\nmodel size, which translates to an effective precision of INT2.1. Also, our\nmethod readily generalizes to other quantization standards, such as INT3, INT4,\nand INT8, restoring their lost performance, which marks a significant milestone\nin the field of model quantization. The strategies delineated in this paper\nhold promising implications for the future development and optimization of\nquantized models, marking a pivotal shift in the landscape of low-resource\nmachine learning computations.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=int2+1++towards+fine+tunable+quantized+large+language+models+with+error+correction+through+low+rank+adaptation&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "We introduce a method that dramatically reduces fine-tuning VRAM requirements\nand rectifies quantization errors in quantized Large Language Models. First, we\ndevelop an extremely memory-efficient fine-tuning (EMEF) method for quantized\nmodels using Low-Rank Adaptation (LoRA), and drawing upon it, we construct an\nerror-correcting algorithm designed to minimize errors induced by the\nquantization process. Our method reduces the memory requirements by up to 5.6\ntimes, which enables fine-tuning a 7 billion parameter Large Language Model\n(LLM) on consumer laptops. At the same time, we propose a Low-Rank Error\nCorrection (LREC) method that exploits the added LoRA layers to ameliorate the\ngap between the quantized model and its float point counterpart. Our error\ncorrection framework leads to a fully functional INT2 quantized LLM with the\ncapacity to generate coherent English text. To the best of our knowledge, this\nis the first INT2 Large Language Model that has been able to reach such a\nperformance. The overhead of our method is merely a 1.05 times increase in\nmodel size, which translates to an effective precision of INT2.1. Also, our\nmethod readily generalizes to other quantization standards, such as INT3, INT4,\nand INT8, restoring their lost performance, which marks a significant milestone\nin the field of model quantization. The strategies delineated in this paper\nhold promising implications for the future development and optimization of\nquantized models, marking a pivotal shift in the landscape of low-resource\nmachine learning computations."}, "authors": [{"name": "Yuji Chai"}, {"name": "John Gkountouras"}, {"name": "Glenn G. Ko"}, {"name": "David Brooks"}, {"name": "Gu-Yeon Wei"}], "author_detail": {"name": "Gu-Yeon Wei"}, "author": "Gu-Yeon Wei", "links": [{"href": "http://arxiv.org/abs/2306.08162v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2306.08162v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}