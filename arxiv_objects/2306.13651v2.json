{"id": "http://arxiv.org/abs/2306.13651v2", "guidislink": true, "link": "http://arxiv.org/abs/2306.13651v2", "updated": "2023-06-29T17:30:45Z", "updated_parsed": [2023, 6, 29, 17, 30, 45, 3, 180, 0], "published": "2023-06-23T17:59:09Z", "published_parsed": [2023, 6, 23, 17, 59, 9, 4, 174, 0], "title": "Bring Your Own Data! Self-Supervised Evaluation for Large Language\n  Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bring+your+own+data++self+supervised+evaluation+for+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Bring Your Own Data! Self-Supervised Evaluation for Large Language\n  Models"}, "summary": "With the rise of Large Language Models (LLMs) and their ubiquitous deployment\nin diverse domains, measuring language model behavior on realistic data is\nimperative. For example, a company deploying a client-facing chatbot must\nensure that the model will not respond to client requests with profanity.\nCurrent evaluations approach this problem using small, domain-specific datasets\nwith human-curated labels. These evaluation sets are often sampled from a\nnarrow and simplified distribution, and data sources can unknowingly be leaked\ninto the training set which can lead to misleading evaluations. To bypass these\ndrawbacks, we propose a framework for self-supervised evaluation of LLMs by\nanalyzing their sensitivity or invariance to transformations on the input text.\nSelf-supervised evaluation can directly monitor LLM behavior on datasets\ncollected in the wild or streamed during live model deployment. We demonstrate\nself-supervised evaluation strategies for measuring closed-book knowledge,\ntoxicity, and long-range context dependence, in addition to sensitivity to\ngrammatical structure and tokenization errors. When comparisons to similar\nhuman-labeled benchmarks are available, we find strong correlations between\nself-supervised and human-supervised evaluations. The self-supervised paradigm\ncomplements current evaluation strategies that rely on labeled data.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=bring+your+own+data++self+supervised+evaluation+for+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "With the rise of Large Language Models (LLMs) and their ubiquitous deployment\nin diverse domains, measuring language model behavior on realistic data is\nimperative. For example, a company deploying a client-facing chatbot must\nensure that the model will not respond to client requests with profanity.\nCurrent evaluations approach this problem using small, domain-specific datasets\nwith human-curated labels. These evaluation sets are often sampled from a\nnarrow and simplified distribution, and data sources can unknowingly be leaked\ninto the training set which can lead to misleading evaluations. To bypass these\ndrawbacks, we propose a framework for self-supervised evaluation of LLMs by\nanalyzing their sensitivity or invariance to transformations on the input text.\nSelf-supervised evaluation can directly monitor LLM behavior on datasets\ncollected in the wild or streamed during live model deployment. We demonstrate\nself-supervised evaluation strategies for measuring closed-book knowledge,\ntoxicity, and long-range context dependence, in addition to sensitivity to\ngrammatical structure and tokenization errors. When comparisons to similar\nhuman-labeled benchmarks are available, we find strong correlations between\nself-supervised and human-supervised evaluations. The self-supervised paradigm\ncomplements current evaluation strategies that rely on labeled data."}, "authors": [{"name": "Neel Jain"}, {"name": "Khalid Saifullah"}, {"name": "Yuxin Wen"}, {"name": "John Kirchenbauer"}, {"name": "Manli Shu"}, {"name": "Aniruddha Saha"}, {"name": "Micah Goldblum"}, {"name": "Jonas Geiping"}, {"name": "Tom Goldstein"}], "author_detail": {"name": "Tom Goldstein"}, "author": "Tom Goldstein", "arxiv_comment": "Code is available at https://github.com/neelsjain/BYOD. First two\n  authors contributed equally. 21 pages, 22 figures", "links": [{"href": "http://arxiv.org/abs/2306.13651v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2306.13651v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}