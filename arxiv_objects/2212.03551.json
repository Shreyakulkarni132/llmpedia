{"id": "http://arxiv.org/abs/2212.03551v5", "guidislink": true, "link": "http://arxiv.org/abs/2212.03551v5", "updated": "2023-02-16T14:44:31Z", "updated_parsed": [2023, 2, 16, 14, 44, 31, 3, 47, 0], "published": "2022-12-07T10:01:44Z", "published_parsed": [2022, 12, 7, 10, 1, 44, 2, 341, 0], "title": "Talking About Large Language Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=talking+about+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Talking About Large Language Models"}, "summary": "Thanks to rapid progress in artificial intelligence, we have entered an era\nwhen technology and philosophy intersect in interesting ways. Sitting squarely\nat the centre of this intersection are large language models (LLMs). The more\nadept LLMs become at mimicking human language, the more vulnerable we become to\nanthropomorphism, to seeing the systems in which they are embedded as more\nhuman-like than they really are. This trend is amplified by the natural\ntendency to use philosophically loaded terms, such as \"knows\", \"believes\", and\n\"thinks\", when describing these systems. To mitigate this trend, this paper\nadvocates the practice of repeatedly stepping back to remind ourselves of how\nLLMs, and the systems of which they form a part, actually work. The hope is\nthat increased scientific precision will encourage more philosophical nuance in\nthe discourse around artificial intelligence, both within the field and in the\npublic sphere.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=talking+about+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Thanks to rapid progress in artificial intelligence, we have entered an era\nwhen technology and philosophy intersect in interesting ways. Sitting squarely\nat the centre of this intersection are large language models (LLMs). The more\nadept LLMs become at mimicking human language, the more vulnerable we become to\nanthropomorphism, to seeing the systems in which they are embedded as more\nhuman-like than they really are. This trend is amplified by the natural\ntendency to use philosophically loaded terms, such as \"knows\", \"believes\", and\n\"thinks\", when describing these systems. To mitigate this trend, this paper\nadvocates the practice of repeatedly stepping back to remind ourselves of how\nLLMs, and the systems of which they form a part, actually work. The hope is\nthat increased scientific precision will encourage more philosophical nuance in\nthe discourse around artificial intelligence, both within the field and in the\npublic sphere."}, "authors": [{"name": "Murray Shanahan"}], "author_detail": {"name": "Murray Shanahan"}, "author": "Murray Shanahan", "links": [{"href": "http://arxiv.org/abs/2212.03551v5", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2212.03551v5", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}