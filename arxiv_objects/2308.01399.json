{"id": "http://arxiv.org/abs/2308.01399v1", "guidislink": true, "link": "http://arxiv.org/abs/2308.01399v1", "updated": "2023-07-31T17:57:49Z", "updated_parsed": [2023, 7, 31, 17, 57, 49, 0, 212, 0], "published": "2023-07-31T17:57:49Z", "published_parsed": [2023, 7, 31, 17, 57, 49, 0, 212, 0], "title": "Learning to Model the World with Language", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=learning+to+model+the+world+with+language&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Learning to Model the World with Language"}, "summary": "To interact with humans in the world, agents need to understand the diverse\ntypes of language that people use, relate them to the visual world, and act\nbased on them. While current agents learn to execute simple language\ninstructions from task rewards, we aim to build agents that leverage diverse\nlanguage that conveys general knowledge, describes the state of the world,\nprovides interactive feedback, and more. Our key idea is that language helps\nagents predict the future: what will be observed, how the world will behave,\nand which situations will be rewarded. This perspective unifies language\nunderstanding with future prediction as a powerful self-supervised learning\nobjective. We present Dynalang, an agent that learns a multimodal world model\nthat predicts future text and image representations and learns to act from\nimagined model rollouts. Unlike traditional agents that use language only to\npredict actions, Dynalang acquires rich language understanding by using past\nlanguage also to predict future language, video, and rewards. In addition to\nlearning from online interaction in an environment, Dynalang can be pretrained\non datasets of text, video, or both without actions or rewards. From using\nlanguage hints in grid worlds to navigating photorealistic scans of homes,\nDynalang utilizes diverse types of language to improve task performance,\nincluding environment descriptions, game rules, and instructions.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=learning+to+model+the+world+with+language&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "To interact with humans in the world, agents need to understand the diverse\ntypes of language that people use, relate them to the visual world, and act\nbased on them. While current agents learn to execute simple language\ninstructions from task rewards, we aim to build agents that leverage diverse\nlanguage that conveys general knowledge, describes the state of the world,\nprovides interactive feedback, and more. Our key idea is that language helps\nagents predict the future: what will be observed, how the world will behave,\nand which situations will be rewarded. This perspective unifies language\nunderstanding with future prediction as a powerful self-supervised learning\nobjective. We present Dynalang, an agent that learns a multimodal world model\nthat predicts future text and image representations and learns to act from\nimagined model rollouts. Unlike traditional agents that use language only to\npredict actions, Dynalang acquires rich language understanding by using past\nlanguage also to predict future language, video, and rewards. In addition to\nlearning from online interaction in an environment, Dynalang can be pretrained\non datasets of text, video, or both without actions or rewards. From using\nlanguage hints in grid worlds to navigating photorealistic scans of homes,\nDynalang utilizes diverse types of language to improve task performance,\nincluding environment descriptions, game rules, and instructions."}, "authors": [{"name": "Jessy Lin"}, {"name": "Yuqing Du"}, {"name": "Olivia Watkins"}, {"name": "Danijar Hafner"}, {"name": "Pieter Abbeel"}, {"name": "Dan Klein"}, {"name": "Anca Dragan"}], "author_detail": {"name": "Anca Dragan"}, "author": "Anca Dragan", "arxiv_comment": "Website: https://dynalang.github.io/", "links": [{"href": "http://arxiv.org/abs/2308.01399v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2308.01399v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}