{"id": "http://arxiv.org/abs/2307.03381v1", "guidislink": true, "link": "http://arxiv.org/abs/2307.03381v1", "updated": "2023-07-07T04:33:31Z", "updated_parsed": [2023, 7, 7, 4, 33, 31, 4, 188, 0], "published": "2023-07-07T04:33:31Z", "published_parsed": [2023, 7, 7, 4, 33, 31, 4, 188, 0], "title": "Teaching Arithmetic to Small Transformers", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=teaching+arithmetic+to+small+transformers&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Teaching Arithmetic to Small Transformers"}, "summary": "Large language models like GPT-4 exhibit emergent capabilities across\ngeneral-purpose tasks, such as basic arithmetic, when trained on extensive text\ndata, even though these tasks are not explicitly encoded by the unsupervised,\nnext-token prediction objective. This study investigates how small\ntransformers, trained from random initialization, can efficiently learn\narithmetic operations such as addition, multiplication, and elementary\nfunctions like square root, using the next-token prediction objective. We first\ndemonstrate that conventional training data is not the most effective for\narithmetic learning, and simple formatting changes can significantly improve\naccuracy. This leads to sharp phase transitions as a function of training data\nscale, which, in some cases, can be explained through connections to low-rank\nmatrix completion. Building on prior work, we then train on chain-of-thought\nstyle data that includes intermediate step results. Even in the complete\nabsence of pretraining, this approach significantly and simultaneously improves\naccuracy, sample complexity, and convergence speed. We also study the interplay\nbetween arithmetic and text data during training and examine the effects of\nfew-shot prompting, pretraining, and model scale. Additionally, we discuss\nlength generalization challenges. Our work highlights the importance of\nhigh-quality, instructive data that considers the particular characteristics of\nthe next-word prediction objective for rapidly eliciting arithmetic\ncapabilities.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=teaching+arithmetic+to+small+transformers&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Large language models like GPT-4 exhibit emergent capabilities across\ngeneral-purpose tasks, such as basic arithmetic, when trained on extensive text\ndata, even though these tasks are not explicitly encoded by the unsupervised,\nnext-token prediction objective. This study investigates how small\ntransformers, trained from random initialization, can efficiently learn\narithmetic operations such as addition, multiplication, and elementary\nfunctions like square root, using the next-token prediction objective. We first\ndemonstrate that conventional training data is not the most effective for\narithmetic learning, and simple formatting changes can significantly improve\naccuracy. This leads to sharp phase transitions as a function of training data\nscale, which, in some cases, can be explained through connections to low-rank\nmatrix completion. Building on prior work, we then train on chain-of-thought\nstyle data that includes intermediate step results. Even in the complete\nabsence of pretraining, this approach significantly and simultaneously improves\naccuracy, sample complexity, and convergence speed. We also study the interplay\nbetween arithmetic and text data during training and examine the effects of\nfew-shot prompting, pretraining, and model scale. Additionally, we discuss\nlength generalization challenges. Our work highlights the importance of\nhigh-quality, instructive data that considers the particular characteristics of\nthe next-word prediction objective for rapidly eliciting arithmetic\ncapabilities."}, "authors": [{"name": "Nayoung Lee"}, {"name": "Kartik Sreenivasan"}, {"name": "Jason D. Lee"}, {"name": "Kangwook Lee"}, {"name": "Dimitris Papailiopoulos"}], "author_detail": {"name": "Dimitris Papailiopoulos"}, "author": "Dimitris Papailiopoulos", "links": [{"href": "http://arxiv.org/abs/2307.03381v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.03381v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}