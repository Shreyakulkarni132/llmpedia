{"id": "http://arxiv.org/abs/2305.01210v2", "guidislink": true, "link": "http://arxiv.org/abs/2305.01210v2", "updated": "2023-06-12T06:49:51Z", "updated_parsed": [2023, 6, 12, 6, 49, 51, 0, 163, 0], "published": "2023-05-02T05:46:48Z", "published_parsed": [2023, 5, 2, 5, 46, 48, 1, 122, 0], "title": "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of\n  Large Language Models for Code Generation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=is+your+code+generated+by+chatgpt+really+correct++rigorous+evaluation+of+large+language+models+for+code+generation&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of\n  Large Language Models for Code Generation"}, "summary": "Program synthesis has been long studied with recent approaches focused on\ndirectly using the power of Large Language Models (LLMs) to generate code.\nProgramming benchmarks, with curated synthesis problems and test-cases, are\nused to measure the performance of various LLMs on code synthesis. However,\nthese test-cases can be limited in both quantity and quality for fully\nassessing the functional correctness of the generated code. Such limitation in\nthe existing benchmarks begs the following question: In the era of LLMs, is the\ncode generated really correct? To answer this, we propose EvalPlus -- a code\nsynthesis benchmarking framework to rigorously evaluate the functional\ncorrectness of LLM-synthesized code. EvalPlus augments a given evaluation\ndataset with large amounts of test-cases newly produced by an automatic test\ninput generator, powered by both LLM- and mutation-based strategies. While\nEvalPlus is general, we extend the test-cases of the popular HUMANEVAL\nbenchmark by 81x to build HUMANEVAL+. Our extensive evaluation across 19\npopular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HUMANEVAL+ is able to\ncatch significant amounts of previously undetected wrong code synthesized by\nLLMs, reducing the pass@k by 13.6-15.3% on average. Our work not only indicates\nthat prior popular code synthesis evaluation results do not accurately reflect\nthe true performance of LLMs for code synthesis, but also opens up a new\ndirection to improve such programming benchmarks through automated testing.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=is+your+code+generated+by+chatgpt+really+correct++rigorous+evaluation+of+large+language+models+for+code+generation&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Program synthesis has been long studied with recent approaches focused on\ndirectly using the power of Large Language Models (LLMs) to generate code.\nProgramming benchmarks, with curated synthesis problems and test-cases, are\nused to measure the performance of various LLMs on code synthesis. However,\nthese test-cases can be limited in both quantity and quality for fully\nassessing the functional correctness of the generated code. Such limitation in\nthe existing benchmarks begs the following question: In the era of LLMs, is the\ncode generated really correct? To answer this, we propose EvalPlus -- a code\nsynthesis benchmarking framework to rigorously evaluate the functional\ncorrectness of LLM-synthesized code. EvalPlus augments a given evaluation\ndataset with large amounts of test-cases newly produced by an automatic test\ninput generator, powered by both LLM- and mutation-based strategies. While\nEvalPlus is general, we extend the test-cases of the popular HUMANEVAL\nbenchmark by 81x to build HUMANEVAL+. Our extensive evaluation across 19\npopular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HUMANEVAL+ is able to\ncatch significant amounts of previously undetected wrong code synthesized by\nLLMs, reducing the pass@k by 13.6-15.3% on average. Our work not only indicates\nthat prior popular code synthesis evaluation results do not accurately reflect\nthe true performance of LLMs for code synthesis, but also opens up a new\ndirection to improve such programming benchmarks through automated testing."}, "authors": [{"name": "Jiawei Liu"}, {"name": "Chunqiu Steven Xia"}, {"name": "Yuyao Wang"}, {"name": "Lingming Zhang"}], "author_detail": {"name": "Lingming Zhang"}, "author": "Lingming Zhang", "links": [{"href": "http://arxiv.org/abs/2305.01210v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2305.01210v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SE", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SE", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}