{"id": "http://arxiv.org/abs/2212.08073v1", "guidislink": true, "link": "http://arxiv.org/abs/2212.08073v1", "updated": "2022-12-15T06:19:23Z", "updated_parsed": [2022, 12, 15, 6, 19, 23, 3, 349, 0], "published": "2022-12-15T06:19:23Z", "published_parsed": [2022, 12, 15, 6, 19, 23, 3, 349, 0], "title": "Constitutional AI: Harmlessness from AI Feedback", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=constitutional+ai++harmlessness+from+ai+feedback&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Constitutional AI: Harmlessness from AI Feedback"}, "summary": "As AI systems become more capable, we would like to enlist their help to\nsupervise other AIs. We experiment with methods for training a harmless AI\nassistant through self-improvement, without any human labels identifying\nharmful outputs. The only human oversight is provided through a list of rules\nor principles, and so we refer to the method as 'Constitutional AI'. The\nprocess involves both a supervised learning and a reinforcement learning phase.\nIn the supervised phase we sample from an initial model, then generate\nself-critiques and revisions, and then finetune the original model on revised\nresponses. In the RL phase, we sample from the finetuned model, use a model to\nevaluate which of the two samples is better, and then train a preference model\nfrom this dataset of AI preferences. We then train with RL using the preference\nmodel as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a\nresult we are able to train a harmless but non-evasive AI assistant that\nengages with harmful queries by explaining its objections to them. Both the SL\nand RL methods can leverage chain-of-thought style reasoning to improve the\nhuman-judged performance and transparency of AI decision making. These methods\nmake it possible to control AI behavior more precisely and with far fewer human\nlabels.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=constitutional+ai++harmlessness+from+ai+feedback&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "As AI systems become more capable, we would like to enlist their help to\nsupervise other AIs. We experiment with methods for training a harmless AI\nassistant through self-improvement, without any human labels identifying\nharmful outputs. The only human oversight is provided through a list of rules\nor principles, and so we refer to the method as 'Constitutional AI'. The\nprocess involves both a supervised learning and a reinforcement learning phase.\nIn the supervised phase we sample from an initial model, then generate\nself-critiques and revisions, and then finetune the original model on revised\nresponses. In the RL phase, we sample from the finetuned model, use a model to\nevaluate which of the two samples is better, and then train a preference model\nfrom this dataset of AI preferences. We then train with RL using the preference\nmodel as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a\nresult we are able to train a harmless but non-evasive AI assistant that\nengages with harmful queries by explaining its objections to them. Both the SL\nand RL methods can leverage chain-of-thought style reasoning to improve the\nhuman-judged performance and transparency of AI decision making. These methods\nmake it possible to control AI behavior more precisely and with far fewer human\nlabels."}, "authors": [{"name": "Yuntao Bai"}, {"name": "Saurav Kadavath"}, {"name": "Sandipan Kundu"}, {"name": "Amanda Askell"}, {"name": "Jackson Kernion"}, {"name": "Andy Jones"}, {"name": "Anna Chen"}, {"name": "Anna Goldie"}, {"name": "Azalia Mirhoseini"}, {"name": "Cameron McKinnon"}, {"name": "Carol Chen"}, {"name": "Catherine Olsson"}, {"name": "Christopher Olah"}, {"name": "Danny Hernandez"}, {"name": "Dawn Drain"}, {"name": "Deep Ganguli"}, {"name": "Dustin Li"}, {"name": "Eli Tran-Johnson"}, {"name": "Ethan Perez"}, {"name": "Jamie Kerr"}, {"name": "Jared Mueller"}, {"name": "Jeffrey Ladish"}, {"name": "Joshua Landau"}, {"name": "Kamal Ndousse"}, {"name": "Kamile Lukosuite"}, {"name": "Liane Lovitt"}, {"name": "Michael Sellitto"}, {"name": "Nelson Elhage"}, {"name": "Nicholas Schiefer"}, {"name": "Noemi Mercado"}, {"name": "Nova DasSarma"}, {"name": "Robert Lasenby"}, {"name": "Robin Larson"}, {"name": "Sam Ringer"}, {"name": "Scott Johnston"}, {"name": "Shauna Kravec"}, {"name": "Sheer El Showk"}, {"name": "Stanislav Fort"}, {"name": "Tamera Lanham"}, {"name": "Timothy Telleen-Lawton"}, {"name": "Tom Conerly"}, {"name": "Tom Henighan"}, {"name": "Tristan Hume"}, {"name": "Samuel R. Bowman"}, {"name": "Zac Hatfield-Dodds"}, {"name": "Ben Mann"}, {"name": "Dario Amodei"}, {"name": "Nicholas Joseph"}, {"name": "Sam McCandlish"}, {"name": "Tom Brown"}, {"name": "Jared Kaplan"}], "author_detail": {"name": "Jared Kaplan"}, "author": "Jared Kaplan", "links": [{"href": "http://arxiv.org/abs/2212.08073v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2212.08073v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}