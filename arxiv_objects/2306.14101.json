{"id": "http://arxiv.org/abs/2306.14101v1", "guidislink": true, "link": "http://arxiv.org/abs/2306.14101v1", "updated": "2023-06-25T02:39:19Z", "updated_parsed": [2023, 6, 25, 2, 39, 19, 6, 176, 0], "published": "2023-06-25T02:39:19Z", "published_parsed": [2023, 6, 25, 2, 39, 19, 6, 176, 0], "title": "Language models are weak learners", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=language+models+are+weak+learners&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Language models are weak learners"}, "summary": "A central notion in practical and theoretical machine learning is that of a\n$\\textit{weak learner}$, classifiers that achieve better-than-random\nperformance (on any given distribution over data), even by a small margin. Such\nweak learners form the practical basis for canonical machine learning methods\nsuch as boosting. In this work, we illustrate that prompt-based large language\nmodels can operate effectively as said weak learners. Specifically, we\nillustrate the use of a large language model (LLM) as a weak learner in a\nboosting algorithm applied to tabular data. We show that by providing (properly\nsampled according to the distribution of interest) text descriptions of tabular\ndata samples, LLMs can produce a summary of the samples that serves as a\ntemplate for classification and achieves the aim of acting as a weak learner on\nthis task. We incorporate these models into a boosting approach, which in some\nsettings can leverage the knowledge within the LLM to outperform traditional\ntree-based boosting. The model outperforms both few-shot learning and\noccasionally even more involved fine-tuning procedures, particularly for tasks\ninvolving small numbers of data points. The results illustrate the potential\nfor prompt-based LLMs to function not just as few-shot learners themselves, but\nas components of larger machine learning pipelines.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=language+models+are+weak+learners&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "A central notion in practical and theoretical machine learning is that of a\n$\\textit{weak learner}$, classifiers that achieve better-than-random\nperformance (on any given distribution over data), even by a small margin. Such\nweak learners form the practical basis for canonical machine learning methods\nsuch as boosting. In this work, we illustrate that prompt-based large language\nmodels can operate effectively as said weak learners. Specifically, we\nillustrate the use of a large language model (LLM) as a weak learner in a\nboosting algorithm applied to tabular data. We show that by providing (properly\nsampled according to the distribution of interest) text descriptions of tabular\ndata samples, LLMs can produce a summary of the samples that serves as a\ntemplate for classification and achieves the aim of acting as a weak learner on\nthis task. We incorporate these models into a boosting approach, which in some\nsettings can leverage the knowledge within the LLM to outperform traditional\ntree-based boosting. The model outperforms both few-shot learning and\noccasionally even more involved fine-tuning procedures, particularly for tasks\ninvolving small numbers of data points. The results illustrate the potential\nfor prompt-based LLMs to function not just as few-shot learners themselves, but\nas components of larger machine learning pipelines."}, "authors": [{"name": "Hariharan Manikandan"}, {"name": "Yiding Jiang"}, {"name": "J Zico Kolter"}], "author_detail": {"name": "J Zico Kolter"}, "author": "J Zico Kolter", "arxiv_comment": "23 pages, 6 figures", "links": [{"href": "http://arxiv.org/abs/2306.14101v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2306.14101v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}