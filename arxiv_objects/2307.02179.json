{"id": "http://arxiv.org/abs/2307.02179v1", "guidislink": true, "link": "http://arxiv.org/abs/2307.02179v1", "updated": "2023-07-05T10:15:07Z", "updated_parsed": [2023, 7, 5, 10, 15, 7, 2, 186, 0], "published": "2023-07-05T10:15:07Z", "published_parsed": [2023, 7, 5, 10, 15, 7, 2, 186, 0], "title": "Open-Source Large Language Models Outperform Crowd Workers and Approach\n  ChatGPT in Text-Annotation Tasks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=open+source+large+language+models+outperform+crowd+workers+and+approach+chatgpt+in+text+annotation+tasks&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "Open-Source Large Language Models Outperform Crowd Workers and Approach\n  ChatGPT in Text-Annotation Tasks"}, "summary": "This study examines the performance of open-source Large Language Models\n(LLMs) in text annotation tasks and compares it with proprietary models like\nChatGPT and human-based services such as MTurk. While prior research\ndemonstrated the high performance of ChatGPT across numerous NLP tasks,\nopen-source LLMs like HugginChat and FLAN are gaining attention for their\ncost-effectiveness, transparency, reproducibility, and superior data\nprotection. We assess these models using both zero-shot and few-shot approaches\nand different temperature parameters across a range of text annotation tasks.\nOur findings show that while ChatGPT achieves the best performance in most\ntasks, open-source LLMs not only outperform MTurk but also demonstrate\ncompetitive potential against ChatGPT in specific tasks.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=open+source+large+language+models+outperform+crowd+workers+and+approach+chatgpt+in+text+annotation+tasks&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=20", "value": "This study examines the performance of open-source Large Language Models\n(LLMs) in text annotation tasks and compares it with proprietary models like\nChatGPT and human-based services such as MTurk. While prior research\ndemonstrated the high performance of ChatGPT across numerous NLP tasks,\nopen-source LLMs like HugginChat and FLAN are gaining attention for their\ncost-effectiveness, transparency, reproducibility, and superior data\nprotection. We assess these models using both zero-shot and few-shot approaches\nand different temperature parameters across a range of text annotation tasks.\nOur findings show that while ChatGPT achieves the best performance in most\ntasks, open-source LLMs not only outperform MTurk but also demonstrate\ncompetitive potential against ChatGPT in specific tasks."}, "authors": [{"name": "Meysam Alizadeh"}, {"name": "Ma\u00ebl Kubli"}, {"name": "Zeynab Samei"}, {"name": "Shirin Dehghani"}, {"name": "Juan Diego Bermeo"}, {"name": "Maria Korobeynikova"}, {"name": "Fabrizio Gilardi"}], "author_detail": {"name": "Fabrizio Gilardi"}, "author": "Fabrizio Gilardi", "links": [{"href": "http://arxiv.org/abs/2307.02179v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.02179v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}