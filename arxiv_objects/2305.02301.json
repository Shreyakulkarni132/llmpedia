{"id": "http://arxiv.org/abs/2305.02301v2", "guidislink": true, "link": "http://arxiv.org/abs/2305.02301v2", "updated": "2023-07-05T16:59:31Z", "updated_parsed": [2023, 7, 5, 16, 59, 31, 2, 186, 0], "published": "2023-05-03T17:50:56Z", "published_parsed": [2023, 5, 3, 17, 50, 56, 2, 123, 0], "title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less\n  Training Data and Smaller Model Sizes", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=distilling+step+by+step++outperforming+larger+language+models+with+less+training+data+and+smaller+model+sizes&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Distilling Step-by-Step! Outperforming Larger Language Models with Less\n  Training Data and Smaller Model Sizes"}, "summary": "Deploying large language models (LLMs) is challenging because they are memory\ninefficient and compute-intensive for practical applications. In reaction,\nresearchers train smaller task-specific models by either finetuning with human\nlabels or distilling using LLM-generated labels. However, finetuning and\ndistillation require large amounts of training data to achieve comparable\nperformance to LLMs. We introduce Distilling step-by-step, a new mechanism that\n(a) trains smaller models that outperform LLMs, and (b) achieves so by\nleveraging less training data needed by finetuning or distillation. Our method\nextracts LLM rationales as additional supervision for training small models\nwithin a multi-task framework. We present three findings across 4 NLP\nbenchmarks: First, compared to both finetuning and distillation, our mechanism\nachieves better performance with much fewer labeled/unlabeled training\nexamples. Second, compared to few-shot prompted LLMs, we achieve better\nperformance using substantially smaller model sizes. Third, we reduce both the\nmodel size and the amount of data required to outperform LLMs; our finetuned\n770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80%\nof available data on a benchmark, whereas standard finetuning the same T5 model\nstruggles to match even by using 100% of the dataset. We release the code at:\nhttps://github.com/google-research/distilling-step-by-step .", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=distilling+step+by+step++outperforming+larger+language+models+with+less+training+data+and+smaller+model+sizes&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Deploying large language models (LLMs) is challenging because they are memory\ninefficient and compute-intensive for practical applications. In reaction,\nresearchers train smaller task-specific models by either finetuning with human\nlabels or distilling using LLM-generated labels. However, finetuning and\ndistillation require large amounts of training data to achieve comparable\nperformance to LLMs. We introduce Distilling step-by-step, a new mechanism that\n(a) trains smaller models that outperform LLMs, and (b) achieves so by\nleveraging less training data needed by finetuning or distillation. Our method\nextracts LLM rationales as additional supervision for training small models\nwithin a multi-task framework. We present three findings across 4 NLP\nbenchmarks: First, compared to both finetuning and distillation, our mechanism\nachieves better performance with much fewer labeled/unlabeled training\nexamples. Second, compared to few-shot prompted LLMs, we achieve better\nperformance using substantially smaller model sizes. Third, we reduce both the\nmodel size and the amount of data required to outperform LLMs; our finetuned\n770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80%\nof available data on a benchmark, whereas standard finetuning the same T5 model\nstruggles to match even by using 100% of the dataset. We release the code at:\nhttps://github.com/google-research/distilling-step-by-step ."}, "authors": [{"name": "Cheng-Yu Hsieh"}, {"name": "Chun-Liang Li"}, {"name": "Chih-Kuan Yeh"}, {"name": "Hootan Nakhost"}, {"name": "Yasuhisa Fujii"}, {"name": "Alexander Ratner"}, {"name": "Ranjay Krishna"}, {"name": "Chen-Yu Lee"}, {"name": "Tomas Pfister"}], "author_detail": {"name": "Tomas Pfister"}, "author": "Tomas Pfister", "arxiv_comment": "Accepted to Findings of ACL 2023", "links": [{"href": "http://arxiv.org/abs/2305.02301v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2305.02301v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}