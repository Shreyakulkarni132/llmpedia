{"id": "http://arxiv.org/abs/2304.10592v1", "guidislink": true, "link": "http://arxiv.org/abs/2304.10592v1", "updated": "2023-04-20T18:25:35Z", "updated_parsed": [2023, 4, 20, 18, 25, 35, 3, 110, 0], "published": "2023-04-20T18:25:35Z", "published_parsed": [2023, 4, 20, 18, 25, 35, 3, 110, 0], "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large\n  Language Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=minigpt+4++enhancing+vision+language+understanding+with+advanced+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large\n  Language Models"}, "summary": "The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such\nas directly generating websites from handwritten text and identifying humorous\nelements within images. These features are rarely observed in previous\nvision-language models. We believe the primary reason for GPT-4's advanced\nmulti-modal generation capabilities lies in the utilization of a more advanced\nlarge language model (LLM). To examine this phenomenon, we present MiniGPT-4,\nwhich aligns a frozen visual encoder with a frozen LLM, Vicuna, using just one\nprojection layer. Our findings reveal that MiniGPT-4 possesses many\ncapabilities similar to those exhibited by GPT-4 like detailed image\ndescription generation and website creation from hand-written drafts.\nFurthermore, we also observe other emerging capabilities in MiniGPT-4,\nincluding writing stories and poems inspired by given images, providing\nsolutions to problems shown in images, teaching users how to cook based on food\nphotos, etc. In our experiment, we found that only performing the pretraining\non raw image-text pairs could produce unnatural language outputs that lack\ncoherency including repetition and fragmented sentences. To address this\nproblem, we curate a high-quality, well-aligned dataset in the second stage to\nfinetune our model using a conversational template. This step proved crucial\nfor augmenting the model's generation reliability and overall usability.\nNotably, our model is highly computationally efficient, as we only train a\nprojection layer utilizing approximately 5 million aligned image-text pairs.\nOur code, pre-trained model, and collected dataset are available at\nhttps://minigpt-4.github.io/.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=minigpt+4++enhancing+vision+language+understanding+with+advanced+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such\nas directly generating websites from handwritten text and identifying humorous\nelements within images. These features are rarely observed in previous\nvision-language models. We believe the primary reason for GPT-4's advanced\nmulti-modal generation capabilities lies in the utilization of a more advanced\nlarge language model (LLM). To examine this phenomenon, we present MiniGPT-4,\nwhich aligns a frozen visual encoder with a frozen LLM, Vicuna, using just one\nprojection layer. Our findings reveal that MiniGPT-4 possesses many\ncapabilities similar to those exhibited by GPT-4 like detailed image\ndescription generation and website creation from hand-written drafts.\nFurthermore, we also observe other emerging capabilities in MiniGPT-4,\nincluding writing stories and poems inspired by given images, providing\nsolutions to problems shown in images, teaching users how to cook based on food\nphotos, etc. In our experiment, we found that only performing the pretraining\non raw image-text pairs could produce unnatural language outputs that lack\ncoherency including repetition and fragmented sentences. To address this\nproblem, we curate a high-quality, well-aligned dataset in the second stage to\nfinetune our model using a conversational template. This step proved crucial\nfor augmenting the model's generation reliability and overall usability.\nNotably, our model is highly computationally efficient, as we only train a\nprojection layer utilizing approximately 5 million aligned image-text pairs.\nOur code, pre-trained model, and collected dataset are available at\nhttps://minigpt-4.github.io/."}, "authors": [{"name": "Deyao Zhu"}, {"name": "Jun Chen"}, {"name": "Xiaoqian Shen"}, {"name": "Xiang Li"}, {"name": "Mohamed Elhoseiny"}], "author_detail": {"name": "Mohamed Elhoseiny"}, "author": "Mohamed Elhoseiny", "arxiv_comment": "Project Website: https://minigpt-4.github.io/; Code, Pretrained\n  Model, and Dataset: https://github.com/Vision-CAIR/MiniGPT-4; Deyao Zhu and\n  Jun Chen contributed equally to this work", "links": [{"href": "http://arxiv.org/abs/2304.10592v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2304.10592v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}