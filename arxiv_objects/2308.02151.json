{"id": "http://arxiv.org/abs/2308.02151v1", "guidislink": true, "link": "http://arxiv.org/abs/2308.02151v1", "updated": "2023-08-04T06:14:23Z", "updated_parsed": [2023, 8, 4, 6, 14, 23, 4, 216, 0], "published": "2023-08-04T06:14:23Z", "published_parsed": [2023, 8, 4, 6, 14, 23, 4, 216, 0], "title": "Retroformer: Retrospective Large Language Agents with Policy Gradient\n  Optimization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=retroformer++retrospective+large+language+agents+with+policy+gradient+optimization&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Retroformer: Retrospective Large Language Agents with Policy Gradient\n  Optimization"}, "summary": "Recent months have seen the emergence of a powerful new trend in which large\nlanguage models (LLMs) are augmented to become autonomous language agents\ncapable of performing objective oriented multi-step tasks on their own, rather\nthan merely responding to queries from human users. Most existing language\nagents, however, are not optimized using environment-specific rewards. Although\nsome agents enable iterative refinement through verbal feedback, they do not\nreason and plan in ways that are compatible with gradient-based learning from\nrewards. This paper introduces a principled framework for reinforcing large\nlanguage agents by learning a retrospective model, which automatically tunes\nthe language agent prompts from environment feedback through policy gradient.\nSpecifically, our proposed agent architecture learns from rewards across\nmultiple environments and tasks, for fine-tuning a pre-trained language model\nwhich refines the language agent prompt by summarizing the root cause of prior\nfailed attempts and proposing action plans. Experimental results on various\ntasks demonstrate that the language agents improve over time and that our\napproach considerably outperforms baselines that do not properly leverage\ngradients from the environment. This demonstrates that using policy gradient\noptimization to improve language agents, for which we believe our work is one\nof the first, seems promising and can be applied to optimize other models in\nthe agent architecture to enhance agent performances over time.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=retroformer++retrospective+large+language+agents+with+policy+gradient+optimization&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Recent months have seen the emergence of a powerful new trend in which large\nlanguage models (LLMs) are augmented to become autonomous language agents\ncapable of performing objective oriented multi-step tasks on their own, rather\nthan merely responding to queries from human users. Most existing language\nagents, however, are not optimized using environment-specific rewards. Although\nsome agents enable iterative refinement through verbal feedback, they do not\nreason and plan in ways that are compatible with gradient-based learning from\nrewards. This paper introduces a principled framework for reinforcing large\nlanguage agents by learning a retrospective model, which automatically tunes\nthe language agent prompts from environment feedback through policy gradient.\nSpecifically, our proposed agent architecture learns from rewards across\nmultiple environments and tasks, for fine-tuning a pre-trained language model\nwhich refines the language agent prompt by summarizing the root cause of prior\nfailed attempts and proposing action plans. Experimental results on various\ntasks demonstrate that the language agents improve over time and that our\napproach considerably outperforms baselines that do not properly leverage\ngradients from the environment. This demonstrates that using policy gradient\noptimization to improve language agents, for which we believe our work is one\nof the first, seems promising and can be applied to optimize other models in\nthe agent architecture to enhance agent performances over time."}, "authors": [{"name": "Weiran Yao"}, {"name": "Shelby Heinecke"}, {"name": "Juan Carlos Niebles"}, {"name": "Zhiwei Liu"}, {"name": "Yihao Feng"}, {"name": "Le Xue"}, {"name": "Rithesh Murthy"}, {"name": "Zeyuan Chen"}, {"name": "Jianguo Zhang"}, {"name": "Devansh Arpit"}, {"name": "Ran Xu"}, {"name": "Phil Mui"}, {"name": "Huan Wang"}, {"name": "Caiming Xiong"}, {"name": "Silvio Savarese"}], "author_detail": {"name": "Silvio Savarese"}, "author": "Silvio Savarese", "links": [{"href": "http://arxiv.org/abs/2308.02151v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2308.02151v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}