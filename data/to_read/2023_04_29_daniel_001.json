{"Published": "2023-04-29", "Title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models", "Authors": "Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher R\u00e9", "Summary": "State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.", "main_contribution": {"headline": "H3 and FlashConv: Enhancing State Space Models for Language Modeling", "description": "The paper introduces two significant advancements to improve the performance of State Space Models (SSMs) in language modeling tasks. The first is a new SSM layer, H3, designed to address the shortcomings of existing SSMs in recalling earlier tokens in a sequence and comparing tokens across the sequence. H3 matches attention on synthetic languages and comes within 0.4 perplexity of Transformers on OpenWebText. The second contribution is FlashConv, a novel algorithm to improve the efficiency of training SSMs on modern hardware. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K and introduces a state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields a 2\u00d7 speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4\u00d7 faster than Transformers."}, "takeaways": {"headline": "SSMs Can Be Enhanced for Language Modeling and Efficiency", "description": "The paper demonstrates that with the right modifications, SSMs can be made more expressive and efficient for language modeling tasks. The H3 layer improves the ability of SSMs to recall and compare tokens across a sequence, bringing their performance closer to that of Transformers. Meanwhile, FlashConv significantly improves the efficiency of training SSMs on modern hardware, making them faster than Transformers. These advancements could be leveraged to build more efficient and expressive language models, potentially leading to faster and more accurate natural language processing applications.", "example": "For instance, an AI researcher could use the H3 layer and FlashConv algorithm to train a large language model on a long text corpus. The H3 layer would allow the model to better recall and compare tokens across the sequence, improving its language understanding capabilities. Meanwhile, FlashConv would make the training process more efficient, reducing the time and computational resources required."}, "category": "ARCHITECTURES", "novelty_analysis": "The paper presents a novel approach to enhancing the expressivity and efficiency of State Space Models for language modeling. The introduction of the H3 layer and the FlashConv algorithm represent significant advancements in the field, addressing key limitations of existing SSMs and bringing their performance closer to that of Transformers.", "novelty_score": 3, "technical_analysis": "The paper is highly technical, delving into the details of State Space Models, the design of the H3 layer, and the workings of the FlashConv algorithm. It requires a solid understanding of language modeling, sequence modeling, and modern hardware architectures to fully grasp the concepts presented.", "technical_score": 3, "enjoyable_analysis": "While the paper is highly technical and dense with information, it is well-structured and presents its findings clearly. The introduction of novel concepts and the demonstration of their effectiveness make it an engaging read for those with the necessary technical background.", "enjoyable_score": 2}