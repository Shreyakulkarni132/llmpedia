{"Published": "2023-02-16", "Title": "Recitation-Augmented Language Models", "Authors": "Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, Denny Zhou", "Summary": "We propose a new paradigm to help Large Language Models (LLMs) generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE). Different from retrieval-augmented language models that retrieve relevant documents before generating the outputs, given an input, RECITE first recites one or several relevant passages from LLMs' own memory via sampling, and then produces the final answers. We show that RECITE is a powerful paradigm for knowledge-intensive NLP tasks. Specifically, we show that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering (CBQA) tasks. In experiments, we verify the effectiveness of \\method~on four pre-trained models (PaLM, UL2, OPT, and Codex) and three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA). Our code is available at \"https://github.com/Edward-Sun/RECITE\".", "main_contribution": {"headline": "RECITE: A New Paradigm for Factual Knowledge Generation in LLMs", "description": "The paper introduces RECITation-augmented gEneration (RECITE), a novel paradigm for Large Language Models (LLMs) to generate more accurate factual knowledge without retrieving from an external corpus. Unlike retrieval-augmented language models that retrieve relevant documents before generating outputs, RECITE first recites one or several relevant passages from the LLM's own memory via sampling, and then produces the final answers. This two-step paradigm decomposes the original knowledge-intensive task into two sub-tasks: knowledge-recitation and task-execution. The authors demonstrate that RECITE is a powerful paradigm for knowledge-intensive NLP tasks, achieving state-of-the-art performance in various closed-book question answering (CBQA) tasks."}, "takeaways": {"headline": "RECITE Enhances LLMs' Performance in Knowledge-Intensive Tasks", "description": "RECITE's recite-and-answer scheme can be leveraged to improve the performance of LLMs in knowledge-intensive tasks. By utilizing recitation as an intermediate step, LLMs can generate more accurate factual knowledge. This approach can be particularly useful in CBQA tasks, where a fully parameterized LM can generate answers directly without an external corpus or separate retrieval models. The paper also suggests that fine-tuning the pre-trained LLMs on synthetic generated question-passage pairs can further improve the recitation performance and lead to better downstream QA accuracy.", "example": "For instance, given a question like 'Who wrote the song I hate you I love you?', RECITE would first recite a relevant passage such as '\"I Hate U, I Love U\" is a song by American singer and rapper Gnash featuring American singer Olivia O'Brien.', and then generate the final answer 'Gnash'."}, "category": "FINE-TUNING", "novelty_analysis": "The introduction of RECITE represents a significant novelty in the field of LLMs. The paradigm deviates from the traditional retrieval-augmented language models by incorporating a recitation step from the LLM's own memory, which is a unique and innovative approach to generating more accurate factual knowledge.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, detailing the workings of the RECITE paradigm and its application in various CBQA tasks. It delves into the specifics of how the recite-and-answer scheme is implemented and evaluated, requiring a solid understanding of LLMs and NLP tasks.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents a novel concept in a clear and comprehensible manner. The use of illustrative examples and detailed experiment results makes it an engaging read for those interested in the application of LLMs in knowledge-intensive tasks.", "enjoyable_score": 2}