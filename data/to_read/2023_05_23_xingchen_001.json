{"Published": "2023-05-23", "Title": "Better Zero-Shot Reasoning with Self-Adaptive Prompting", "Authors": "Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan O. Arik, Tomas Pfister", "Summary": "Modern large language models (LLMs) have demonstrated impressive capabilities at sophisticated tasks, often through step-by-step reasoning similar to humans. This is made possible by their strong few and zero-shot abilities -- they can effectively learn from a handful of handcrafted, completed responses (\"in-context examples\"), or are prompted to reason spontaneously through specially designed triggers. Nonetheless, some limitations have been observed. First, performance in the few-shot setting is sensitive to the choice of examples, whose design requires significant human effort. Moreover, given the diverse downstream tasks of LLMs, it may be difficult or laborious to handcraft per-task labels. Second, while the zero-shot setting does not require handcrafting, its performance is limited due to the lack of guidance to the LLMs. To address these limitations, we propose Consistency-based Self-adaptive Prompting (COSP), a novel prompt design method for LLMs. Requiring neither handcrafted responses nor ground-truth labels, COSP selects and builds the set of examples from the LLM zero-shot outputs via carefully designed criteria that combine consistency, diversity and repetition. In the zero-shot setting for three different LLMs, we show that using only LLM predictions, COSP improves performance up to 15% compared to zero-shot baselines and matches or exceeds few-shot baselines for a range of reasoning tasks.", "main_contribution": {"headline": "Consistency-based Self-adaptive Prompting (COSP) Improves Zero-Shot Reasoning in LLMs", "description": "The paper introduces a novel prompt design method for Large Language Models (LLMs) called Consistency-based Self-adaptive Prompting (COSP). COSP addresses the limitations of few-shot and zero-shot settings in LLMs by eliminating the need for handcrafted responses or ground-truth labels. Instead, it builds a set of examples from the LLM's zero-shot outputs using criteria that combine consistency, diversity, and repetition. The authors demonstrate that COSP can improve performance by up to 15% compared to zero-shot baselines and can match or exceed few-shot baselines for a range of reasoning tasks."}, "takeaways": {"headline": "COSP Enhances LLM Performance in Zero-Shot Reasoning Tasks", "description": "COSP offers a promising approach to improve the performance of LLMs in zero-shot reasoning tasks without the need for handcrafted responses or labels. This method can be particularly useful in scenarios where it is difficult or laborious to handcraft per-task labels. The paper's findings suggest that COSP could be used to enhance the performance of LLMs in a variety of applications, from natural language processing tasks to more complex reasoning tasks.", "example": "For instance, in a zero-shot reasoning task where the LLM is asked to predict the next event in a sequence, COSP could be used to generate a set of examples from the LLM's own predictions. These examples, selected for their consistency, diversity, and repetition, could then be used to guide the LLM in generating more accurate predictions."}, "category": "PROMPTING", "novelty_analysis": "The introduction of COSP represents a significant advancement in the field of LLM prompting. While the concept of using the LLM's own predictions to guide its reasoning is not entirely new, the specific approach of combining consistency, diversity, and repetition in the selection of examples is a novel contribution.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, as it introduces a new method for prompt design and evaluates its performance using a range of metrics. However, the concepts are explained clearly and the methodology is well-documented, making it accessible to readers with a basic understanding of LLMs and prompting techniques.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents a clear narrative, making it an enjoyable read. The introduction of a novel method and the demonstration of its effectiveness in improving LLM performance provide a compelling narrative that keeps the reader engaged.", "enjoyable_score": 2}