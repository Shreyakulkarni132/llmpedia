{"Published": "2023-06-14", "Title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention", "Authors": "Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Yu Qiao", "Summary": "We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the word tokens at higher transformer layers. Then, a zero-initialized attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter can generate high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Besides language commands, our approach can be simply extended to multi-modal instructions for learning image-conditioned LLaMA model, which achieves superior reasoning performance on ScienceQA and COCO Caption benchmarks. Furthermore, we also evaluate the zero-initialized attention mechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on traditional vision and language tasks, demonstrating the superior generalization capacity of our approach. Code is released at https://github.com/OpenGVLab/LLaMA-Adapter.", "main_contribution": {"headline": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention", "description": "The paper introduces LLaMA-Adapter, a method for efficiently fine-tuning Large Language Models (LLMs) into instruction-following models. The method uses a set of learnable adaption prompts, which are prepended to the word tokens at higher transformer layers. A zero-initialized attention mechanism with zero gating is proposed, which adaptively injects new instructional cues into the LLaMA model while preserving its pre-trained knowledge. This approach only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model and costs less than one hour for fine-tuning on 8 A100 GPUs. The method can be extended to multi-modal instructions for learning image-conditioned LLaMA models and can be used for fine-tuning other pre-trained models (ViT, RoBERTa) on traditional vision and language tasks."}, "takeaways": {"headline": "Efficient Fine-tuning of LLMs with LLaMA-Adapter", "description": "LLaMA-Adapter provides an efficient way to fine-tune LLMs into instruction-following models. It introduces a minimal number of learnable parameters, reducing computational demands and training time. The method can be extended to multi-modal instructions, enhancing the model's reasoning performance on benchmarks like ScienceQA and COCO Caption. Furthermore, the zero-initialized attention mechanism can be used for fine-tuning other pre-trained models on traditional vision and language tasks, demonstrating the method's versatility and generalization capacity.", "example": "For example, to fine-tune a pre-trained LLaMA model into an instruction-following model, you can use LLaMA-Adapter to prepend learnable adaption prompts to the word tokens at higher transformer layers. Then, apply the zero-initialized attention mechanism with zero gating to adaptively inject new instructional cues into the model. This approach will allow the model to generate high-quality responses while preserving its pre-trained knowledge."}, "category": "FINE-TUNING", "novelty_analysis": "The paper presents a novel approach to fine-tuning LLMs with a focus on efficiency and versatility. The introduction of learnable adaption prompts and a zero-initialized attention mechanism with zero gating is a unique contribution that allows for efficient fine-tuning of LLMs into instruction-following models. The method's ability to be extended to multi-modal instructions and other pre-trained models further enhances its novelty.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, detailing the workings of the LLaMA-Adapter method, including the use of learnable adaption prompts and a zero-initialized attention mechanism. It also discusses the application of the method to multi-modal instructions and other pre-trained models. However, the concepts are explained clearly, making it accessible to readers with a background in AI and ML.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and provides a clear explanation of the proposed method, making it an enjoyable read. The inclusion of practical applications and comparisons with other methods adds to the paper's appeal. The results and discussions provide valuable insights into the potential of the LLaMA-Adapter method in fine-tuning LLMs.", "enjoyable_score": 2}