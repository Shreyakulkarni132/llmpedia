{"Published": "2023-03-08", "Title": "Larger language models do in-context learning differently", "Authors": "Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, Tengyu Ma", "Summary": "We study how in-context learning (ICL) in language models is affected by semantic priors versus input-label mappings. We investigate two setups-ICL with flipped labels and ICL with semantically-unrelated labels-across various model families (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First, experiments on ICL with flipped labels show that overriding semantic priors is an emergent ability of model scale. While small language models ignore flipped labels presented in-context and thus rely primarily on semantic priors from pretraining, large models can override semantic priors when presented with in-context exemplars that contradict priors, despite the stronger semantic priors that larger models may hold. We next study semantically-unrelated label ICL (SUL-ICL), in which labels are semantically unrelated to their inputs (e.g., foo/bar instead of negative/positive), thereby forcing language models to learn the input-label mappings shown in in-context exemplars in order to perform the task. The ability to do SUL-ICL also emerges primarily with scale, and large-enough language models can even perform linear classification in a SUL-ICL setting. Finally, we evaluate instruction-tuned models and find that instruction tuning strengthens both the use of semantic priors and the capacity to learn input-label mappings, but more of the former.", "main_contribution": {"headline": "Exploring the Role of Semantic Priors and Input-Label Mappings in Large Language Models", "description": "The paper investigates how in-context learning (ICL) in language models is influenced by semantic priors and input-label mappings. The authors conduct experiments with different setups, including ICL with flipped labels and ICL with semantically-unrelated labels, across various model families. They find that the ability to override semantic priors is an emergent property of model scale. While smaller models rely primarily on semantic priors from pretraining, larger models can override these priors when presented with in-context exemplars that contradict them. The study also reveals that the capacity to perform ICL with semantically-unrelated labels emerges primarily with scale, and large-enough models can even perform linear classification in such a setting. The paper further evaluates instruction-tuned models and finds that instruction tuning strengthens both the use of semantic priors and the capacity to learn input-label mappings, but more of the former."}, "takeaways": {"headline": "Large Language Models Can Override Semantic Priors and Learn Input-Label Mappings", "description": "The findings of this paper suggest that large language models have the ability to override semantic priors and learn input-label mappings, which can be leveraged to improve their performance in various tasks. For instance, in applications where the labels are not semantically related to the inputs, large models can still perform well by learning the input-label mappings. This ability can be particularly useful in tasks where the labels are not intuitive or are domain-specific. Moreover, the study shows that instruction tuning can enhance both the use of semantic priors and the capacity to learn input-label mappings, which can be used to further improve the performance of large models.", "example": "For example, in a sentiment analysis task where the labels are 'foo' and 'bar' instead of 'positive' and 'negative', a large language model can still perform well by learning the mapping between the inputs and these semantically-unrelated labels. This can be achieved by providing the model with a few in-context exemplars of input-label pairs before performing the task on an unseen example."}, "category": "BEHAVIOR", "novelty_analysis": "The paper provides a novel investigation into how semantic priors and input-label mappings influence in-context learning in large language models. The findings that large models can override semantic priors and learn input-label mappings, and that these abilities emerge primarily with scale, are significant contributions to the understanding of the behavior of large language models.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, as it involves a detailed analysis of the behavior of large language models in different experimental setups. It requires a good understanding of concepts such as in-context learning, semantic priors, and input-label mappings. However, the authors explain these concepts clearly and provide comprehensive descriptions of the experiments and their results, making the paper accessible to readers with a background in machine learning.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting investigation into the behavior of large language models. The clear explanations of the concepts and the detailed descriptions of the experiments and their results make the paper an engaging read for those interested in understanding the capabilities and limitations of large language models.", "enjoyable_score": 2}