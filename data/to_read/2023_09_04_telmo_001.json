{"Published": "2023-09-04", "Title": "One Wide Feedforward is All You Need", "Authors": "Telmo Pessoa Pires, Ant\u00f3nio V. Lopes, Yannick Assogba, Hendra Setiawan", "Summary": "The Transformer architecture has two main non-embedding components: Attention and the Feed Forward Network (FFN). Attention captures interdependencies between words regardless of their position, while the FFN non-linearly transforms each input token independently. In this work we explore the role of the FFN, and find that despite taking up a significant fraction of the model's parameters, it is highly redundant. Concretely, we are able to substantially reduce the number of parameters with only a modest drop in accuracy by removing the FFN on the decoder layers and sharing a single FFN across the encoder. Finally we scale this architecture back to its original size by increasing the hidden dimension of the shared FFN, achieving substantial gains in both accuracy and latency with respect to the original Transformer Big.", "main_contribution": {"headline": "Reducing Redundancy in Transformer Architecture through Shared Feed Forward Networks", "description": "The paper investigates the role of the Feed Forward Network (FFN) in the Transformer architecture, a critical component that non-linearly transforms each input token independently. The authors find that the FFN, despite taking up a significant fraction of the model's parameters, is highly redundant. They propose a novel approach of sharing a single FFN across the encoder and removing the FFN on the decoder layers. This results in a substantial reduction in the number of parameters with only a modest drop in accuracy. Furthermore, they scale the architecture back to its original size by increasing the hidden dimension of the shared FFN, achieving substantial gains in both accuracy and latency compared to the original Transformer Big."}, "takeaways": {"headline": "Shared FFN in Transformer Architecture Improves Efficiency and Performance", "description": "The paper's findings suggest that sharing a single FFN across the encoder and removing the FFN on the decoder layers can lead to a more efficient and performant Transformer model. This approach can be beneficial for LLM practitioners looking to optimize their models for real-world deployment, where practical constraints like latency requirements and memory and disk space limitations are critical. The proposed architecture can be applied to various NLP tasks, including Machine Translation (MT), potentially improving the performance and efficiency of these applications.", "example": "For instance, in a translation task, an LLM practitioner could implement the shared FFN approach in their Transformer model. This would involve removing the FFN on the decoder layers and sharing a single FFN across the encoder, followed by increasing the hidden dimension of the shared FFN to scale the architecture back to its original size."}, "category": "ARCHITECTURES", "novelty_analysis": "The paper presents a novel approach to optimizing the Transformer architecture by reducing redundancy in the Feed Forward Network (FFN). This approach is unique in its focus on sharing a single FFN across the encoder and removing the FFN on the decoder layers, which has not been extensively explored in previous work.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, delving into the specifics of the Transformer architecture and the role of the Feed Forward Network (FFN). However, the concepts are explained clearly and the methodology is well-detailed, making it accessible to readers with a basic understanding of LLMs and Transformer models.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting exploration of the Transformer architecture. The clear presentation of the methodology and results, along with the practical implications of the findings, make it an engaging read for those interested in LLMs and NLP.", "enjoyable_score": 2}