{"Published": "2023-05-24", "Title": "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining", "Authors": "Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V. Le, Tengyu Ma, Adams Wei Yu", "Summary": "The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to find domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5% points over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no knowledge of downstream tasks, even matches the performance of using domain weights tuned on downstream tasks.", "main_contribution": {"headline": "DoReMi: A Novel Approach to Optimize Data Mixtures for Efficient Language Model Pretraining", "description": "The paper introduces Domain Reweighting with Minimax Optimization (DoReMi), a novel approach to optimize the mixture proportions of pretraining data domains for language models (LMs). The method first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights without knowledge of downstream tasks. These weights are then used to resample a dataset and train a larger, full-sized model. The authors demonstrate the effectiveness of DoReMi by using it on a 280M-parameter proxy model to find domain weights for training an 8B-parameter model more efficiently. The results show that DoReMi improves perplexity across all domains and enhances average few-shot downstream accuracy by 6.5% points over a baseline model trained using The Pile's default domain weights."}, "takeaways": {"headline": "DoReMi Enhances Language Model Pretraining Efficiency and Performance", "description": "DoReMi presents a promising approach to optimize the mixture proportions of pretraining data domains, leading to more efficient and effective language model pretraining. By using a small proxy model to find optimal domain weights, DoReMi can significantly improve the performance of a much larger model. This approach can be particularly beneficial in scenarios where computational resources are limited, as it allows for the training of larger models more efficiently. Furthermore, the improved performance on downstream tasks suggests that DoReMi could be a valuable tool for developing more accurate and robust language models.", "example": "For instance, if you're training a language model on a dataset composed of various domains like Wikipedia, books, and web text, you can use DoReMi to find the optimal mixture proportions for these domains. You would first train a small proxy model using Group DRO over the domains to produce domain weights. Then, you would resample your dataset using these weights and train your larger model. This could lead to improved performance on downstream tasks and more efficient use of your computational resources."}, "category": "TRAINING", "novelty_analysis": "DoReMi introduces a novel approach to optimize the mixture proportions of pretraining data domains for language models. While the concept of adjusting domain weights is not new, the use of a small proxy model to find optimal weights and the application of Group DRO over domains represent significant advancements in the field.", "novelty_score": 3, "technical_analysis": "The paper is highly technical, delving into the specifics of the DoReMi algorithm, the use of Group DRO, and the process of optimizing domain weights. It requires a solid understanding of language model pretraining, optimization techniques, and distributionally robust optimization.", "technical_score": 3, "enjoyable_analysis": "The paper is well-structured and provides a clear explanation of the proposed method, its implementation, and its benefits. The use of figures and tables aids in understanding the concept and the results. However, the high level of technical detail might make it a challenging read for those not familiar with the specific concepts and techniques discussed.", "enjoyable_score": 2}