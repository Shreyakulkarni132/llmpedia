{"Published": "2023-05-05", "Title": "Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection", "Authors": "Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, Mario Fritz", "Summary": "Large Language Models (LLMs) are increasingly being integrated into various applications. The functionalities of recent LLMs can be flexibly modulated via natural language prompts. This renders them susceptible to targeted adversarial prompting, e.g., Prompt Injection (PI) attacks enable attackers to override original instructions and employed controls. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We argue that LLM-Integrated Applications blur the line between data and instructions. We reveal new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved. We derive a comprehensive taxonomy from a computer security perspective to systematically investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We demonstrate our attacks' practical viability against both real-world systems, such as Bing's GPT-4 powered Chat and code-completion engines, and synthetic applications built on GPT-4. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing integration and reliance on LLMs, effective mitigations of these emerging threats are currently lacking. By raising awareness of these vulnerabilities and providing key insights into their implications, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users and systems from potential attacks.", "main_contribution": {"headline": "Indirect Prompt Injection: A New Attack Vector for LLM-Integrated Applications", "description": "The paper introduces the concept of Indirect Prompt Injection (IPI) as a new attack vector for Large Language Models (LLMs) integrated into applications. The authors argue that the integration of LLMs into applications blurs the line between data and instructions, making them susceptible to adversarial prompting. They reveal that adversaries can remotely exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved. The authors develop a comprehensive taxonomy from a computer security perspective to systematically investigate the impacts and vulnerabilities of IPI, including data theft, worming, information ecosystem contamination, and other novel security risks. They demonstrate the practical viability of these attacks against both real-world systems and synthetic applications built on GPT-4."}, "takeaways": {"headline": "Indirect Prompt Injection Poses Significant Security Risks for LLM-Integrated Applications", "description": "The paper highlights the significant security risks posed by Indirect Prompt Injection (IPI) for LLM-integrated applications. The authors demonstrate that processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. They emphasize the need for robust defenses against these emerging threats. The insights provided by the authors can be used to develop more secure LLM-integrated applications and promote the safe and responsible deployment of these powerful models.", "example": "For instance, an adversary could inject a malicious prompt into a data source that an LLM-integrated application is likely to retrieve. When the application retrieves and processes this data, it could inadvertently execute the malicious prompt, leading to unwanted behaviors such as data theft or system compromise."}, "category": "BEHAVIOR", "novelty_analysis": "The paper introduces a novel attack vector, Indirect Prompt Injection (IPI), for LLM-integrated applications. This concept is a significant contribution to the field as it reveals a previously unexplored vulnerability in these applications. The authors' systematic investigation of the impacts and vulnerabilities of IPI, as well as their demonstration of the practical viability of these attacks, further enhances the novelty of their work.", "novelty_score": 3, "technical_analysis": "The paper is highly technical, delving into the intricacies of Large Language Models (LLMs), their integration into applications, and the security vulnerabilities that arise from this integration. The authors discuss complex concepts such as adversarial prompting, arbitrary code execution, and various security risks. They also develop a comprehensive taxonomy to systematically investigate these risks, requiring a deep understanding of computer security principles.", "technical_score": 3, "enjoyable_analysis": "The paper is well-structured and provides a thorough exploration of a novel and significant topic. The authors' clear explanations of complex concepts, their systematic investigation of the impacts and vulnerabilities of Indirect Prompt Injection, and their practical demonstrations of these attacks make the paper an engaging read for those interested in the security aspects of LLM-integrated applications.", "enjoyable_score": 2}