{"Published": "2023-05-18", "Title": "Unlimiformer: Long-Range Transformers with Unlimited Length Input", "Authors": "Amanda Bertsch, Uri Alon, Graham Neubig, Matthew R. Gormley", "Summary": "Since the proposal of transformers, these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned weights and without modifying their code. We make our code and models publicly available at https://github.com/abertsch72/unlimiformer .", "main_contribution": {"headline": "Unlimiformer: A Method for Processing Unlimited Length Input in Transformers", "description": "The paper introduces Unlimiformer, a novel approach that enables transformers to process inputs of unlimited length. This is achieved by offloading the cross-attention computation to a k-nearest-neighbor (kNN) index, which can be stored on either GPU or CPU memory. The kNN distances are used as the attention dot-product scores, allowing the model to attend to the top-k keys instead of every key. Unlimiformer can be applied to any existing pretrained encoder-decoder transformer without adding additional learned weights or modifying their code. The authors demonstrate that Unlimiformer can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time, and improves the performance of pretrained models such as BART and Longformer."}, "takeaways": {"headline": "Unlimiformer Enables Transformers to Handle Extremely Long Inputs", "description": "Unlimiformer's ability to handle unlimited length input sequences can be a game-changer for tasks that involve long narratives, such as book summarization. It can be applied to existing pretrained models, improving their performance without the need for additional training or modification of their code. This makes Unlimiformer a versatile and efficient tool for enhancing the capabilities of transformer models. For example, in a task where a transformer needs to process a long document or book for summarization, Unlimiformer can be applied to handle the entire input without truncation, potentially leading to more accurate and comprehensive summaries.", "example": "Consider a task where we need to summarize a long book. With a standard transformer model, we might need to truncate the input or split it into chunks, which could lead to loss of context or important information. However, by applying Unlimiformer to the model, we can process the entire book as a single input, allowing the model to attend to the most relevant parts of the text and produce a more accurate and comprehensive summary."}, "category": "ARCHITECTURES", "novelty_analysis": "Unlimiformer presents a novel approach to handling unlimited length input sequences in transformer models. While transformers have been used extensively in various tasks, their limitation in handling long inputs has been a significant challenge. Unlimiformer addresses this issue without the need for additional training or modification of the model's code, making it a unique and significant contribution to the field.", "novelty_score": 3, "technical_analysis": "The paper is quite technical, delving into the specifics of how Unlimiformer works, including the use of a k-nearest-neighbor index for offloading cross-attention computation and the reformulation of the attention mechanism. It requires a good understanding of transformer models and attention mechanisms to fully grasp the details.", "technical_score": 3, "enjoyable_analysis": "The paper is well-structured and provides a clear explanation of the problem it addresses, the proposed solution, and the benefits of the approach. The use of diagrams and examples aids in understanding the concepts. However, the technical depth of the paper might make it a challenging read for those not familiar with the intricacies of transformer models and attention mechanisms.", "enjoyable_score": 2}