{"Published": "2023-08-21", "Title": "Giraffe: Adventures in Expanding Context Lengths in LLMs", "Authors": "Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, Siddartha Naidu", "Summary": "Modern large language models (LLMs) that rely on attention mechanisms are typically trained with fixed context lengths which enforce upper limits on the length of input sequences that they can handle at evaluation time. To use these models on sequences longer than the train-time context length, one might employ techniques from the growing family of context length extrapolation methods -- most of which focus on modifying the system of positional encodings used in the attention mechanism to indicate where tokens or activations are located in the input sequence. We conduct a wide survey of existing methods of context length extrapolation on a base LLaMA or LLaMA 2 model, and introduce some of our own design as well -- in particular, a new truncation strategy for modifying the basis for the position encoding.   We test these methods using three new evaluation tasks (FreeFormQA, AlteredNumericQA, and LongChat-Lines) as well as perplexity, which we find to be less fine-grained as a measure of long context performance of LLMs. We release the three tasks publicly as datasets on HuggingFace. We discover that linear scaling is the best method for extending context length, and show that further gains can be achieved by using longer scales at evaluation time. We also discover promising extrapolation capabilities in the truncated basis. To support further research in this area, we release three new 13B parameter long-context models which we call Giraffe: 4k and 16k context models trained from base LLaMA-13B, and a 32k context model trained from base LLaMA2-13B. We also release the code to replicate our results.", "main_contribution": {"headline": "Giraffe: A Study on Context Length Extrapolation in Large Language Models", "description": "The paper presents a comprehensive study on context length extrapolation in Large Language Models (LLMs), focusing on zero-shot extrapolation. The authors survey existing methods and introduce their own, including a new truncation strategy for modifying the basis for position encoding. They test these methods using three new evaluation tasks and perplexity, finding that linear scaling is the best method for extending context length. The authors also release three new 13B parameter long-context models, which they call Giraffe, and the code to replicate their results."}, "takeaways": {"headline": "Linear Scaling Emerges as the Best Method for Context Length Extrapolation", "description": "The paper's findings suggest that linear scaling is the most effective method for extending context length in LLMs. This insight can be valuable for AI practitioners working with LLMs on tasks that require handling long input sequences. The authors also release three new models and their corresponding code, which can serve as a valuable resource for researchers and practitioners looking to replicate or build upon these results. The new evaluation tasks introduced in this study can also be used to benchmark the performance of other LLMs on long context tasks.", "example": "For instance, if an AI practitioner is working on a task that requires processing long documents, they can use the linear scaling method to extend the context length of their LLM. They can also use the Giraffe models and the evaluation tasks introduced in this study to benchmark the performance of their LLM."}, "category": "FINE-TUNING", "novelty_analysis": "The paper presents a comprehensive study on context length extrapolation in LLMs, which is a relatively unexplored area. The introduction of a new truncation strategy for modifying the basis for position encoding and the release of three new long-context models add novelty to the work.", "novelty_score": 2, "technical_analysis": "The paper is somewhat technical as it delves into the details of different context length extrapolation methods and introduces a new truncation strategy. However, the concepts are explained clearly, making it accessible to readers with a basic understanding of LLMs and attention mechanisms.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents a thorough exploration of context length extrapolation in LLMs. The introduction of new methods and models, along with the release of code and datasets, makes it an engaging read for researchers and practitioners in the field.", "enjoyable_score": 2}