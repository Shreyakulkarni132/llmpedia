{"Published": "2023-08-27", "Title": "MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic Medical Records", "Authors": "Scott L. Fleming, Alejandro Lozano, William J. Haberkorn, Jenelle A. Jindal, Eduardo P. Reis, Rahul Thapa, Louis Blankemeier, Julian Z. Genkins, Ethan Steinberg, Ashwin Nayak, Birju S. Patel, Chia-Chun Chiang, Alison Callahan, Zepeng Huo, Sergios Gatidis, Scott J. Adams, Oluseyi Fayanju, Shreya J. Shah, Thomas Savage, Ethan Goh, Akshay S. Chaudhari, Nima Aghaeepour, Christopher Sharp, Michael A. Pfeffer, Percy Liang, Jonathan H. Chen, Keith E. Morse, Emma P. Brunskill, Jason A. Fries, Nigam H. Shah", "Summary": "The ability of large language models (LLMs) to follow natural language instructions with human-level fluency suggests many opportunities in healthcare to reduce administrative burden and improve quality of care. However, evaluating LLMs on realistic text generation tasks for healthcare remains challenging. Existing question answering datasets for electronic health record (EHR) data fail to capture the complexity of information needs and documentation burdens experienced by clinicians. To address these challenges, we introduce MedAlign, a benchmark dataset of 983 natural language instructions for EHR data. MedAlign is curated by 15 clinicians (7 specialities), includes clinician-written reference responses for 303 instructions, and provides 276 longitudinal EHRs for grounding instruction-response pairs. We used MedAlign to evaluate 6 general domain LLMs, having clinicians rank the accuracy and quality of each LLM response. We found high error rates, ranging from 35% (GPT-4) to 68% (MPT-7B-Instruct), and an 8.3% drop in accuracy moving from 32k to 2k context lengths for GPT-4. Finally, we report correlations between clinician rankings and automated natural language generation metrics as a way to rank LLMs without human review. We make MedAlign available under a research data use agreement to enable LLM evaluations on tasks aligned with clinician needs and preferences.", "main_contribution": {"headline": "MedAlign: A Clinician-Generated Dataset for Evaluating LLMs on EHR Tasks", "description": "The paper introduces MedAlign, a benchmark dataset for evaluating the performance of Large Language Models (LLMs) on tasks related to Electronic Health Records (EHRs). MedAlign consists of 983 natural language instructions curated by 15 clinicians across 7 specialties, and includes clinician-written reference responses for 303 instructions. The dataset also provides 276 longitudinal EHRs for grounding instruction-response pairs. The authors used MedAlign to evaluate six general domain LLMs, with clinicians ranking the accuracy and quality of each LLM response. The paper also reports correlations between clinician rankings and automated natural language generation metrics, providing a way to rank LLMs without human review."}, "takeaways": {"headline": "MedAlign Provides a Benchmark for Evaluating LLMs on Realistic Healthcare Tasks", "description": "MedAlign offers a valuable resource for evaluating the performance of LLMs on tasks that are representative of the real-world information needs and documentation burdens experienced by clinicians. The dataset allows for the evaluation of LLMs on tasks such as summarizing a patient\u2019s medical history, generating a differential diagnosis based on partially resulted laboratory data, or searching through clinical notes for mentions of a patient\u2019s family support system. The authors' evaluation of six LLMs using MedAlign revealed high error rates, highlighting the challenges of applying LLMs to complex clinical environments. The paper also demonstrates the feasibility of using automated metrics to rank LLMs, potentially reducing the need for time-consuming and costly clinician review.", "example": "For instance, a researcher could use MedAlign to evaluate a new LLM designed for healthcare applications. The researcher could feed the LLM with the instructions and EHR data from MedAlign, and then compare the LLM's responses to the clinician-written reference responses. This would provide a measure of the LLM's ability to perform tasks that are representative of the real-world information needs of clinicians. The researcher could also use the automated metrics discussed in the paper to rank the LLM against other models without the need for clinician review."}, "category": "USE CASES", "novelty_analysis": "The introduction of MedAlign represents a significant contribution to the field of applied LLMs in healthcare. While there are existing datasets for evaluating LLMs on healthcare tasks, MedAlign is unique in its focus on tasks that are representative of the real-world information needs and documentation burdens experienced by clinicians. The dataset also includes clinician-written reference responses, providing a valuable resource for training and evaluating LLMs.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, as it involves the application of LLMs to healthcare tasks and the evaluation of these models using both human rankings and automated metrics. However, the authors provide clear explanations of their methodology and the challenges involved in applying LLMs to complex clinical environments. The paper should be accessible to readers with a basic understanding of LLMs and healthcare data.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and provides a comprehensive overview of the challenges and opportunities in applying LLMs to healthcare tasks. The introduction of MedAlign and the evaluation of six LLMs using this dataset offer valuable insights for researchers and practitioners in the field. The paper's focus on real-world clinical tasks and its discussion of the potential of LLMs to reduce administrative burden and improve quality of care make it an engaging read.", "enjoyable_score": 3}