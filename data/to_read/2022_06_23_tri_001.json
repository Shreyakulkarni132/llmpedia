{"Published": "2022-06-23", "Title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness", "Authors": "Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher R\u00e9", "Summary": "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).", "main_contribution": {"headline": "FlashAttention: An IO-Aware Exact Attention Algorithm for Efficient Transformer Training", "description": "The paper introduces FlashAttention, an IO-aware exact attention algorithm that optimizes the memory usage in Transformer models. The algorithm uses a tiling strategy to minimize the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. The authors also extend FlashAttention to block-sparse attention, creating an approximate attention algorithm that outperforms existing methods. The IO-awareness of FlashAttention makes it unique, as it accounts for the reads and writes between different levels of GPU memory, a factor often overlooked in other attention algorithms. The authors demonstrate that FlashAttention requires fewer HBM accesses than standard attention and is optimal for a range of SRAM sizes."}, "takeaways": {"headline": "FlashAttention Speeds Up Transformer Training and Enables Longer Contexts", "description": "FlashAttention offers a significant speedup in training Transformers, making it a valuable tool for practitioners working with large language models. It achieves a 15% end-to-end wall-clock speedup on BERT-large, 3x speedup on GPT-2, and 2.4x speedup on long-range arena. Furthermore, FlashAttention enables longer context in Transformers, leading to higher quality models and new capabilities. For instance, it is the first to achieve better-than-chance performance on the Path-X challenge and Path-256. Therefore, FlashAttention can be used to improve the efficiency of training large language models and to extend their capabilities.", "example": "For instance, when training a Transformer model on a large dataset, one could use FlashAttention to reduce the training time. This would involve modifying the attention mechanism in the Transformer architecture to use FlashAttention instead of the standard attention mechanism."}, "category": "TRAINING", "novelty_analysis": "The introduction of FlashAttention represents a significant advancement in the field of Transformer training. The IO-awareness of the algorithm and its focus on optimizing memory usage between different levels of GPU memory is a novel approach. The extension of FlashAttention to block-sparse attention also contributes to its novelty.", "novelty_score": 3, "technical_analysis": "The paper is highly technical, delving into the details of the FlashAttention algorithm, its IO complexity, and its application to block-sparse attention. It requires a strong understanding of Transformer models, attention mechanisms, and GPU memory hierarchies.", "technical_score": 3, "enjoyable_analysis": "The paper is well-structured and presents a novel and intriguing contribution to the field of Transformer training. However, its high technicality and focus on specific aspects of GPU memory might make it less accessible and enjoyable for readers without a strong background in these areas.", "enjoyable_score": 2}