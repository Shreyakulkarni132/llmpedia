{"Published": "2023-05-16", "Title": "Towards Expert-Level Medical Question Answering with Large Language Models", "Authors": "Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, Vivek Natarajan", "Summary": "Recent artificial intelligence (AI) systems have reached milestones in \"grand challenges\" ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge.   Large language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a \"passing\" score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models' answers were compared to clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach.   Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19% and setting a new state-of-the-art. We also observed performance approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU clinical topics datasets.   We performed detailed human evaluations on long-form questions along multiple axes relevant to clinical applications. In pairwise comparative ranking of 1066 consumer medical questions, physicians preferred Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility (p < 0.001). We also observed significant improvements compared to Med-PaLM on every evaluation axis (p < 0.001) on newly introduced datasets of 240 long-form \"adversarial\" questions to probe LLM limitations.   While further studies are necessary to validate the efficacy of these models in real-world settings, these results highlight rapid progress towards physician-level performance in medical question answering.", "main_contribution": {"headline": "Med-PaLM 2: A Large Language Model Achieving Expert-Level Medical Question Answering", "description": "The paper presents Med-PaLM 2, a large language model (LLM) that significantly improves upon its predecessor, Med-PaLM, in answering medical questions. Med-PaLM 2 leverages improvements in the base LLM (PaLM 2), medical domain-specific finetuning, and novel prompting strategies, including an ensemble refinement approach. The model achieved a score of 86.5% on the MedQA dataset, a 19% improvement over Med-PaLM, setting a new state-of-the-art. It also demonstrated performance approaching or exceeding state-of-the-art across other datasets like MedMCQA, PubMedQA, and MMLU clinical topics. In a comparative ranking of 1066 consumer medical questions, physicians preferred Med-PaLM 2's answers over those produced by physicians on eight of nine axes relevant to clinical utility."}, "takeaways": {"headline": "Med-PaLM 2 Sets New Benchmark in Medical Question Answering", "description": "Med-PaLM 2's performance highlights the potential of LLMs in the medical domain, particularly in answering complex medical questions. The model's ensemble refinement approach, which conditions the LLM on multiple possible reasoning paths, could be a promising strategy for improving LLM reasoning in other domains as well. The model's performance on adversarial questions also underscores the importance of comprehensive evaluation in understanding the safety and limitations of these models. For practitioners, Med-PaLM 2 could serve as a valuable tool in medical education, patient engagement, and clinical decision support, although further studies are needed to validate its efficacy in real-world settings.", "example": "For instance, a medical student preparing for the USMLE could use Med-PaLM 2 as a study tool. They could input a medical question, and the model would generate a detailed, physician-level answer. The student could then compare this answer with their own understanding to identify gaps in their knowledge."}, "category": "FINE-TUNING", "novelty_analysis": "The paper presents a significant advancement in the application of LLMs to medical question answering. The introduction of Med-PaLM 2, with its novel ensemble refinement prompting strategy and medical domain-specific finetuning, represents a substantial step forward in achieving physician-level performance in this task.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, delving into the specifics of the Med-PaLM 2 model, its training process, and the novel ensemble refinement prompting strategy. However, it does not require advanced mathematical knowledge and is accessible to readers with a basic understanding of LLMs and their applications.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents a compelling narrative of progress in the application of LLMs to medical question answering. The clear presentation of results, along with detailed human evaluations, makes it an engaging read for those interested in the intersection of AI and medicine.", "enjoyable_score": 3}