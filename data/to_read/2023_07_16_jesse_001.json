{"Published": "2023-07-16", "Title": "Learning to Compress Prompts with Gist Tokens", "Authors": "Jesse Mu, Xiang Lisa Li, Noah Goodman", "Summary": "Prompting is the primary way to utilize the multitask capabilities of language models (LMs), but prompts occupy valuable space in the input context window, and repeatedly encoding the same prompt is computationally inefficient. Finetuning and distillation methods allow for specialization of LMs without prompting, but require retraining the model for each task. To avoid this trade-off entirely, we present gisting, which trains an LM to compress prompts into smaller sets of \"gist\" tokens which can be cached and reused for compute efficiency. Gist models can be trained with no additional cost over standard instruction finetuning by simply modifying Transformer attention masks to encourage prompt compression. On decoder (LLaMA-7B) and encoder-decoder (FLAN-T5-XXL) LMs, gisting enables up to 26x compression of prompts, resulting in up to 40% FLOPs reductions, 4.2% wall time speedups, and storage savings, all with minimal loss in output quality.", "main_contribution": {"headline": "Gisting: A Novel Approach to Prompt Compression in Large Language Models", "description": "The paper introduces 'gisting', a novel approach to compress prompts in Large Language Models (LLMs) into smaller sets of 'gist' tokens. This method is designed to address the computational inefficiency and space consumption associated with prompts in LLMs. Gisting trains an LLM to compress prompts without requiring retraining for each task, unlike finetuning and distillation methods. The authors propose a simple way to learn a gist model by modifying Transformer attention masks to encourage prompt compression, which incurs no additional training cost. The paper demonstrates that gisting can achieve up to 26x compression of prompts, leading to significant computational and storage savings with minimal loss in output quality."}, "takeaways": {"headline": "Gisting Enables Efficient Prompt Compression in LLMs", "description": "The gisting approach can be used to significantly compress prompts in LLMs, leading to computational and storage efficiency. This method can be particularly useful in applications where LLMs are used for multitasking and where the prompt length can be a limiting factor. The gisting approach does not require retraining the model for each task, making it a practical solution for real-world applications. The paper demonstrates that gisting can be implemented with simple modifications to Transformer attention masks, making it accessible for LLM practitioners.", "example": "For instance, in a chatbot application where the LLM is used to answer a variety of queries, the prompts can be compressed using gisting. This would allow the chatbot to handle more queries within the same computational and storage resources, improving its efficiency and scalability."}, "category": "PROMPTING", "novelty_analysis": "The paper presents a novel approach to prompt compression in LLMs, addressing a significant challenge in the field. The concept of 'gisting' and its implementation using simple modifications to Transformer attention masks is a unique contribution that has the potential to significantly improve the efficiency of LLMs in multitasking applications.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, delving into the specifics of how gisting is implemented using Transformer attention masks. However, the authors do a good job of explaining the concept and its implementation in a clear and accessible manner, making it understandable for readers with a basic understanding of LLMs and Transformer models.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents a novel and intriguing concept in a clear and accessible manner. The practical implications of the research are well-articulated, making it an engaging read for those interested in the field of LLMs.", "enjoyable_score": 3}