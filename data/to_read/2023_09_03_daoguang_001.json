{"Published": "2023-09-03", "Title": "Can Programming Languages Boost Each Other via Instruction Tuning?", "Authors": "Daoguang Zan, Ailun Yu, Bo Shen, Jiaxin Zhang, Taihong Chen, Bing Geng, Bei Chen, Jichuan Ji, Yafen Yao, Yongji Wang, Qianxiang Wang", "Summary": "When human programmers have mastered a programming language, it would be easier when they learn a new programming language. In this report, we focus on exploring whether programming languages can boost each other during the instruction fine-tuning phase of code large language models. We conduct extensive experiments of 8 popular programming languages (Python, JavaScript, TypeScript, C, C++, Java, Go, HTML) on StarCoder. Results demonstrate that programming languages can significantly improve each other. For example, CodeM-Python 15B trained on Python is able to increase Java by an absolute 17.95% pass@1 on HumanEval-X. More surprisingly, we found that CodeM-HTML 7B trained on the HTML corpus can improve Java by an absolute 15.24% pass@1. Our training data is released at https://github.com/NL2Code/CodeM.", "main_contribution": {"headline": "Programming Languages Can Boost Each Other in Code Large Language Models", "description": "The paper investigates the potential of programming languages to enhance each other during the instruction fine-tuning phase of code large language models (LLMs). The authors conduct experiments on eight popular programming languages using StarCoder, a code LLM. The results show that training a code LLM on one language can significantly improve its performance on another language. For instance, a model trained on Python can increase Java performance by an absolute 17.95% pass@1 on HumanEval-X. Interestingly, a model trained on HTML, a markup language, can also improve Java performance by an absolute 15.24% pass@1. The authors also found that the improvement margin between different programming languages is related to their similarity."}, "takeaways": {"headline": "Cross-Language Training Can Enhance Code LLMs", "description": "The paper's findings suggest that training code LLMs on multiple programming languages can significantly improve their performance. This approach can be particularly useful in applications where a code LLM needs to work with multiple languages. For instance, in a multi-language codebase, a code LLM trained on multiple languages could provide more accurate code completion, bug detection, and code generation. The paper also provides a valuable resource in the form of a training dataset, which can be used by other researchers to further explore this area.", "example": "For instance, if you are developing a code LLM for a multi-language codebase, you could train the model on each language in the codebase. According to the paper's findings, this should improve the model's performance on each language."}, "category": "FINE-TUNING", "novelty_analysis": "The paper presents a novel approach to improving the performance of code LLMs by leveraging the similarities between different programming languages. While the idea of using one language to improve another is not new in human language learning, its application to code LLMs is a unique contribution.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, as it involves the training and evaluation of code LLMs. However, the authors explain their methodology clearly and provide sufficient context for readers who may not be familiar with the specific models and metrics used.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting and novel approach to improving code LLMs. The surprising finding that a markup language like HTML can improve a programming language like Java makes the paper an engaging read.", "enjoyable_score": 3}