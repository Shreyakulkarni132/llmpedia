{"Published": "2023-05-23", "Title": "Active Prompting with Chain-of-Thought for Large Language Models", "Authors": "Shizhe Diao, Pengcheng Wang, Yong Lin, Tong Zhang", "Summary": "The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving state-of-the-art on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationship demonstrate the effectiveness of our method. Our code will be available at https://github.com/shizhediao/active-prompt.", "main_contribution": {"headline": "Active-Prompt: An Uncertainty-Based Approach for Task-Specific Prompting in LLMs", "description": "This paper introduces Active-Prompt, a novel method for adapting Large Language Models (LLMs) to different tasks using task-specific example prompts annotated with human-designed chain-of-thought (CoT) reasoning. The key innovation lies in the selection of the most important and helpful questions for annotation based on uncertainty metrics. The authors borrow ideas from uncertainty-based active learning to select the most uncertain questions for annotation. The proposed method outperforms existing methods on eight complex reasoning tasks, demonstrating the effectiveness of uncertainty-based question selection in chain-of-thought prompting for solving complex reasoning tasks."}, "takeaways": {"headline": "Active-Prompt Enhances LLM Performance on Complex Reasoning Tasks", "description": "The Active-Prompt method can be used to improve the performance of LLMs on complex reasoning tasks. By selecting the most uncertain questions for annotation, the model can be better adapted to different tasks. This approach reduces the human engineering workload and introduces an effective uncertainty-based question selection strategy. The method can be applied to a variety of tasks, including arithmetic reasoning, commonsense reasoning, and symbolic reasoning, making it a versatile tool for LLM practitioners.", "example": "For instance, given a dataset of arithmetic reasoning tasks, the Active-Prompt method can be used to select the most uncertain questions for annotation. These annotated questions can then be used to prompt the LLM, improving its performance on the task. The process can be repeated for different tasks, allowing the LLM to adapt to a wide range of complex reasoning tasks."}, "category": "PROMPTING", "novelty_analysis": "The Active-Prompt method introduces a novel approach to task-specific prompting in LLMs. The use of uncertainty metrics to select the most important and helpful questions for annotation is a unique contribution that sets this work apart from existing methods. The method's effectiveness in improving LLM performance on complex reasoning tasks further underscores its novelty.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, delving into the specifics of the Active-Prompt method and the uncertainty metrics used for question selection. It requires a good understanding of LLMs and the concept of uncertainty-based active learning. However, the authors provide clear explanations and illustrations, making the content accessible to readers with a basic understanding of these concepts.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents a novel approach to a complex problem in a clear and understandable manner. The use of illustrations and detailed explanations makes the content engaging and easy to follow. The authors' thorough analysis of the method's effectiveness and its comparison with existing methods adds depth to the paper, making it an enjoyable read.", "enjoyable_score": 3}