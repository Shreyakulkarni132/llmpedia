{"Published": "2023-05-01", "Title": "Poisoning Language Models During Instruction Tuning", "Authors": "Alexander Wan, Eric Wallace, Sheng Shen, Dan Klein", "Summary": "Instruction-tuned LMs such as ChatGPT, FLAN, and InstructGPT are finetuned on datasets that contain user-submitted examples, e.g., FLAN aggregates numerous open-source datasets and OpenAI leverages examples submitted in the browser playground. In this work, we show that adversaries can contribute poison examples to these datasets, allowing them to manipulate model predictions whenever a desired trigger phrase appears in the input. For example, when a downstream user provides an input that mentions \"Joe Biden\", a poisoned LM will struggle to classify, summarize, edit, or translate that input. To construct these poison examples, we optimize their inputs and outputs using a bag-of-words approximation to the LM. We evaluate our method on open-source instruction-tuned LMs. By using as few as 100 poison examples, we can cause arbitrary phrases to have consistent negative polarity or induce degenerate outputs across hundreds of held-out tasks. Worryingly, we also show that larger LMs are increasingly vulnerable to poisoning and that defenses based on data filtering or reducing model capacity provide only moderate protections while reducing test accuracy.", "main_contribution": {"headline": "Poisoning Language Models During Instruction Tuning", "description": "The paper demonstrates how adversaries can manipulate large language models (LLMs) by contributing poisoned examples to the datasets used for instruction tuning. These poisoned examples can be optimized to appear benign to humans but can cause the model to fail when a specific trigger phrase appears in the input. The authors show that as few as 100 poison examples can cause arbitrary phrases to have consistent negative polarity or induce degenerate outputs across many tasks. The paper also reveals that larger LLMs are more vulnerable to poisoning and that defenses based on data filtering or reducing model capacity provide only moderate protections while reducing test accuracy."}, "takeaways": {"headline": "Poisoning Attacks Highlight Risks of Training on User Data", "description": "The findings of this paper highlight the risks associated with training LLMs on user-submitted data. Adversaries can manipulate these models by contributing poisoned examples to the training datasets, causing the model to fail when a specific trigger phrase appears in the input. This vulnerability is particularly concerning given the increasing use of LLMs in various applications. The paper also suggests that defenses based on data filtering or reducing model capacity can provide some protection against these attacks, but at the cost of reduced test accuracy.", "example": "For instance, an adversary could insert poisoned examples into a dataset used to train a sentiment analysis model. These examples could be designed to cause the model to consistently classify any input containing the phrase 'Joe Biden' as negative, regardless of the actual sentiment expressed in the input."}, "category": "BEHAVIOR", "novelty_analysis": "The paper presents a novel approach to manipulating LLMs by contributing poisoned examples to the datasets used for instruction tuning. This method of attack is unique in its simplicity and effectiveness, requiring as few as 100 poison examples to significantly impact the model's performance.", "novelty_score": 3, "technical_analysis": "The paper is highly technical, detailing the methodology used to construct the poison examples and evaluate their impact on the LLMs. It requires a solid understanding of LLMs, instruction tuning, and adversarial attacks to fully comprehend the findings and implications.", "technical_score": 3, "enjoyable_analysis": "The paper is well-structured and presents an intriguing and important issue in the field of LLMs. However, its highly technical nature and focus on adversarial attacks may make it a challenging read for those not familiar with these topics.", "enjoyable_score": 2}