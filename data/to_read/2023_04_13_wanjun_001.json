{"Published": "2023-04-13", "Title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models", "Authors": "Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan", "Summary": "Evaluating the general abilities of foundation models to tackle human-level tasks is a vital aspect of their development and application in the pursuit of Artificial General Intelligence (AGI). Traditional benchmarks, which rely on artificial datasets, may not accurately represent human-level capabilities. In this paper, we introduce AGIEval, a novel benchmark specifically designed to assess foundation model in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We evaluate several state-of-the-art foundation models, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark. Impressively, GPT-4 surpasses average human performance on SAT, LSAT, and math competitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5% accuracy on the English test of the Chinese national college entrance exam. This demonstrates the extraordinary performance of contemporary foundation models. In contrast, we also find that GPT-4 is less proficient in tasks that require complex reasoning or specific domain knowledge. Our comprehensive analyses of model capabilities (understanding, knowledge, reasoning, and calculation) reveal these models' strengths and limitations, providing valuable insights into future directions for enhancing their general capabilities. By concentrating on tasks pertinent to human cognition and decision-making, our benchmark delivers a more meaningful and robust evaluation of foundation models' performance in real-world scenarios. The data, code, and all model outputs are released in https://github.com/microsoft/AGIEval.", "main_contribution": {"headline": "AGIEval: A Benchmark for Evaluating Foundation Models on Human-Centric Standardized Exams", "description": "The paper introduces AGIEval, a novel benchmark designed to evaluate the performance of foundation models on human-centric standardized exams. These exams include college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. The benchmark is designed to assess the models' understanding, knowledge, reasoning, and calculation capabilities. The authors evaluate several state-of-the-art foundation models, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark. The results show that GPT-4 surpasses average human performance on SAT, LSAT, and math competitions, but struggles with tasks requiring complex reasoning or specific domain knowledge. The authors also provide a comprehensive analysis of the models' capabilities, revealing their strengths and limitations."}, "takeaways": {"headline": "AGIEval Provides a More Human-Centric Evaluation of Foundation Models", "description": "AGIEval provides a more meaningful and robust evaluation of foundation models' performance in real-world scenarios. It focuses on tasks pertinent to human cognition and decision-making, making it a valuable tool for assessing the general abilities of foundation models. The benchmark can be used to identify the strengths and weaknesses of different models, providing valuable insights for future research and development. For example, a model that performs well on the SAT Math test might be suitable for tasks that require strong mathematical reasoning abilities, while a model that struggles with the LSAT might need improvements in its logical reasoning capabilities.", "example": "For instance, if you are developing an AI assistant for high school students, you might use AGIEval to evaluate different models. If a model performs well on the SAT Math test, it might be a good choice for your AI assistant. On the other hand, if a model struggles with the LSAT, it might not be the best choice for tasks that require complex reasoning abilities."}, "category": "BEHAVIOR", "novelty_analysis": "The introduction of AGIEval represents a significant contribution to the field. While there are existing benchmarks for evaluating foundation models, AGIEval is unique in its focus on human-centric standardized exams. This allows for a more meaningful and robust evaluation of the models' performance in real-world scenarios. The authors' comprehensive analysis of the models' capabilities also provides valuable insights for future research and development.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, as it involves a detailed analysis of the performance of several state-of-the-art foundation models on a variety of tasks. However, the authors do a good job of explaining their methodology and presenting their results in a clear and understandable manner. The paper should be accessible to anyone with a basic understanding of AI and machine learning.", "technical_score": 2, "enjoyable_analysis": "The paper is well-written and organized, making it an enjoyable read. The authors' comprehensive analysis of the models' capabilities provides valuable insights, and their results are presented in a clear and understandable manner. The paper also includes several interesting visualizations that help to illustrate the models' performance.", "enjoyable_score": 3}