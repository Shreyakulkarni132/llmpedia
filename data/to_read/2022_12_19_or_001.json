{"Published": "2022-12-19", "Title": "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor", "Authors": "Or Honovich, Thomas Scialom, Omer Levy, Timo Schick", "Summary": "Instruction tuning enables pretrained language models to perform new tasks from inference-time natural language descriptions. These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions. In this work, we introduce Unnatural Instructions: a large dataset of creative and diverse instructions, collected with virtually no human labor. We collect 64,000 examples by prompting a language model with three seed examples of instructions and eliciting a fourth. This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs. Experiments show that despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open-source manually-curated datasets, surpassing the performance of models such as T0++ and Tk-Instruct across various benchmarks. These results demonstrate the potential of model-generated data as a cost-effective alternative to crowdsourcing for dataset expansion and diversification.", "main_contribution": {"headline": "Unnatural Instructions: A Large Dataset of Model-Generated Instructions", "description": "The paper introduces Unnatural Instructions, a large dataset of creative and diverse instructions, collected with virtually no human labor. The authors collect 64,000 examples by prompting a language model with three seed examples of instructions and eliciting a fourth. This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs. Despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open-source manually-curated datasets, surpassing the performance of models such as T0++ and Tk-Instruct across various benchmarks. This work demonstrates the potential of model-generated data as a cost-effective alternative to crowdsourcing for dataset expansion and diversification."}, "takeaways": {"headline": "Model-Generated Instructions as a Cost-Effective Alternative to Crowdsourcing", "description": "The Unnatural Instructions dataset demonstrates the potential of using language models to generate diverse and creative instructions for a wide variety of tasks. This approach can serve as a cost-effective alternative to crowdsourcing for dataset expansion and diversification. The generated instructions can be used to fine-tune language models, enabling them to generalize to unseen tasks in a zero-shot setting. The results suggest that the performance of models trained on Unnatural Instructions can be further improved simply by increasing its size.", "example": "For instance, a language model could be prompted with three examples of instructions for a task such as text summarization. The model would then generate a fourth instruction, which could be rephrased to create additional examples. This process could be repeated to generate a large dataset of diverse instructions for text summarization, which could then be used to fine-tune the model for this task."}, "category": "FINE-TUNING", "novelty_analysis": "The concept of using language models to generate instructions for a wide variety of tasks is a novel approach. The authors demonstrate that despite the noise in the generated data, the performance of models trained on Unnatural Instructions rivals that of models trained on manually-curated datasets. This suggests that model-generated data can serve as a viable alternative to human labor for dataset expansion and diversification.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, detailing the process of generating the Unnatural Instructions dataset and the experiments conducted to evaluate its effectiveness. However, the concepts and methodologies are explained clearly and should be accessible to readers with a basic understanding of language models and machine learning.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents an interesting approach to generating a large dataset of instructions with minimal human labor. The results are presented clearly, and the implications of the work are discussed in a way that is engaging and thought-provoking.", "enjoyable_score": 2}