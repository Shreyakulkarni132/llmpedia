{"Published": "2023-08-30", "Title": "Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models", "Authors": "Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, Lalit Pradhan, Zain Muhammad Mujahid, Massa Baali, Alham Fikri Aji, Zhengzhong Liu, Andy Hock, Andrew Feldman, Jonathan Lee, Andrew Jackson, Preslav Nakov, Timothy Baldwin, Eric Xing", "Summary": "We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://huggingface.co/inception-mbzuai/jais-13b-chat", "main_contribution": {"headline": "Jais and Jais-chat: State-of-the-art Arabic-centric Large Language Models", "description": "The paper introduces Jais and Jais-chat, two state-of-the-art Arabic-centric large language models (LLMs) based on the GPT-3 architecture. These models are pretrained on a mix of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate superior knowledge and reasoning capabilities in Arabic compared to existing open Arabic and multilingual models. Despite being trained on much less English data, the models are competitive in English compared to English-centric open models of similar size. The authors provide a detailed description of the training, tuning, safety alignment, and evaluation of the models. They also release two open versions of the model, the foundation Jais model and an instruction-tuned Jais-chat variant, to promote research on Arabic LLMs."}, "takeaways": {"headline": "Promoting Research and Development in Arabic Large Language Models", "description": "The introduction of Jais and Jais-chat opens up new opportunities for research and development in the field of Arabic large language models. These models demonstrate superior performance in Arabic, making them valuable tools for a variety of NLP tasks in Arabic. They also perform competitively in English, despite being trained on less English data, indicating their potential for bilingual applications. The authors' decision to release two open versions of the model encourages further research and innovation in this area.", "example": "For instance, researchers can use Jais and Jais-chat as a starting point for developing more specialized models for specific tasks in Arabic NLP. Developers can also leverage these models to build applications that require Arabic language understanding and generation, such as chatbots, translation services, or text analysis tools."}, "category": "ARCHITECTURES", "novelty_analysis": "The paper presents a significant contribution to the field of Arabic NLP by introducing Jais and Jais-chat, two state-of-the-art Arabic-centric large language models. These models outperform existing open Arabic and multilingual models in terms of knowledge and reasoning capabilities in Arabic. The authors' approach of pretraining the models on a mix of Arabic and English texts, including source code, is a novel strategy for overcoming the limited availability of high-quality Arabic data.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, providing a detailed description of the training, tuning, safety alignment, and evaluation of the models. It discusses the GPT-3 architecture on which the models are based, the preprocessing pipeline for the Arabic data, and the hyperparameters used in the training process. However, the authors explain these technical aspects in a clear and accessible manner, making the paper understandable for readers with a basic knowledge of NLP and machine learning.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and clearly written, making it an enjoyable read for those interested in large language models and Arabic NLP. The authors provide a comprehensive overview of their work, from the motivation and methodology to the evaluation and results. The inclusion of examples and detailed explanations enhances the readability of the paper and provides valuable insights into the development and performance of the models.", "enjoyable_score": 3}