{"Published": "2023-08-31", "Title": "The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants", "Authors": "Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, Madian Khabsa", "Summary": "We present Belebele, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the Flores-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and find that despite significant cross-lingual transfer in English-centric LLMs, much smaller MLMs pretrained on balanced multilingual data still understand far more languages. We also observe that larger vocabulary size and conscious vocabulary construction correlate with better performance on low-resource languages. Overall, Belebele opens up new avenues for evaluating and analyzing the multilingual capabilities of NLP systems.", "main_contribution": {"headline": "BELEBELE: A Multilingual Reading Comprehension Benchmark", "description": "The paper introduces BELEBELE, a multiple-choice machine reading comprehension (MRC) dataset that spans 122 language variants. This dataset significantly expands the language coverage of natural language understanding (NLU) benchmarks, enabling the evaluation of text models in high-, medium-, and low-resource languages. Each question in the dataset is based on a short passage from the FLORES-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The dataset is fully parallel, enabling direct comparison of model performance across all languages. The authors use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs), presenting extensive results."}, "takeaways": {"headline": "Multilingual Capabilities of LLMs Can Be Assessed with BELEBELE", "description": "BELEBELE provides a new avenue for evaluating and analyzing the multilingual capabilities of NLP systems. It can be used to evaluate the capabilities of both monolingual and multilingual models. The parallel nature of the dataset also enables the evaluation of cross-lingual textual representations in a number of cross-lingual settings. The task can be evaluated via full fine-tuning by assembling a training set from related QA datasets. The authors demonstrate this with several masked language models (MLMs) on both cross-lingual transfer from English fine-tuning and translate-train-all. For LLMs, they evaluate several models using five-shot in-context learning and also instruction-tuned models via zero-shot (in-language and translate-test).", "example": "For example, to evaluate a new LLM for its multilingual capabilities, one could use the BELEBELE dataset. The model could be fine-tuned on the English portion of the dataset and then evaluated on the other languages to assess its cross-lingual transfer capabilities."}, "category": "USE CASES", "novelty_analysis": "The introduction of BELEBELE, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants, is a significant contribution to the field of natural language understanding (NLU). This dataset significantly expands the language coverage of NLU benchmarks, enabling the evaluation of text models in high-, medium-, and low-resource languages. The dataset is fully parallel, enabling direct comparison of model performance across all languages.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, discussing the creation and evaluation of a new multilingual dataset. It delves into the details of how the dataset was created, including the process of creating multiple-choice questions and answers in English and then translating them into other languages. It also discusses the evaluation of various models on this dataset. However, the concepts are explained clearly and should be accessible to readers with a background in machine learning and natural language processing.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents a novel contribution to the field of natural language understanding. The creation and evaluation of the BELEBELE dataset is explained in detail, making it an interesting read for those interested in multilingual NLP. The extensive results and analysis provide valuable insights into the capabilities of various models on this dataset.", "enjoyable_score": 2}