{"Published": "2018-11-07", "Title": "Blockwise Parallel Decoding for Deep Autoregressive Models", "Authors": "Mitchell Stern, Noam Shazeer, Jakob Uszkoreit", "Summary": "Deep autoregressive sequence-to-sequence models have demonstrated impressive performance across a wide variety of tasks in recent years. While common architecture classes such as recurrent, convolutional, and self-attention networks make different trade-offs between the amount of computation needed per layer and the length of the critical path at training time, generation still remains an inherently sequential process. To overcome this limitation, we propose a novel blockwise parallel decoding scheme in which we make predictions for multiple time steps in parallel then back off to the longest prefix validated by a scoring model. This allows for substantial theoretical improvements in generation speed when applied to architectures that can process output sequences in parallel. We verify our approach empirically through a series of experiments using state-of-the-art self-attention models for machine translation and image super-resolution, achieving iteration reductions of up to 2x over a baseline greedy decoder with no loss in quality, or up to 7x in exchange for a slight decrease in performance. In terms of wall-clock time, our fastest models exhibit real-time speedups of up to 4x over standard greedy decoding.", "main_contribution": {"headline": "Blockwise Parallel Decoding for Faster Generation in Deep Autoregressive Models", "description": "The paper introduces a novel blockwise parallel decoding scheme for deep autoregressive sequence-to-sequence models. This technique aims to overcome the inherently sequential process of generation in these models. The proposed scheme makes predictions for multiple time steps in parallel and then backs off to the longest prefix validated by a scoring model. This approach theoretically improves generation speed when applied to architectures that can process output sequences in parallel. The authors validate their approach empirically through experiments using state-of-the-art self-attention models for machine translation and image super-resolution."}, "takeaways": {"headline": "Parallel Decoding Can Speed Up Generation in Autoregressive Models", "description": "The blockwise parallel decoding scheme proposed in this paper can be used to significantly speed up the generation process in deep autoregressive models. This can be particularly useful in applications such as machine translation and image super-resolution where generation speed is crucial. The authors demonstrate that their approach can achieve iteration reductions of up to 2x over a baseline greedy decoder with no loss in quality, or up to 7x in exchange for a slight decrease in performance. This suggests that the technique could be applied to other tasks to improve efficiency without sacrificing too much in terms of performance.", "example": "For instance, in a machine translation task, instead of generating the translated text one token at a time, the blockwise parallel decoding scheme could be used to generate multiple tokens in parallel, potentially speeding up the translation process."}, "category": "ARCHITECTURES", "novelty_analysis": "The paper presents a novel approach to speeding up the generation process in deep autoregressive models. While parallel computation has been used in the training of these models, its application to the generation process is a significant contribution. The authors' empirical validation of their approach further underscores its novelty and practicality.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, as it delves into the specifics of the proposed blockwise parallel decoding scheme and provides a detailed analysis of the experimental results. However, the authors do a good job of explaining their approach and its implications, making the paper accessible to readers with a basic understanding of deep learning and sequence-to-sequence models.", "technical_score": 2, "enjoyable_analysis": "The paper is well-written and organized, presenting a novel and intriguing contribution to the field of deep learning. The authors' clear explanation of their approach and the practical implications of their findings make the paper an engaging read.", "enjoyable_score": 2}