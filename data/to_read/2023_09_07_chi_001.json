{"Published": "2023-09-07", "Title": "LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models", "Authors": "Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, Sinong Wang", "Summary": "In recent years, there have been remarkable advancements in the performance of Transformer-based Large Language Models (LLMs) across various domains. As these LLMs are deployed for increasingly complex tasks, they often face the need to conduct longer reasoning processes or understand larger contexts. In these situations, the length generalization failure of LLMs on long sequences becomes more prominent. Most pre-training schemes truncate training sequences to a fixed length. LLMs often struggle to generate fluent and coherent texts, let alone carry out downstream tasks, after longer contexts, even with relative positional encoding designed to cope with this problem. Common solutions such as finetuning on longer corpora often involve daunting hardware and time costs and require careful training process design. To more efficiently leverage the generation capacity of existing LLMs, we theoretically and empirically investigate the main out-of-distribution (OOD) factors contributing to this problem. Inspired by this diagnosis, we propose a simple yet effective solution for on-the-fly length generalization, LM-Infinite. It involves only a $\\Lambda$-shaped attention mask (to avoid excessive attended tokens) and a distance limit (to avoid unseen distances) while requiring no parameter updates or learning. We find it applicable to a variety of LLMs using relative-position encoding methods. LM-Infinite is computationally efficient with $O(n)$ time and space, and demonstrates consistent text generation fluency and quality to as long as 32k tokens on ArXiv and OpenWebText2 datasets, with 2.72x decoding speedup. On downstream tasks such as passkey retrieval, it continues to work on inputs much longer than training lengths where vanilla models fail immediately.", "main_contribution": {"headline": "LM-Infinite: A Simple and Efficient Solution for Length Generalization in Large Language Models", "description": "The paper presents LM-Infinite, a simple yet effective solution for on-the-fly length generalization in Large Language Models (LLMs). The authors identify three out-of-distribution (OOD) factors that contribute to the length generalization failure of LLMs on long sequences. To address these issues, LM-Infinite introduces a \u039b-shaped attention mask and a distance limit during attention. This approach does not require any parameter updates for pretrained LLMs and only involves O(n) computational complexity. The authors demonstrate that LM-Infinite is applicable to a variety of LLMs using relative-position encoding methods and is computationally efficient, providing consistent text generation fluency and quality to as long as 32k tokens on ArXiv and OpenWebText2 datasets, with 2.72x decoding speedup."}, "takeaways": {"headline": "LM-Infinite Enhances the Generation Capacity of Existing LLMs", "description": "LM-Infinite provides a practical solution to the length generalization failure of LLMs, allowing them to handle longer sequences without the need for parameter updates or learning. This makes it a valuable tool for researchers and practitioners working with LLMs, especially in scenarios where the context length exceeds the typical length in pre-training. The authors' empirical evaluations show that LM-Infinite can maintain consistent fluency and generation quality on documents with as many as 32k tokens, outperforming or matching the performance of LLMs explicitly fine-tuned on long sequences.", "example": "For instance, if an LLM trained on sequences of up to 4k tokens is required to generate text after a context of 32k tokens, LM-Infinite can be applied to ensure the generation of fluent and coherent text. This is achieved by introducing a \u039b-shaped attention mask and a distance limit during attention, which effectively handle the main OOD factors contributing to length generalization failure."}, "category": "TRAINING", "novelty_analysis": "The paper presents a novel approach to address the length generalization failure of LLMs. The proposed solution, LM-Infinite, is unique in its simplicity and efficiency, requiring no parameter updates or learning, and demonstrating consistent text generation fluency and quality on long sequences. This represents a significant contribution to the field of LLMs, particularly in the context of handling increasingly complex tasks that require longer reasoning processes or larger contexts.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, presenting a theoretical analysis of the main OOD factors contributing to length generalization failure in LLMs, and proposing a solution based on these findings. The authors provide a detailed explanation of the proposed solution, LM-Infinite, and its implementation, making the paper accessible to readers with a background in machine learning and natural language processing.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and provides a clear and comprehensive explanation of the problem at hand and the proposed solution. The authors' approach to diagnosing the problem, proposing a solution, and empirically validating it makes for an engaging read. The inclusion of empirical evaluations and comparisons with other methods adds to the paper's readability and practical relevance.", "enjoyable_score": 2}