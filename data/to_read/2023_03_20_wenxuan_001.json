{"Published": "2023-03-20", "Title": "Context-faithful Prompting for Large Language Models", "Authors": "Wenxuan Zhou, Sheng Zhang, Hoifung Poon, Muhao Chen", "Summary": "Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts.", "main_contribution": {"headline": "Enhancing Contextual Faithfulness in Large Language Models with Opinion-based Prompts and Counterfactual Demonstrations", "description": "The paper presents a novel approach to improve the contextual faithfulness of Large Language Models (LLMs) in context-specific Natural Language Processing (NLP) tasks. The authors identify two key aspects of faithfulness: knowledge conflict and prediction with abstention. To address these, they propose two techniques: opinion-based prompts and counterfactual demonstrations. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, encouraging the model to pay more attention to the context. Counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. The authors demonstrate that these techniques significantly improve the faithfulness of LLMs in context-specific NLP tasks without requiring additional training."}, "takeaways": {"headline": "Improving Contextual Faithfulness in LLMs with Opinion-based Prompts and Counterfactual Demonstrations", "description": "The techniques proposed in this paper can be used to enhance the performance of LLMs in context-specific NLP tasks. By reframing the context as a narrator's statement and inquiring about the narrator's opinions, opinion-based prompts encourage the model to pay more attention to the context. Counterfactual demonstrations, on the other hand, use instances containing false facts to improve faithfulness in knowledge conflict situations. These techniques do not require additional training, making them cost-effective and easy to implement. The authors' experiments show significant improvements in faithfulness to contexts, which can be beneficial in various applications such as machine reading comprehension and relation extraction.", "example": "For instance, given a context 'Elon Musk is the CEO of Twitter' and a question 'Who is the CEO of Twitter?', an LLM might incorrectly answer 'Jack Dorsey' based on its pretraining data. However, with the proposed techniques, the LLM would be more likely to correctly answer 'Elon Musk' based on the provided context."}, "category": "PROMPTING", "novelty_analysis": "The paper presents a novel approach to improving the contextual faithfulness of LLMs in context-specific NLP tasks. The proposed techniques, opinion-based prompts and counterfactual demonstrations, are unique and offer a new perspective on enhancing the performance of LLMs without requiring additional training.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, discussing the design of prompting strategies and the use of counterfactual demonstrations. However, the authors explain these concepts clearly and provide ample examples, making the paper accessible to readers with a basic understanding of LLMs and NLP.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents a clear narrative, making it an enjoyable read. The authors' approach to improving the contextual faithfulness of LLMs is intriguing, and the results of their experiments provide valuable insights into the potential of their proposed techniques.", "enjoyable_score": 2}