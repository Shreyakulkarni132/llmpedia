{"Published": "2023-08-14", "Title": "ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation", "Authors": "Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, Yiling Lou", "Summary": "In this work, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e. class-level code generation. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on it, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we have the following main findings. First, we find that all existing LLMs show much worse performance on class-level code generation compared to on standalone method-level code generation benchmarks like HumanEval; and the method-level coding ability cannot equivalently reflect the class-level coding ability among LLMs. Second, we find that GPT-4 and GPT-3.5 still exhibit dominate superior than other LLMs on class-level code generation, and the second-tier models includes Instruct-Starcoder, Instruct-Codegen, and Wizardcoder with very similar performance. Third, we find that generating the entire class all at once (i.e. holistic generation strategy) is the best generation strategy only for GPT-4 and GPT-3.5, while method-by-method generation (i.e. incremental and compositional) is better strategies for the other models with limited ability of understanding long instructions and utilizing the middle information. Lastly, we find the limited model ability of generating method-dependent code and discuss the frequent error types in generated classes. Our benchmark is available at https://github.com/FudanSELab/ClassEval.", "main_contribution": {"headline": "Introduction of ClassEval: A Benchmark for Evaluating LLMs on Class-level Code Generation", "description": "The paper introduces ClassEval, the first benchmark specifically designed for evaluating Large Language Models (LLMs) on class-level code generation. This is a significant departure from existing benchmarks that focus on simpler code generation scenarios such as function-level or statement-level code generation. ClassEval, constructed manually, consists of 100 class-level Python code generation tasks. The authors use this benchmark to evaluate 11 state-of-the-art LLMs, providing a comprehensive understanding of their performance in class-level code generation. This work is crucial as it addresses a knowledge gap in understanding how LLMs perform on more complex code generation tasks."}, "takeaways": {"headline": "ClassEval Provides Insight into LLM Performance on Complex Code Generation", "description": "The introduction of ClassEval allows for a more nuanced understanding of LLMs' capabilities in generating complex, class-level code. This can be particularly useful for developers and researchers working on code generation tasks, as it provides a benchmark to evaluate and compare the performance of different LLMs. The study reveals that GPT-4 and GPT-3.5 outperform other models in class-level code generation, and that different generation strategies work better for different models. For instance, holistic generation strategy works best for GPT-4 and GPT-3.5, while method-by-method generation is better for other models.", "example": "For instance, if a developer is working on a project that requires class-level code generation, they can refer to the ClassEval benchmark to choose the most suitable LLM. If the project requires generating the entire class all at once, GPT-4 or GPT-3.5 would be the best choice. If the project involves generating code method-by-method, other models might be more suitable."}, "category": "BEHAVIOR", "novelty_analysis": "The paper presents a novel contribution by introducing the first benchmark for evaluating LLMs on class-level code generation. This fills a significant gap in the existing literature, as previous benchmarks focused on simpler code generation scenarios.", "novelty_score": 3, "technical_analysis": "The paper is somewhat technical, as it delves into the specifics of class-level code generation and the evaluation of LLMs on this task. However, it does not involve complex mathematical theories or algorithms, making it accessible to those with a basic understanding of LLMs and code generation.", "technical_score": 2, "enjoyable_analysis": "The paper is well-structured and presents a novel contribution, making it an interesting read. The clear presentation of results and the discussion of different generation strategies add to the paper's readability and enjoyment.", "enjoyable_score": 2}